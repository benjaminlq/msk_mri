{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only best guideline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import GUIDELINES\n",
    "\n",
    "all_guidelines = \"\\n\".join([\"- \" + guideline for guideline in GUIDELINES])\n",
    "\n",
    "extract_template = \"\"\"You are a radiologist expert. Do not make up additional information.\n",
    "=========\n",
    "TASK: You are given a PATIENT PROFILE. You need to perform the following information referencing from the PATIENT PROFILE:\n",
    "1. Extract relevant information for recommendation of imaging procedure, including age, symptomps, previous diagnosis, stage of diagnosis (INITIAL IMAGING OR NEXT STUDY) and suspected conditions, if any.\n",
    "Only return information given inside the PROFILE, do not make up other information.\n",
    "2. Return the best guideline from the following list of guidelines that is relevant to the recommendations of imaging procedure given patient profile. If there are no relevant guideline, return empty string.\n",
    "The recommended guideline must match the exact text given in the list.\n",
    "{}\n",
    "=========\n",
    "OUTPUT INSTRUCTION:\n",
    "Output your answer as follow:\n",
    "1. Relevant information:\n",
    "2. Relevant guidelines:\n",
    "=========\n",
    "\"\"\".format(all_guidelines)\n",
    "\n",
    "human_template = \"PATIENT PROFILE: {query_str}\"\n",
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate as LCChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "EXTRACT_TEMPLATE = LCChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(extract_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template)\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_chain = LLMChain(\n",
    "    prompt=EXTRACT_TEMPLATE,\n",
    "    llm=ChatOpenAI(model=\"gpt-4\", temperature=0, max_tokens=512)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|██████████| 71/71 [24:14<00:00, 20.49s/it]   \n"
     ]
    }
   ],
   "source": [
    "best_guideline_answers = []\n",
    "for clinical_file in tqdm(testcase_df[\"Clinical File\"],\n",
    "                          total=len(testcase_df[\"Clinical File\"])):\n",
    "    profile = remove_final_sentence(clinical_file)\n",
    "    response = extract_chain(profile)\n",
    "    best_guideline_answers.append(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles, guidelines = [], []\n",
    "\n",
    "for answer in best_guideline_answers:\n",
    "    if answer.endswith(\".\"):\n",
    "        answer = answer[:-1]\n",
    "    pattern = r\"1. Relevant information:([\\S\\s]+)2. Relevant guidelines:([\\S\\s]*)\"\n",
    "    profile, guidelines_str = re.findall(pattern, answer)[0]\n",
    "    \n",
    "    guidelines_str = guidelines_str.replace(\"- \", \"\")\n",
    "    guidelines_str = guidelines_str.strip()\n",
    "    guidelines_str = guidelines_str.replace(\"\\n\", \", \")\n",
    "    \n",
    "    extracted_guidelines = re.findall(r\"([A-Za-z ]+)\", guidelines_str)\n",
    "    for i, extracted_guideline in enumerate(extracted_guidelines):\n",
    "        extracted_guideline = extracted_guideline.lower()\n",
    "        min_dist, nearest_text = calculate_min_dist(extracted_guideline, GUIDELINES, True)\n",
    "        if min_dist <= 1:\n",
    "            extracted_guidelines[i] = nearest_text\n",
    "        else:\n",
    "            print(extracted_guideline, nearest_text)\n",
    "    \n",
    "    profiles.append(profile)    \n",
    "    guidelines.append(extracted_guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_info_best_json = {\"profiles\": profiles, \"guidelines\": guidelines}\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_best.json\"), \"w\") as f:\n",
    "    json.dump(extracted_info_best_json, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
