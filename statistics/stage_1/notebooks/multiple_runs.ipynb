{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from config import MAIN_DIR\n",
    "from math import comb\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score\n",
    "from typing import Sequence, Dict, Any\n",
    "\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\", \"data\", \"answers\")\n",
    "\n",
    "human_result_path = os.path.join(MAIN_DIR, \"artifacts\", \"human_responses.xlsx\")\n",
    "ai_result_path = os.path.join(MAIN_DIR, \"artifacts\", \"ai_results_summary.xlsx\")\n",
    "save_folder = os.path.join(\"..\", \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(\n",
    "    data: Sequence  \n",
    ") -> Any:\n",
    "    counter = Counter(data)\n",
    "    return max(counter, key=counter.get)\n",
    "\n",
    "def find_median(\n",
    "    sequence: Sequence  \n",
    ") -> int:\n",
    "    return np.argsort(sequence)[len(sequence)//2]\n",
    "\n",
    "def rename_column(\n",
    "    column: pd.Series,\n",
    "    rename_dict: Dict = {\n",
    "        \"USUALLY APPROPRIATE\": \"UA/MBA\",\n",
    "        \"MAY BE APPROPRIATE\": \"UA/MBA\",\n",
    "        \"UA\": \"UA/MBA\", \"MBA\": \"UA/MBA\",\n",
    "        \"USUALLY NOT APPROPRIATE\": \"UNA\",\n",
    "        \"INSUFFICIENT INFORMATION\": \"ICI\"\n",
    "        }\n",
    ") -> pd.Series:\n",
    "    return column.replace(rename_dict)\n",
    "\n",
    "def evaluate_results(\n",
    "    labels: Sequence, preds: Sequence    \n",
    ") -> Dict:\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_precision = precision_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_precision = precision_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    macro_recall = recall_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_recall = recall_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    macro_f1 = f1_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_f1 = f1_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "    }\n",
    "    \n",
    "def preprocess(result_df):\n",
    "    result_df.columns = result_df.columns.str.strip()\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY APPROPRIATE\", \"UA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY NOT APPROPRIATE\", \"UNA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"MAY BE APPROPRIATE\", \"MBA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"INSUFFICIENT INFORMATION\", \"ICI\")\n",
    "    result_df[\"ANSKEY1\"] = result_df[\"ANSKEY1\"].str.strip()\n",
    "    return result_df\n",
    "    \n",
    "def evaluate_human_results(result_df, classification_dict):\n",
    "    result_df = preprocess(result_df)\n",
    "    assert len(result_df) == 70, \"Number of testcases must be 70.\"\n",
    "\n",
    "    preds = result_df[\"Approp Score\"].replace(classification_dict)\n",
    "    labels = result_df[\"ANSKEY1\"].replace(classification_dict)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_precision = precision_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_precision = precision_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    macro_recall = recall_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_recall = recall_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    macro_f1 = f1_score(labels, preds, average = \"macro\", zero_division=np.nan)\n",
    "    weighted_f1 = f1_score(labels, preds, average = \"weighted\", zero_division=np.nan)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n",
    "\n",
    "def _make_df_square(table):\n",
    "    \"\"\"\n",
    "    Reindex a pandas DataFrame so that it becomes square, meaning that\n",
    "    the row and column indices contain the same values, in the same\n",
    "    order.  The row and column index are extended to achieve this.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(table, pd.DataFrame):\n",
    "        return table\n",
    "\n",
    "    # If the table is not square, make it square\n",
    "    if not table.index.equals(table.columns):\n",
    "        ix = list(set(table.index) | set(table.columns))\n",
    "        ix.sort()\n",
    "        table = table.reindex(index=ix, columns=ix, fill_value=0)\n",
    "\n",
    "    # Ensures that the rows and columns are in the same order.\n",
    "    table = table.reindex(table.columns)\n",
    "\n",
    "    return table\n",
    "\n",
    "def calculate_midp_mcnemar(confusion_matrix):\n",
    "    confusion_matrix = _make_df_square(confusion_matrix)\n",
    "    confusion_matrix = np.asarray(confusion_matrix, dtype=int)\n",
    "    n1, n2 = confusion_matrix[0, 1], confusion_matrix[1, 0]\n",
    "    statistic = np.minimum(n1, n2)\n",
    "    total_sum = n1 + n2\n",
    "    mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "    mcnemar_pvalue = mcnemar_results.pvalue\n",
    "    midp_mcnemar_pvalue = mcnemar_pvalue - comb(total_sum, statistic) * (0.5 ** total_sum)\n",
    "    return midp_mcnemar_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "matching_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Human Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels = [\n",
    "    'Human_rad1(CLP)',\n",
    "    'Human_rad2(KGY)',\n",
    "    'Human_trainee1(CHY)',\n",
    "    'Human_trainee2(SUD)',\n",
    "    'Human_ortho(NYH)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for human_label in human_labels:\n",
    "    summary = {\"respondent\": human_label}\n",
    "    human_df = pd.read_excel(human_result_path, sheet_name=human_label)\n",
    "    human_df[\"human_gt\"] = rename_column(human_df[\"human_gt\"])\n",
    "    human_df[\"answer\"] = rename_column(\n",
    "        human_df[\"answer\"],\n",
    "        rename_dict = {\n",
    "        \"USUALLY APPROPRIATE\": \"UA/MBA\",\n",
    "        \"MAY BE APPROPRIATE\": \"UA/MBA\",\n",
    "        \"UA\": \"UA/MBA\", \"MBA\": \"UA/MBA\",\n",
    "        \"USUALLY NOT APPROPRIATE\": \"UNA\",\n",
    "        \"INSUFFICIENT INFORMATION\": \"ICI\"\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    evaluations = evaluate_results(human_df[\"human_gt\"], human_df[\"answer\"])\n",
    "    # evaluations = evaluate_human_results(\n",
    "    #     human_df, {\"UA\": \"UA/MBA\", \"MBA\": \"UA/MBA\", \"UNA\": \"UNA\", \"ICI\": \"ICI\"})\n",
    "    summary.update(evaluations)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    matching_dict[human_label] = human_df[\"human_gt\"] == human_df[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process AI Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "summary_results = {}\n",
    "\n",
    "ai_modes = [\n",
    "    \"NoRAG\", \"NoRAG Reorder\",\n",
    "    \"BaseRAG\", \"BaseRAG Reorder\",\n",
    "    \"CombinedRAG\", \"CombinedRAG Reorder\"\n",
    "    # \"BaseRAG-COT\"\n",
    "]\n",
    "\n",
    "for ai_mode in ai_modes:\n",
    "    \n",
    "    summary_results[ai_mode] = {\n",
    "        \"accuracy\": [],\n",
    "        \"macro_precision\": [],\n",
    "        \"weighted_precision\": [],\n",
    "        \"macro_recall\": [],\n",
    "        \"weighted_recall\": [],\n",
    "        \"macro_f1\": [],\n",
    "        \"weighted_f1\": [],\n",
    "    }\n",
    "    \n",
    "    df_dict[ai_mode] = pd.read_excel(ai_result_path, sheet_name=ai_mode, index_col=\"No\")\n",
    "    df_dict[ai_mode][\"human_gt\"] = rename_column(df_dict[ai_mode][\"human_gt\"])\n",
    "    \n",
    "    for i in range(5):\n",
    "        df_dict[ai_mode][f\"answer_{i+1}\"] = rename_column(df_dict[ai_mode][f\"answer_{i+1}\"])\n",
    "        result_metrics = evaluate_results(df_dict[ai_mode][\"human_gt\"], df_dict[ai_mode][f\"answer_{i+1}\"])\n",
    "        \n",
    "        summary_results[ai_mode][\"accuracy\"].append(result_metrics[\"accuracy\"])\n",
    "        summary_results[ai_mode][\"macro_precision\"].append(result_metrics[\"macro_precision\"])\n",
    "        summary_results[ai_mode][\"weighted_precision\"].append(result_metrics[\"weighted_precision\"])\n",
    "        summary_results[ai_mode][\"macro_recall\"].append(result_metrics[\"macro_recall\"])\n",
    "        summary_results[ai_mode][\"weighted_recall\"].append(result_metrics[\"weighted_recall\"])\n",
    "        summary_results[ai_mode][\"macro_f1\"].append(result_metrics[\"macro_f1\"])\n",
    "        summary_results[ai_mode][\"weighted_f1\"].append(result_metrics[\"weighted_f1\"])\n",
    "        \n",
    "    median_idx = find_median(summary_results[ai_mode][\"accuracy\"])\n",
    "    df_dict[ai_mode][\"median_run\"] = df_dict[ai_mode][f\"answer_{median_idx+1}\"]\n",
    "    summary_results[ai_mode][\"accuracy\"].append(summary_results[ai_mode][\"accuracy\"][median_idx])\n",
    "    summary_results[ai_mode][\"macro_precision\"].append(summary_results[ai_mode][\"macro_precision\"][median_idx])\n",
    "    summary_results[ai_mode][\"weighted_precision\"].append(summary_results[ai_mode][\"weighted_precision\"][median_idx])\n",
    "    summary_results[ai_mode][\"macro_recall\"].append(summary_results[ai_mode][\"macro_recall\"][median_idx])\n",
    "    summary_results[ai_mode][\"weighted_recall\"].append(summary_results[ai_mode][\"weighted_recall\"][median_idx])\n",
    "    summary_results[ai_mode][\"macro_f1\"].append(summary_results[ai_mode][\"macro_f1\"][median_idx])\n",
    "    summary_results[ai_mode][\"weighted_f1\"].append(summary_results[ai_mode][\"weighted_f1\"][median_idx])\n",
    "\n",
    "    df_dict[ai_mode][\"majority_vote\"] = df_dict[ai_mode][[f\"answer_{idx+1}\" for idx in range(5)]].apply(lambda x: majority_vote(x), axis = 1)\n",
    "    majority_metrics = evaluate_results(df_dict[ai_mode][\"human_gt\"], df_dict[ai_mode][\"majority_vote\"])\n",
    "    summary_results[ai_mode][\"accuracy\"].append(majority_metrics[\"accuracy\"])\n",
    "    summary_results[ai_mode][\"macro_precision\"].append(majority_metrics[\"macro_precision\"])\n",
    "    summary_results[ai_mode][\"weighted_precision\"].append(majority_metrics[\"weighted_precision\"])\n",
    "    summary_results[ai_mode][\"macro_recall\"].append(majority_metrics[\"macro_recall\"])\n",
    "    summary_results[ai_mode][\"weighted_recall\"].append(majority_metrics[\"weighted_recall\"])\n",
    "    summary_results[ai_mode][\"macro_f1\"].append(majority_metrics[\"macro_f1\"])\n",
    "    summary_results[ai_mode][\"weighted_f1\"].append(majority_metrics[\"weighted_f1\"])\n",
    "\n",
    "    summary_results[ai_mode][\"labels\"] = [\n",
    "        \"Run1\", \"Run2\", \"Run3\", \"Run4\", \"Run5\", \"median_run\", \"Majority Vote\"\n",
    "    ]\n",
    "    \n",
    "    matching_dict[f\"{ai_mode}_median\"] = df_dict[ai_mode][\"median_run\"] == df_dict[ai_mode][\"human_gt\"]\n",
    "    matching_dict[f\"{ai_mode}_majorityvote\"] = df_dict[ai_mode][\"majority_vote\"] == df_dict[ai_mode][\"human_gt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ai_mode, metrics in summary_results.items():\n",
    "    for label, accuracy, macro_precision, weighted_precision, macro_recall, weighted_recall, macro_f1, weighted_f1 \\\n",
    "        in zip(\n",
    "            metrics[\"labels\"],\n",
    "            metrics[\"accuracy\"],\n",
    "            metrics[\"macro_precision\"],\n",
    "            metrics[\"weighted_precision\"],\n",
    "            metrics[\"macro_recall\"],\n",
    "            metrics[\"weighted_recall\"],\n",
    "            metrics[\"macro_f1\"],\n",
    "            metrics[\"weighted_f1\"]\n",
    "        ):\n",
    "            summaries.append(\n",
    "                {\n",
    "                    'respondent': f\"{ai_mode}_{label}\",\n",
    "                    'accuracy': accuracy,\n",
    "                    'macro_precision': macro_precision,\n",
    "                    'weighted_precision': weighted_precision,\n",
    "                    'macro_recall': macro_recall,\n",
    "                    'weighted_recall': weighted_recall,\n",
    "                    'macro_f1': macro_f1,\n",
    "                    'weighted_f1': weighted_f1\n",
    "                }\n",
    "            )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "summary_df.to_csv(\n",
    "    os.path.join(save_folder, \"result_summaries_3classes_updated.csv\"),\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondents = list(matching_dict.keys())\n",
    "mcnemar_matrix = [[None]*len(respondents) for _ in range(len(respondents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0, 0], [0, 0]]\n",
    "            confusion_matrix[0][0] = (matching_dict[respondents[row_idx]] & matching_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (matching_dict[respondents[row_idx]] & ~matching_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~matching_dict[respondents[row_idx]] & matching_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~matching_dict[respondents[row_idx]] & ~matching_dict[respondents[col_idx]]).sum()\n",
    "            midp_mcnemar_pvalue = calculate_midp_mcnemar(confusion_matrix)\n",
    "            mcnemar_matrix[row_idx][col_idx] = midp_mcnemar_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_df = pd.DataFrame(mcnemar_matrix,\n",
    "                          columns=respondents,index=respondents)\n",
    "mcnemar_df.to_csv(\n",
    "    os.path.join(save_folder, \"mcnemar_3classes.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\"NoRAG\", \"BaseRAG\", \"CombinedRAG\"]\n",
    "run_labels = [\n",
    "    'answer_1', 'answer_2', 'answer_3', 'answer_4', 'answer_5',\n",
    "    'median_run', 'majority_vote'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(os.path.join(save_folder, \"pairwise_cohen_kappa.xlsx\")) as writer:\n",
    "\n",
    "    for setting in settings:\n",
    "        row_respondents = [f\"{setting}_{label}\" for label in run_labels]\n",
    "        row_data = [df_dict[setting][label] for label in run_labels]\n",
    "        column_respondents = [f\"{setting} Reorder_{label}\" for label in run_labels]\n",
    "        col_data = [df_dict[f\"{setting} Reorder\"][label] for label in run_labels]\n",
    "        cohen_kappa_matrix = [[None]*len(column_respondents)\n",
    "                            for _ in range(len(row_respondents))]\n",
    "        \n",
    "        for row_idx in range(len(row_respondents)):\n",
    "            for col_idx in range(len(column_respondents)):\n",
    "                row_response = row_data[row_idx]\n",
    "                col_response = col_data[col_idx]\n",
    "                cohen_kappa_matrix[row_idx][col_idx] = cohen_kappa_score(row_response, col_response)\n",
    "                \n",
    "        cohen_kappa_df = pd.DataFrame(\n",
    "            cohen_kappa_matrix, \n",
    "            columns=column_respondents, index=row_respondents\n",
    "            )\n",
    "        \n",
    "        cohen_kappa_df.to_excel(writer, sheet_name=setting, engine='xlsxwriter')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fleiss Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_settings = [\n",
    "    [\"NoRAG\"],\n",
    "    [\"NoRAG Reorder\"],\n",
    "    [\"NoRAG\", \"NoRAG Reorder\"],\n",
    "    [\"BaseRAG\"],\n",
    "    [\"BaseRAG Reorder\"],\n",
    "    [\"BaseRAG\", \"BaseRAG Reorder\"],\n",
    "    [\"CombinedRAG\"],\n",
    "    [\"CombinedRAG Reorder\"],\n",
    "    [\"CombinedRAG\", \"CombinedRAG Reorder\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa_dict = {}\n",
    "\n",
    "for ai_df_names in ai_settings:\n",
    "    df_list = [df_dict[ai_df_name][[f\"answer_{idx+1}\" for idx in range(5)]] for ai_df_name in ai_df_names]\n",
    "    agg = irr.aggregate_raters(pd.concat(df_list, axis=1))\n",
    "    ai_mode = \" + \".join(ai_df_names)\n",
    "    fleiss_kappa_dict[ai_mode] = irr.fleiss_kappa(agg[0], method='fleiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_kappa_df = pd.DataFrame(\n",
    "    {\n",
    "        \"ai_mode\": list(fleiss_kappa_dict.keys()),\n",
    "        \"fleiss_kappa_score\": list(fleiss_kappa_dict.values())\n",
    "    }\n",
    ")\n",
    "\n",
    "fleiss_kappa_df.to_csv(\n",
    "    os.path.join(save_folder, \"fleiss_kappa.csv\"),\n",
    "    index = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
