{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dict = {\"UA\": 0, \"MBA\": 1, \"UNA\": 2, \"ICI\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = result_df[\"Approp Score\"].replace(classification_dict)\n",
    "labels = result_df[\"ANSKEY1\"].replace(classification_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = round(accuracy_score(preds, labels) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(result_df):\n",
    "    result_df.columns = result_df.columns.str.strip()\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY APPROPRIATE\", \"UA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY NOT APPROPRIATE\", \"UNA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"MAY BE APPROPRIATE\", \"MBA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"INSUFFICIENT INFORMATION\", \"ICI\")\n",
    "    result_df[\"ANSKEY1\"] = result_df[\"ANSKEY1\"].str.strip()\n",
    "    return result_df\n",
    "    \n",
    "def evaluate_results(result_df):\n",
    "    result_df = preprocess(result_df)\n",
    "    assert len(result_df) == 70, \"Number of testcases must be 70.\"\n",
    "\n",
    "    preds = result_df[\"Approp Score\"].replace(classification_dict)\n",
    "    labels = result_df[\"ANSKEY1\"].replace(classification_dict)\n",
    "\n",
    "    accuracy = round(accuracy_score(preds, labels) * 100, 3)\n",
    "    \n",
    "    df = result_df.groupby([\"Approp Score\", \"ANSKEY1\"])[\"Match\"].count().reset_index()\n",
    "    match_df = df[df[\"Approp Score\"] == df[\"ANSKEY1\"]][[\"Approp Score\", \"Match\"]]\n",
    "    \n",
    "    precision_df = df.groupby(\"Approp Score\")[\"Match\"].sum().reset_index().rename(columns={\"Match\": \"Total\"})\n",
    "    precision_df = precision_df.merge(match_df, on=[\"Approp Score\"])\n",
    "    precision_df[\"Precision\"] = round(precision_df[\"Match\"] / precision_df[\"Total\"] * 100, 3)\n",
    "    \n",
    "    recall_df = df.groupby(\"ANSKEY1\")[\"Match\"].sum().reset_index().rename(columns={\"Match\": \"Total\"})\n",
    "    recall_df = recall_df.merge(match_df, left_on=[\"ANSKEY1\"], right_on=[\"Approp Score\"])\n",
    "    recall_df[\"Recall\"] = round(recall_df[\"Match\"] / recall_df[\"Total\"] * 100, 3)\n",
    "    \n",
    "    try:\n",
    "        precision_ua = precision_df[precision_df[\"Approp Score\"] == \"UA\"][\"Precision\"].values[0]\n",
    "    except:\n",
    "        precision_ua = 0\n",
    "    \n",
    "    try:\n",
    "        precision_mba = precision_df[precision_df[\"Approp Score\"] == \"MBA\"][\"Precision\"].values[0]\n",
    "    except:\n",
    "        precision_mba = 0\n",
    "        \n",
    "    try:\n",
    "        precision_una = precision_df[precision_df[\"Approp Score\"] == \"UNA\"][\"Precision\"].values[0]\n",
    "    except:\n",
    "        precision_una = 0\n",
    "        \n",
    "    try:\n",
    "        precision_ici = precision_df[precision_df[\"Approp Score\"] == \"ICI\"][\"Precision\"].values[0]\n",
    "    except:\n",
    "        precision_ici = 0\n",
    "    \n",
    "    try:\n",
    "        recall_ua = recall_df[recall_df[\"ANSKEY1\"] == \"UA\"][\"Recall\"].values[0]\n",
    "    except:\n",
    "        recall_ua = 0\n",
    "    try:\n",
    "        recall_mba = recall_df[recall_df[\"ANSKEY1\"] == \"MBA\"][\"Recall\"].values[0]\n",
    "    except:\n",
    "        recall_mba = 0\n",
    "    try:\n",
    "        recall_una = recall_df[recall_df[\"ANSKEY1\"] == \"UNA\"][\"Recall\"].values[0]\n",
    "    except:\n",
    "        recall_una = 0\n",
    "    try:\n",
    "        recall_ici = recall_df[recall_df[\"ANSKEY1\"] == \"ICI\"][\"Recall\"].values[0]\n",
    "    except:\n",
    "        recall_ici = 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_ua\": precision_ua,\n",
    "        \"precision_mba\": precision_mba,\n",
    "        \"precision_una\": precision_una,\n",
    "        \"precision_ici\": precision_ici,\n",
    "        \"recall_ua\": recall_ua,\n",
    "        \"recall_mba\": recall_mba,\n",
    "        \"recall_una\": recall_una,\n",
    "        \"recall_ici\": recall_ici \n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human_CLPRad.csv\n",
      "Human_HiokRes.csv\n",
      "Human_KGYRad.csv\n",
      "Human_NYHOrtho.csv\n",
      "Human_SudRes.csv\n",
      "Naive_RAG_LLM.csv\n",
      "No_RAG_LLM.csv\n",
      "Optimized_cLLM.csv\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "answer_files = os.listdir(DATA_DIR)\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    print(answer_file)\n",
    "    summary = {\"respondent\": answer_file.split(\".\")[0]}\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = pd.read_csv(answer_path)\n",
    "    evaluations = evaluate_results(result_df)\n",
    "    summary.update(evaluations)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summaries)\n",
    "summary_df.to_csv(\"result_summaries.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: Human_CLPRad.csv\n"
     ]
    }
   ],
   "source": [
    "answer_matching_df = None\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = preprocess(pd.read_csv(answer_path))\n",
    "    if answer_matching_df is None:\n",
    "        print('master:', answer_file)\n",
    "        answer_matching_df = result_df[[\"Clinical File\", \"ANSKEY1\", \"Approp Score\"]]\n",
    "        answer_matching_df = answer_matching_df.rename(columns={\"ANSKEY1\": \"Ground Truth\", \"Approp Score\": answer_file[:-4]})\n",
    "        answer_matching_df[answer_file[:-4]] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    else:\n",
    "        answer_matching_df[answer_file[:-4]] = result_df[\"Approp Score\"]\n",
    "        answer_matching_df[answer_file[:-4]] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    # Check groundtruth\n",
    "    match_gt = (answer_matching_df[\"Ground Truth\"] == result_df[\"ANSKEY1\"]).sum()\n",
    "    if match_gt != 70:\n",
    "        print(answer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "respondents = [answer_file[:-4] for answer_file in answer_files]\n",
    "mcnemar_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "            mcnemar_matrix[row_idx][col_idx] = round(mcnemar_results.pvalue, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human_CLPRad</th>\n",
       "      <th>Human_HiokRes</th>\n",
       "      <th>Human_KGYRad</th>\n",
       "      <th>Human_NYHOrtho</th>\n",
       "      <th>Human_SudRes</th>\n",
       "      <th>Naive_RAG_LLM</th>\n",
       "      <th>No_RAG_LLM</th>\n",
       "      <th>Optimized_cLLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Human_CLPRad</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02006</td>\n",
       "      <td>0.09625</td>\n",
       "      <td>0.01067</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00107</td>\n",
       "      <td>0.04329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human_HiokRes</th>\n",
       "      <td>0.02006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.01916</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>0.22952</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human_KGYRad</th>\n",
       "      <td>0.09625</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.06357</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human_NYHOrtho</th>\n",
       "      <td>0.01067</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01612</td>\n",
       "      <td>0.01612</td>\n",
       "      <td>0.32694</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human_SudRes</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.01916</td>\n",
       "      <td>0.06357</td>\n",
       "      <td>0.01612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>0.02896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_RAG_LLM</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.01612</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.01690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No_RAG_LLM</th>\n",
       "      <td>0.00107</td>\n",
       "      <td>0.22952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32694</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optimized_cLLM</th>\n",
       "      <td>0.04329</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.69004</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.02896</td>\n",
       "      <td>0.01690</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Human_CLPRad  Human_HiokRes  Human_KGYRad  Human_NYHOrtho  \\\n",
       "Human_CLPRad             NaN        0.02006       0.09625         0.01067   \n",
       "Human_HiokRes        0.02006            NaN       0.00020         1.00000   \n",
       "Human_KGYRad         0.09625        0.00020           NaN         0.00002   \n",
       "Human_NYHOrtho       0.01067        1.00000       0.00002             NaN   \n",
       "Human_SudRes         1.00000        0.01916       0.06357         0.01612   \n",
       "Naive_RAG_LLM        1.00000        0.00720       0.10775         0.01612   \n",
       "No_RAG_LLM           0.00107        0.22952       0.00000         0.32694   \n",
       "Optimized_cLLM       0.04329        0.00002       0.69004         0.00001   \n",
       "\n",
       "                Human_SudRes  Naive_RAG_LLM  No_RAG_LLM  Optimized_cLLM  \n",
       "Human_CLPRad         1.00000        1.00000     0.00107         0.04329  \n",
       "Human_HiokRes        0.01916        0.00720     0.22952         0.00002  \n",
       "Human_KGYRad         0.06357        0.10775     0.00000         0.69004  \n",
       "Human_NYHOrtho       0.01612        0.01612     0.32694         0.00001  \n",
       "Human_SudRes             NaN        1.00000     0.00119         0.02896  \n",
       "Naive_RAG_LLM        1.00000            NaN     0.00009         0.01690  \n",
       "No_RAG_LLM           0.00119        0.00009         NaN         0.00000  \n",
       "Optimized_cLLM       0.02896        0.01690     0.00000             NaN  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcnemar_df = pd.DataFrame(mcnemar_matrix, columns=respondents, index=respondents)\n",
    "mcnemar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "respondents = [answer_file[:-4] for answer_file in answer_files]\n",
    "fisher_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            odd_ratio, p_value = fisher_exact(confusion_matrix)\n",
    "            if p_value < 0.05:\n",
    "                print()\n",
    "            fisher_matrix[row_idx][col_idx] = round(p_value, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, 0.44623, 0.00762, 0.3035, 0.00251, 0.78536, 0.79731, 0.73144],\n",
       " [0.44623, None, 0.76763, 0.47274, 0.04339, 0.00032, 0.02644, 1.0],\n",
       " [0.00762, 0.76763, None, 0.03896, 0.00943, 0.52548, 0.76443, 0.10478],\n",
       " [0.3035, 0.47274, 0.03896, None, 0.20361, 0.20361, 0.04964, 1.0],\n",
       " [0.00251, 0.04339, 0.00943, 0.20361, None, 0.10269, 0.79496, 1.0],\n",
       " [0.78536, 0.00032, 0.52548, 0.20361, 0.10269, None, 0.00346, 0.15897],\n",
       " [0.79731, 0.02644, 0.76443, 0.04964, 0.79496, 0.00346, None, 0.04137],\n",
       " [0.73144, 1.0, 0.10478, 1.0, 1.0, 0.15897, 0.04137, None]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fisher_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
