{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(result_df):\n",
    "    result_df.columns = result_df.columns.str.strip()\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY APPROPRIATE\", \"UA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"USUALLY NOT APPROPRIATE\", \"UNA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"MAY BE APPROPRIATE\", \"MBA\")\n",
    "    result_df[\"Approp Score\"] = result_df[\"Approp Score\"].replace(\"INSUFFICIENT INFORMATION\", \"ICI\")\n",
    "    result_df[\"ANSKEY1\"] = result_df[\"ANSKEY1\"].str.strip()\n",
    "    return result_df\n",
    "    \n",
    "def evaluate_results(result_df, classification_dict):\n",
    "    result_df = preprocess(result_df)\n",
    "    assert len(result_df) == 70, \"Number of testcases must be 70.\"\n",
    "\n",
    "    preds = result_df[\"Approp Score\"].replace(classification_dict)\n",
    "    labels = result_df[\"ANSKEY1\"].replace(classification_dict)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_precision = precision_score(labels, preds, average = \"macro\")\n",
    "    weighted_precision = precision_score(labels, preds, average = \"weighted\")\n",
    "    macro_recall = recall_score(labels, preds, average = \"macro\")\n",
    "    weighted_recall = recall_score(labels, preds, average = \"weighted\")\n",
    "    macro_f1 = f1_score(labels, preds, average = \"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average = \"weighted\")\n",
    "    cohen_kappa = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"cohen_kappa\": cohen_kappa\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Labels: UA, MBA, UNA, ICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dict = {\"UA\": 0, \"MBA\": 1, \"UNA\": 2, \"ICI\": 3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_files = ['Human_rad1(CLP).csv',\n",
    " 'Human_rad2(KGY).csv',\n",
    " 'Human_trainee1(CHY).csv',\n",
    " 'Human_trainee2(SUD).csv',\n",
    " 'Human_ortho(NYH).csv',\n",
    " 'Non_RAG_LLM.csv',\n",
    " 'Naive_RAG_cLLM.csv',\n",
    " 'Optimized_cLLM.csv'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human_rad1(CLP).csv\n",
      "Human_rad2(KGY).csv\n",
      "Human_trainee1(CHY).csv\n",
      "Human_trainee2(SUD).csv\n",
      "Human_ortho(NYH).csv\n",
      "Non_RAG_LLM.csv\n",
      "Naive_RAG_cLLM.csv\n",
      "Optimized_cLLM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    print(answer_file)\n",
    "    summary = {\"respondent\": answer_file.split(\".\")[0]}\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = pd.read_csv(answer_path)\n",
    "    evaluations = evaluate_results(result_df, classification_dict)\n",
    "    summary.update(evaluations)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summaries)\n",
    "summary_df.to_csv(\"result_summaries_4classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = preprocess(pd.read_csv(answer_path))\n",
    "    # result_df = result_df.replace({\"ANSKEY1\": classification_dict, \"Approp Score\": classification_dict})\n",
    "    count_df = result_df.groupby(\"ANSKEY1\")[\"Approp Score\"].value_counts().reset_index(name=\"count\")\n",
    "    count_df.to_csv(os.path.join(\".\", \"data\", \"4class_\"+answer_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: Human_rad1(CLP).csv\n"
     ]
    }
   ],
   "source": [
    "answer_matching_df = None\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = preprocess(pd.read_csv(answer_path))\n",
    "    if answer_matching_df is None:\n",
    "        print('master:', answer_file)\n",
    "        answer_matching_df = result_df[[\"Clinical File\", \"ANSKEY1\", \"Approp Score\"]]\n",
    "        answer_matching_df = answer_matching_df.rename(columns={\"ANSKEY1\": \"Ground Truth\", \"Approp Score\": answer_file[:-4]})\n",
    "        answer_matching_df[answer_file[:-4]+\"_match\"] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    else:\n",
    "        answer_matching_df[answer_file[:-4]] = result_df[\"Approp Score\"]\n",
    "        answer_matching_df[answer_file[:-4]+\"_match\"] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    # Check groundtruth\n",
    "    match_gt = (answer_matching_df[\"Ground Truth\"] == result_df[\"ANSKEY1\"]).sum()\n",
    "    if match_gt != 70:\n",
    "        print(answer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_matching_df.to_csv(\"answers_matching_4classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "respondents = [answer_file[:-4]+\"_match\" for answer_file in answer_files]\n",
    "mcnemar_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "            mcnemar_matrix[row_idx][col_idx] = round(mcnemar_results.pvalue, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_df = pd.DataFrame(mcnemar_matrix,\n",
    "                          columns=[answer_file[:-4] for answer_file in answer_files],\n",
    "                          index=[answer_file[:-4] for answer_file in answer_files])\n",
    "mcnemar_df.to_csv(\"mcnemar_4classes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Exact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "respondents = [answer_file[:-4]+\"_match\" for answer_file in answer_files]\n",
    "fisher_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            odd_ratio, p_value = fisher_exact(confusion_matrix)\n",
    "            if p_value < 0.05:\n",
    "                print()\n",
    "            fisher_matrix[row_idx][col_idx] = round(p_value, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_df = pd.DataFrame(fisher_matrix,\n",
    "                         columns=[answer_file[:-4] for answer_file in answer_files],\n",
    "                         index=[answer_file[:-4] for answer_file in answer_files])\n",
    "fisher_df.to_csv(\"fisher_4classes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown by classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Labels: UA/MBA, UNA, ICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_dict = {\"UA\": 0, \"MBA\": 0, \"UNA\": 1, \"ICI\": 2}\n",
    "classification_dict = {\"UA\": \"UA/MBA\", \"MBA\": \"UA/MBA\", \"UNA\": \"UNA\", \"ICI\": \"ICI\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UA': 'UA/MBA', 'MBA': 'UA/MBA', 'UNA': 'UNA', 'ICI': 'ICI'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human_rad1(CLP).csv\n",
      "Human_rad2(KGY).csv\n",
      "Human_trainee1(CHY).csv\n",
      "Human_trainee2(SUD).csv\n",
      "Human_ortho(NYH).csv\n",
      "Non_RAG_LLM.csv\n",
      "Naive_RAG_cLLM.csv\n",
      "Optimized_cLLM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    print(answer_file)\n",
    "    summary = {\"respondent\": answer_file.split(\".\")[0]}\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = pd.read_csv(answer_path)\n",
    "    evaluations = evaluate_results(result_df, classification_dict)\n",
    "    summary.update(evaluations)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = pd.read_csv(answer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summaries)\n",
    "summary_df.to_csv(\"result_summaries_3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = preprocess(pd.read_csv(answer_path))\n",
    "    result_df = result_df.replace({\"ANSKEY1\": classification_dict, \"Approp Score\": classification_dict})\n",
    "    count_df = result_df.groupby(\"ANSKEY1\")[\"Approp Score\"].value_counts().reset_index(name=\"count\")\n",
    "    count_df.to_csv(os.path.join(\".\", \"data\", \"3class_\"+answer_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McNemar Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: Human_rad1(CLP).csv\n"
     ]
    }
   ],
   "source": [
    "answer_matching_df = None\n",
    "\n",
    "for answer_file in answer_files:\n",
    "    answer_path = os.path.join(DATA_DIR, answer_file)\n",
    "    result_df = preprocess(pd.read_csv(answer_path))\n",
    "    result_df = result_df.replace({\"ANSKEY1\": classification_dict, \"Approp Score\": classification_dict})\n",
    "    if answer_matching_df is None:\n",
    "        print('master:', answer_file)\n",
    "        answer_matching_df = result_df[[\"Clinical File\", \"ANSKEY1\", \"Approp Score\"]]\n",
    "        answer_matching_df = answer_matching_df.rename(columns={\"ANSKEY1\": \"Ground Truth\", \"Approp Score\": answer_file[:-4]})\n",
    "        answer_matching_df[answer_file[:-4]+\"_match\"] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    else:\n",
    "        answer_matching_df[answer_file[:-4]] = result_df[\"Approp Score\"]\n",
    "        answer_matching_df[answer_file[:-4]+\"_match\"] = (answer_matching_df[answer_file[:-4]] == answer_matching_df[\"Ground Truth\"])\n",
    "    # Check groundtruth\n",
    "    match_gt = (answer_matching_df[\"Ground Truth\"] == result_df[\"ANSKEY1\"]).sum()\n",
    "    if match_gt != 70:\n",
    "        print(answer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_matching_df.to_csv(\"answers_matching_3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-7a9368eeefc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# mcnemar_results = mcnemar(confusion_matrix, exact=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# mcnemar_matrix[row_idx][col_idx] = round(mcnemar_results.pvalue, 5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mmidp_mcnemar_pvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_midp_mcnemar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mmcnemar_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmidp_mcnemar_pvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-7a9368eeefc0>\u001b[0m in \u001b[0;36mcalculate_midp_mcnemar\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m ):\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mn1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mstatistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtotal_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from math import comb\n",
    "import numpy as np\n",
    "\n",
    "def calculate_midp_mcnemar(\n",
    "    confusion_matrix\n",
    "):\n",
    "    confusion_matrix = _make_df_square(confusion_matrix)\n",
    "    confusion_matrix = np.confusion_matrix(table, dtype=np.float64)\n",
    "    n1, n2 = confusion_matrix[0, 1], confusion_matrix[1, 0]\n",
    "    statistic = np.minimum(n1, n2)\n",
    "    total_sum = n1 + n2\n",
    "    mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "    mcnemar_pvalue = mcnemar_results.pvalue\n",
    "    midp_mcnemar_pvalue = mcnemar_pvalue - comb(total_sum, statistic) * (0.5 ** total_sum)\n",
    "    return midp_mcnemar_pvalue\n",
    "\n",
    "respondents = [answer_file[:-4]+\"_match\" for answer_file in answer_files]\n",
    "mcnemar_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            # mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "            # mcnemar_matrix[row_idx][col_idx] = round(mcnemar_results.pvalue, 5)\n",
    "            midp_mcnemar_pvalue = calculate_midp_mcnemar(confusion_matrix)\n",
    "            mcnemar_matrix[row_idx][col_idx] = round(midp_mcnemar_pvalue, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_df = pd.DataFrame(mcnemar_matrix,\n",
    "                          columns=[answer_file[:-4] for answer_file in answer_files],\n",
    "                          index=[answer_file[:-4] for answer_file in answer_files])\n",
    "mcnemar_df.to_csv(\"mcnemar_3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Exact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "respondents = [answer_file[:-4]+\"_match\" for answer_file in answer_files]\n",
    "fisher_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~answer_matching_df[respondents[row_idx]] & answer_matching_df[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~answer_matching_df[respondents[row_idx]] & ~answer_matching_df[respondents[col_idx]]).sum()\n",
    "            odd_ratio, p_value = fisher_exact(confusion_matrix)\n",
    "            fisher_matrix[row_idx][col_idx] = round(p_value, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_df = pd.DataFrame(fisher_matrix,\n",
    "                         columns=[answer_file[:-4] for answer_file in answer_files],\n",
    "                         index=[answer_file[:-4] for answer_file in answer_files])\n",
    "fisher_df.to_csv(\"fisher_3classes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df_3 = pd.read_csv(\"answers_matching_3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for human_name in ['Human_rad1(CLP)_match', 'Human_rad2(KGY)_match', 'Human_trainee1(CHY)_match', 'Human_trainee2(SUD)_match', 'Human_ortho(NYH)_match']:\n",
    "    print(human_name)\n",
    "    sub_df = match_df_3[[\"Ground Truth\", human_name]]\n",
    "    print(sub_df.groupby(\"Ground Truth\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = match_df_3[[\"Human_trainee2(SUD)_match\", \"Optimized_cLLM_match\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Human_trainee2(SUD)_match  Optimized_cLLM_match\n",
       "False                      True                    14\n",
       "                           False                    3\n",
       "True                       True                    48\n",
       "                           False                    5\n",
       "Name: Optimized_cLLM_match, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_df.groupby(\"Human_trainee2(SUD)_match\")[\"Optimized_cLLM_match\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
