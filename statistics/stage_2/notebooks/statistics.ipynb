{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from math import comb\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\", \"answers\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = {\n",
    "    \"CHY\": {\"Seniority\": \"Novice\", \"Answers\": \"CHY- Senior Resident radiologist.csv\"},\n",
    "    \"CLP\": {\"Seniority\": \"Expert\", \"Answers\": \"CLP- MSK Radiologist.csv\"},\n",
    "    \"Gita\": {\"Seniority\": \"Expert\", \"Answers\": \"Gita- MSK Radiologist.csv\"},\n",
    "    \"Sud\": {\"Seniority\": \"Novice\", \"Answers\": \"Sud- Resident radiologist.csv\"},\n",
    "    \"YH\": {\"Seniority\": \"Novice\", \"Answers\": \"Yeong Huei- Ortho.csv\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_list = []\n",
    "correctness_dict = {}\n",
    "respondents = [\"ai\"]\n",
    "scores = {\n",
    "    \"participant\": [],\n",
    "    \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []\n",
    "}\n",
    "counts = {\n",
    "    \"participant\": [],\n",
    "    \"Total\": [], \"UA/MBA\": [], \"UNA\": [], \"ICI\": []\n",
    "}\n",
    "\n",
    "for participant in participants:\n",
    "    answer_path = os.path.join(DATA_DIR, participants[participant][\"Answers\"])\n",
    "    answer_df = pd.read_csv(answer_path)\n",
    "\n",
    "    rename_dict = {\n",
    "        \"USUALLY APPROPRIATE\": \"UA/MBA\",\n",
    "        \"MAY BE APPROPRIATE\": \"UA/MBA\",\n",
    "        \"MBA\": \"UA/MBA\", \"UA\": \"UA/MBA\",\n",
    "        \"USUALLY NOT APPROPRIATE\": \"UNA\",\n",
    "        \"INSUFFICIENT INFORMATION\": \"ICI\"\n",
    "    }\n",
    "\n",
    "    ALLOWED_CLASSES = [\"UA/MBA\", \"UNA\", \"ICI\", \"NO ANSWER\"]\n",
    "    answer_df[\"GPT Answer\"] = answer_df[\"GPT Answer\"].str.strip().str.upper().replace(rename_dict)\n",
    "    answer_df[\"Human-AI Answer\"] = answer_df[\"Human-AI Answer\"].str.strip().str.upper().replace(rename_dict)\n",
    "    answer_df[\"Correct Answer\"] = answer_df[\"Correct Answer\"].str.strip().str.upper().replace(rename_dict)\n",
    "    answer_df[\"Human Answer\"] = answer_df[\"Human Answer\"].str.strip().str.upper().replace(rename_dict)\n",
    "\n",
    "    answer_df[\"GPT Answer\"] = answer_df[\"GPT Answer\"].fillna(\"NO ANSWER\")\n",
    "    answer_df[\"Human-AI Answer\"] = answer_df[\"Human-AI Answer\"].fillna(\"NO ANSWER\")\n",
    "    answer_df[\"Correct Answer\"] = answer_df[\"Correct Answer\"].fillna(\"NO ANSWER\")\n",
    "    answer_df[\"Human Answer\"] = answer_df[\"Human Answer\"].fillna(\"NO ANSWER\")\n",
    "\n",
    "\n",
    "    for category in answer_df[\"GPT Answer\"].unique():\n",
    "        if category not in ALLOWED_CLASSES:\n",
    "            print(f\"{participant}-GPT-answer: {category} not in allowed class\")\n",
    "\n",
    "    for category in answer_df[\"Human-AI Answer\"].unique():\n",
    "        if category not in ALLOWED_CLASSES:\n",
    "            print(f\"{participant}-HumanAI-answer: {category} not in allowed class\")\n",
    "            \n",
    "    for category in answer_df[\"Correct Answer\"].unique():\n",
    "        if category not in ALLOWED_CLASSES:\n",
    "            print(f\"{participant}-Correct-answer: {category} not in allowed class\")\n",
    "            \n",
    "    for category in answer_df[\"Human Answer\"].unique():\n",
    "        if category not in ALLOWED_CLASSES:\n",
    "            print(f\"{participant}-Human-answer: {category} not in allowed class\")\n",
    "            \n",
    "    ground_truths = answer_df[\"Correct Answer\"]\n",
    "    ai_correctness = (answer_df[\"GPT Answer\"] == answer_df[\"Correct Answer\"]).astype(int).values\n",
    "    human_correctness = (answer_df[\"Human Answer\"] == answer_df[\"Correct Answer\"]).astype(int).values\n",
    "    human_ai_correctness = (answer_df[\"Human-AI Answer\"] == answer_df[\"Correct Answer\"]).astype(int).values\n",
    "\n",
    "    correctness_dict[\"ai\"] = ai_correctness\n",
    "    correctness_dict[f\"{participant}_noai\"] = human_correctness\n",
    "    correctness_dict[f\"{participant}_withai\"] = human_ai_correctness\n",
    "    respondents.extend([f\"{participant}_noai\", f\"{participant}_withai\"])\n",
    "\n",
    "    human_only_df = pd.DataFrame(\n",
    "        {\n",
    "        \"question_no\": list(range(1, 71)),\n",
    "        \"participant\": participant,\n",
    "        \"seniority\": participants[participant][\"Seniority\"],\n",
    "        \"ai_correctness\": ai_correctness,\n",
    "        \"human_correctness\": human_correctness,\n",
    "        \"ai_usage\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    humanai_df = pd.DataFrame(\n",
    "        {\n",
    "        \"question_no\": list(range(1, 71)),\n",
    "        \"participant\": participant,\n",
    "        \"seniority\": participants[participant][\"Seniority\"],\n",
    "        \"ai_correctness\": ai_correctness,\n",
    "        \"human_correctness\": human_ai_correctness,\n",
    "        \"ai_usage\": 1,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    all_df_list.extend([human_only_df, humanai_df])\n",
    "    \n",
    "    from copy import deepcopy\n",
    "    answer_number_df = deepcopy(answer_df)\n",
    "    CLASS2IDX = {\"UA/MBA\": 0, \"UNA\": 1, \"ICI\": 2, 'NO ANSWER': 3}\n",
    "    answer_number_df[\"GPT Answer\"] = answer_number_df[\"GPT Answer\"].replace(CLASS2IDX)\n",
    "    answer_number_df[\"Human-AI Answer\"] = answer_number_df[\"Human-AI Answer\"].replace(CLASS2IDX)\n",
    "    answer_number_df[\"Correct Answer\"] = answer_number_df[\"Correct Answer\"].replace(CLASS2IDX)\n",
    "    answer_number_df[\"Human Answer\"] = answer_number_df[\"Human Answer\"].replace(CLASS2IDX)\n",
    "    \n",
    "    scores[\"participant\"].append(participant)\n",
    "    scores[\"accuracy\"].append(accuracy_score(answer_number_df[\"Correct Answer\"], answer_number_df[\"Human-AI Answer\"]))\n",
    "    scores[\"precision\"].append(precision_score(answer_number_df[\"Correct Answer\"], answer_number_df[\"Human-AI Answer\"],\n",
    "                                               labels = [0, 1, 2], average = \"macro\"))\n",
    "    scores[\"recall\"].append(recall_score(answer_number_df[\"Correct Answer\"], answer_number_df[\"Human-AI Answer\"],\n",
    "                                         labels = [0, 1, 2], average = \"macro\"))\n",
    "    scores[\"f1\"].append(f1_score(answer_number_df[\"Correct Answer\"], answer_number_df[\"Human-AI Answer\"],\n",
    "                                 labels = [0, 1, 2], average = \"macro\"))\n",
    "    \n",
    "    correct_df = answer_df[answer_df[\"Correct Answer\"] == answer_df[\"Human-AI Answer\"]]\n",
    "    count_dict = correct_df.groupby(\"Correct Answer\")[\"Human-AI Answer\"].count().to_dict()\n",
    "    counts[\"participant\"].append(participant)\n",
    "    counts[\"Total\"].append(count_dict[\"UA/MBA\"] + count_dict[\"UNA\"] + count_dict[\"ICI\"])\n",
    "    counts[\"UA/MBA\"].append(count_dict[\"UA/MBA\"])\n",
    "    counts[\"UNA\"].append(count_dict[\"UNA\"])\n",
    "    counts[\"ICI\"].append(count_dict[\"ICI\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(counts).to_csv(os.path.join(ARTIFACT_DIR, \"counts.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial proportion confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "lci = []\n",
    "uci = []\n",
    "\n",
    "for respondent in respondents:\n",
    "    correctness_list = correctness_dict[respondent]\n",
    "    wilson_stats = proportion_confint(correctness_list.sum(), len(correctness_list), alpha=alpha, method=\"wilson\")\n",
    "    lci.append(wilson_stats[0])\n",
    "    uci.append(wilson_stats[1])\n",
    "    \n",
    "wilson_df = pd.DataFrame(\n",
    "    {\n",
    "        \"respondent\": respondents,\n",
    "        \"lci\": lci, \"uci\": uci\n",
    "    }\n",
    ")\n",
    "wilson_df[\"accuracy\"] = (wilson_df[\"lci\"] + wilson_df[\"uci\"])/2\n",
    "wilson_df.to_csv(os.path.join(ARTIFACT_DIR, \"ci.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Mcnemar Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_df_square(table):\n",
    "    \n",
    "    if not isinstance(table, pd.DataFrame):\n",
    "        return table\n",
    "\n",
    "    if not table.index.equals(table.columns):\n",
    "        ix = list(set(table.index) | set(table.columns))\n",
    "        ix.sort()\n",
    "        table = table.reindex(index=ix, columns=ix, fill_value=0)\n",
    "\n",
    "    table = table.reindex(table.columns)\n",
    "\n",
    "    return table\n",
    "\n",
    "def calculate_midp_mcnemar(\n",
    "    confusion_matrix\n",
    "):\n",
    "    table = _make_df_square(confusion_matrix)\n",
    "    table = np.asarray(table, dtype=np.int)\n",
    "    n1, n2 = table[0, 1], table[1, 0]\n",
    "    statistic = np.minimum(n1, n2)\n",
    "    total_sum = n1 + n2\n",
    "    mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "    mcnemar_pvalue = mcnemar_results.pvalue\n",
    "    midp_mcnemar_pvalue = mcnemar_pvalue - comb(total_sum, statistic) * (0.5 ** total_sum)\n",
    "    return midp_mcnemar_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_matrix = [[None]*len(respondents) for _ in range(len(respondents))]\n",
    "\n",
    "for row_idx in range(len(respondents)):\n",
    "    for col_idx in range(len(respondents)):\n",
    "        if row_idx != col_idx:\n",
    "            confusion_matrix = [[0,0],[0,0]]\n",
    "            confusion_matrix[0][0] = (correctness_dict[respondents[row_idx]] & correctness_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[0][1] = (correctness_dict[respondents[row_idx]] & ~correctness_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][0] = (~correctness_dict[respondents[row_idx]] & correctness_dict[respondents[col_idx]]).sum()\n",
    "            confusion_matrix[1][1] = (~correctness_dict[respondents[row_idx]] & ~correctness_dict[respondents[col_idx]]).sum()\n",
    "            # mcnemar_results = mcnemar(confusion_matrix, exact=True)\n",
    "            # mcnemar_matrix[row_idx][col_idx] = round(mcnemar_results.pvalue, 5)\n",
    "            midp_mcnemar_pvalue = calculate_midp_mcnemar(confusion_matrix)\n",
    "            mcnemar_matrix[row_idx][col_idx] = midp_mcnemar_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_df = pd.DataFrame(mcnemar_matrix,\n",
    "                          columns=respondents, index=respondents)\n",
    "\n",
    "mcnemar_df.to_csv(os.path.join(ARTIFACT_DIR, \"mcnemar\", \"mcnemar_3classes.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Mixed Effects Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(all_df_list)\n",
    "# combined_df.to_csv(os.path.join(ARTIFACT_DIR, \"glmm\", \"mixed_effects_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_no</th>\n",
       "      <th>participant</th>\n",
       "      <th>seniority</th>\n",
       "      <th>ai_correctness</th>\n",
       "      <th>human_correctness</th>\n",
       "      <th>ai_usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CHY</td>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CHY</td>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CHY</td>\n",
       "      <td>Novice</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CHY</td>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CHY</td>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_no participant seniority  ai_correctness  human_correctness  \\\n",
       "0            1         CHY    Novice               1                  1   \n",
       "1            2         CHY    Novice               1                  1   \n",
       "2            3         CHY    Novice               0                  1   \n",
       "3            4         CHY    Novice               1                  0   \n",
       "4            5         CHY    Novice               1                  1   \n",
       "\n",
       "   ai_usage  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"seniority\"] = combined_df[\"seniority\"].replace(\n",
    "    {\"Novice\": 0, \"Expert\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = combined_df[\"human_correctness\"].values\n",
    "fixed_effects = combined_df[[\"seniority\", \"ai_correctness\", \"ai_usage\"]].values\n",
    "random_effects = combined_df[[\"question_no\", \"participant\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.genmod.bayes_mixed_glm import BinomialBayesMixedGLM\n",
    "\n",
    "random = {\"a\": '0 + C(question_no)', \"b\": '0 + C(participant)'}\n",
    "\n",
    "model = BinomialBayesMixedGLM.from_formula(\n",
    "    'human_correctness ~ seniority + ai_usage + ai_correctness',\n",
    "    random,\n",
    "    combined_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.fit_vb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>        <th>Type</th> <th>Post. Mean</th> <th>Post. SD</th>  <th>SD</th>   <th>SD (LB)</th> <th>SD (UB)</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>M</td>    <td>-0.0247</td>   <td>0.0990</td>    <td></td>       <td></td>        <td></td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>seniority</th>        <td>M</td>    <td>0.7634</td>    <td>0.1736</td>    <td></td>       <td></td>        <td></td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ai_usage</th>         <td>M</td>    <td>0.9640</td>    <td>0.1563</td>    <td></td>       <td></td>        <td></td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ai_correctness</th>   <td>M</td>    <td>0.8621</td>    <td>0.1072</td>    <td></td>       <td></td>        <td></td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>a</th>                <td>V</td>    <td>-0.0888</td>   <td>0.0843</td>  <td>0.915</td>  <td>0.773</td>   <td>1.083</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b</th>                <td>V</td>    <td>-1.2065</td>   <td>0.3413</td>  <td>0.299</td>  <td>0.151</td>   <td>0.592</td> \n",
       "</tr>\n",
       "</table><br/>\n",
       "Parameter types are mean structure (M) and variance structure (V)<br/>\n",
       "Variance parameters are modeled as log standard deviations"
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Binomial Mixed GLM Results}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                & Type & Post. Mean & Post. SD &    SD & SD (LB) & SD (UB)  \\\\\n",
       "\\hline\n",
       "Intercept       &    M &    -0.0247 &   0.0990 &       &         &          \\\\\n",
       "seniority       &    M &     0.7634 &   0.1736 &       &         &          \\\\\n",
       "ai\\_usage       &    M &     0.9640 &   0.1563 &       &         &          \\\\\n",
       "ai\\_correctness &    M &     0.8621 &   0.1072 &       &         &          \\\\\n",
       "a               &    V &    -0.0888 &   0.0843 & 0.915 &   0.773 &   1.083  \\\\\n",
       "b               &    V &    -1.2065 &   0.3413 & 0.299 &   0.151 &   0.592  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Parameter types are mean structure (M) and variance structure (V) \\newline \n",
       "Variance parameters are modeled as log standard deviations"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Binomial Mixed GLM Results\n",
       "=============================================================\n",
       "               Type Post. Mean Post. SD   SD  SD (LB) SD (UB)\n",
       "-------------------------------------------------------------\n",
       "Intercept         M    -0.0247   0.0990                      \n",
       "seniority         M     0.7634   0.1736                      \n",
       "ai_usage          M     0.9640   0.1563                      \n",
       "ai_correctness    M     0.8621   0.1072                      \n",
       "a                 V    -0.0888   0.0843 0.915   0.773   1.083\n",
       "b                 V    -1.2065   0.3413 0.299   0.151   0.592\n",
       "=============================================================\n",
       "Parameter types are mean structure (M) and variance structure\n",
       "(V)\n",
       "Variance parameters are modeled as log standard deviations\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
