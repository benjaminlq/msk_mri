{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.schema import Document, MetadataMode\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "\n",
    "from config import MAIN_DIR\n",
    "from utils import (\n",
    "    convert_doc_to_dict,\n",
    "    generate_vectorindex,\n",
    "    load_vectorindex,\n",
    "    query_wrapper,\n",
    "    remove_final_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "FINETUNE_DIR = os.path.join(EMB_DIR, \"finetune\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(EMB_DIR, \"texts.json\"), \"r\") as f:\n",
    "    text_list_by_page = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nodes = []\n",
    "\n",
    "for text in text_list_by_page:\n",
    "    text[\"metadata\"][\"mode\"] = \"text\"\n",
    "    doc = Document(\n",
    "        text=text[\"text\"],\n",
    "        metadata=text[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    page_nodes.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Guideline', 'Variant', 'Appropriateness Category',\n",
    "                   'Scan Order', 'Clinical File']\n",
    "        )\n",
    "\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"Scan Order\"]\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"similarity_top_k\": [3, 5, 7],\n",
    "    \"chunk_size\": [256, 512, 1024]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   2%|‚ñè         | 30/1827 [00:02<02:18, 13.02it/s]"
     ]
    }
   ],
   "source": [
    "for chunk_size in search_space[\"chunk_size\"]:\n",
    "\n",
    "    description = \"FreeText-Chunk_size={}\".format(chunk_size)\n",
    "    save_folder = os.path.join(FINETUNE_DIR, description)\n",
    "    \n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    node_parser = SimpleNodeParser.from_defaults(chunk_size = chunk_size)\n",
    "\n",
    "    text_nodes = node_parser.get_nodes_from_documents(page_nodes)\n",
    "    text_docs = []\n",
    "\n",
    "    for node in text_nodes:\n",
    "        doc = Document(\n",
    "            text=node.text,\n",
    "            metadata=node.metadata,\n",
    "            excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "            excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "            )\n",
    "        text_docs.append(doc)\n",
    "        \n",
    "    text_contents_for_embs = [\n",
    "        text_node.get_content(metadata_mode = MetadataMode.EMBED) \n",
    "        for text_node in text_nodes\n",
    "        ]\n",
    "\n",
    "    text_dicts = [convert_doc_to_dict(doc) for doc in text_docs]\n",
    "    text_embs = embed_model.get_text_embedding_batch(text_contents_for_embs, show_progress=True)\n",
    "    \n",
    "    text_info_for_save = []\n",
    "\n",
    "    for text_dict, text_emb, text_doc in zip(text_dicts, text_embs, text_docs):\n",
    "        text_info_for_save.append({\"text_doc\": text_dict, \"text_emb\": text_emb})\n",
    "        text_doc.embedding = text_emb\n",
    "\n",
    "    with open(os.path.join(save_folder, f\"text-embs-chunk_size={chunk_size}.json\"), \"w\") as f:\n",
    "        json.dump(text_info_for_save, f)\n",
    "        \n",
    "    generate_vectorindex(\n",
    "        embeddings=embed_model,\n",
    "        emb_size=1536,\n",
    "        documents=text_docs,\n",
    "        output_directory=os.path.join(save_folder, \"db\"),\n",
    "        emb_store_type=\"chroma\",\n",
    "        chunk_size=chunk_size,\n",
    "        index_name=\"texts\"\n",
    "    )\n",
    "    \n",
    "    texts_index = load_vectorindex(\n",
    "        db_directory = os.path.join(save_folder, \"db\"),\n",
    "        emb_store_type = \"chroma\", index_name = \"texts\",\n",
    "    )\n",
    "    \n",
    "    for top_k in search_space[\"similarity_top_k\"]:\n",
    "        text_retriever = texts_index.as_retriever(similarity_top_k = top_k)\n",
    "        \n",
    "        retrieval_dataset = {\n",
    "            \"question\": [], \"contexts\": [], \"ground_truths\": []\n",
    "        }\n",
    "\n",
    "        for query, variant, guideline in zip(testcase_df[\"queries\"], testcase_df[\"ACR scenario\"], testcase_df[\"Guideline\"]):\n",
    "            correct_variant = \"Condition: {}\\nPatient Category: {}\".format(guideline, variant)\n",
    "            retrieved_nodes = text_retriever.retrieve(query)\n",
    "            retrieval_dataset[\"question\"].append(query)\n",
    "            retrieval_dataset[\"ground_truths\"].append(correct_variant)\n",
    "            retrieval_dataset[\"contexts\"].append(\n",
    "                [node_with_score.node.text for node_with_score in retrieved_nodes]\n",
    "            )\n",
    "                \n",
    "        with open(os.path.join(save_folder, f\"retrieval_dataset_FreeText-Chunk_size={chunk_size}-K={top_k}.json\"), \"w\") as f:\n",
    "            json.dump(retrieval_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
