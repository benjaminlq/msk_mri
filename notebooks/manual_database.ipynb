{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import re\n",
    "import openai\n",
    "import tiktoken\n",
    "import camelot\n",
    "import faiss\n",
    "\n",
    "from typing import Union, Dict, List, Callable\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.schema import Document\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.schema import MetadataMode, NodeWithScore\n",
    "\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "\n",
    "from custom_storage.document_fn import generate_vectorindex\n",
    "from custom_storage.document_fn import load_vectorindex\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "openai.log = \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"..\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, NodeWithScore):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": doc.score\n",
    "            } \n",
    "    elif isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": \"\"\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"score\": \"None\"\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def filter_by_pages(\n",
    "    doc_list: List[Document],\n",
    "    exclude_info: Dict[str, List]\n",
    ") -> List[Document]:\n",
    "    filtered_list = []\n",
    "    for doc in doc_list:\n",
    "        file_name = doc.metadata[\"file_name\"]\n",
    "        page = doc.metadata[\"page_label\"]\n",
    "        if file_name not in exclude_info.keys():\n",
    "            filtered_list.append(doc)\n",
    "            continue\n",
    "        if int(page) not in exclude_info[file_name]:\n",
    "            filtered_list.append(doc)\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "def count_tokens(\n",
    "    text: str,\n",
    "    tokenizer: Callable = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "):\n",
    "    tokens_sequence = tokenizer.encode(text)\n",
    "    return len(tokens_sequence)\n",
    "\n",
    "def organize_by_files(\n",
    "    doc_list: List[Document]\n",
    "):\n",
    "    doc_dict = {}\n",
    "    for doc in doc_list:\n",
    "        filename = doc.metadata[\"file_name\"]\n",
    "        if filename not in doc_dict:\n",
    "            doc_dict[filename] = [doc]\n",
    "        else:\n",
    "            doc_dict[filename].append(doc)\n",
    "            \n",
    "    return doc_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs before filtering: 546\n",
      "Total number of docs after filtering 395\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "print(\"Total no of docs before filtering:\", len(documents))\n",
    "with open(EXCLUDE_DICT, \"r\") as f:\n",
    "    exclude_pages = json.load(f)\n",
    "documents = filter_by_pages(doc_list=documents, exclude_info=exclude_pages)\n",
    "print(\"Total number of docs after filtering\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_prompt = \"\"\"You are given a text which contains descriptions of patient variants and the corresponding table with the appropriateness of different image procedure and radiation level.\n",
    "Extract only the description of the variant. Output your answer as follow:\n",
    "Variant 1:\n",
    "Variant 2: \n",
    "========\n",
    "EXAMPLE:\n",
    "TEXT: Variant 1:  Chronic ankle pain . Initial imaging . \n",
    "Procedure  Appropriateness Category  Relative Radiation Level  \n",
    "Radiography ankle  Usually Appropriate  ☢ \n",
    "Bone scan ankle Usually Not Appropriate  ☢☢☢ \n",
    "US ankle  Usually Not Appropriate  O \n",
    "CT ankle without IV contrast  Usually Not Appropriate  ☢ \n",
    "CT ankle with IV contrast  Usually Not Appropriate  ☢ \n",
    "Variant 2:  Chronic ankle pain. M ultiple sites of degenerative joint disease in the hind foot detected by \n",
    "ankle radiographs . Next study.  \n",
    "Procedure  Appropriateness Catego ry Relative Radiation Level  \n",
    "Image -guided anesthetic injection ankle and \n",
    "hindfoot  May Be Appropriate  Varies\n",
    "ANSWER:\n",
    "Variant 1: Chronic ankle pain. Initial imaging.\n",
    "Variant 2: Chronic ankle pain. Multiple sites of degenerative joint disease in the hind foot detected by \n",
    "ankle radiographs. Next study.\n",
    "========\n",
    "TEXT: {input_query}\n",
    "\"\"\"\n",
    "\n",
    "EXTRACT_PROMPT_TEMPLATE = PromptTemplate.from_template(extract_prompt)\n",
    "\n",
    "extract_chain = LLMChain(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=256), prompt=EXTRACT_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_list = []\n",
    "text_docs = []\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    page = doc.metadata[\"page_label\"]\n",
    "    filename = doc.metadata[\"file_name\"]\n",
    "    condition = os.path.splitext(filename)[0]\n",
    "    tables = camelot.read_pdf(\n",
    "        os.path.join(DOCUMENT_DIR, filename), pages=page, suppress_stdout=True\n",
    "        )\n",
    "    if not tables:\n",
    "        doc.metadata[\"mode\"] = \"text\"\n",
    "        doc.metadata[\"condition\"] = condition.lower()\n",
    "        doc.excluded_embed_metadata_keys = ['file_name', 'page_label']\n",
    "        doc.excluded_llm_metadata_keys = ['file_name', 'page_label']\n",
    "        text_docs.append(doc)\n",
    "    else:\n",
    "        table_texts = []\n",
    "        text_flag = False\n",
    "        for table in tables:\n",
    "            table_df = table.df\n",
    "            table_df = table_df.rename(columns=table_df.iloc[0])\\\n",
    "                    .rename(columns={\"Radiologic Procedure\": \"Procedure\", \"Rating\": \"Appropriateness Category\"})\\\n",
    "                    .drop(table_df.index[0])\\\n",
    "                    .reset_index(drop=True)\n",
    "            if \"Procedure\" in table_df.columns:\n",
    "                table_df = table_df[[\"Procedure\", \"Appropriateness Category\"]]\n",
    "                table_df[\"Procedure\"] = table_df[\"Procedure\"].str.replace(\"\\n\", \" \")\n",
    "                table_str = table_df.to_markdown(index=False)\n",
    "                table_str = re.sub(r\" +\", \" \", table_str)\n",
    "                table_str = re.sub(r\":-+|-+:\", \"---\", table_str)\n",
    "                table_texts.append(table_str)\n",
    "            else:\n",
    "                print(\"File Name: {}, Page: {}, Columns: {}\"\\\n",
    "                    .format(filename, page, table_df.columns))\n",
    "                text_flag = True\n",
    "        if text_flag:\n",
    "            doc.metadata[\"mode\"] = \"text\"\n",
    "            doc.metadata[\"condition\"] = condition.lower()\n",
    "            doc.excluded_embed_metadata_keys = ['file_name', 'page_label']\n",
    "            doc.excluded_llm_metadata_keys = ['file_name', 'page_label']\n",
    "            text_docs.append(doc)\n",
    "        # else:\n",
    "            # # pattern = r\"Variant ([0-9])+ *:([\\s\\S]+?)Procedure Appropriateness Category Relative Radiation Level\"\n",
    "            # text = extract_chain(doc.text)[\"text\"]\n",
    "            # pattern = r\"Variant ([0-9])+ *:([^\\n]+)\"\n",
    "            # table_infos = re.findall(pattern, text)\n",
    "            # # table_descriptions = [description.strip() for description in table_descriptions]\n",
    "            # assert len(table_texts) == len(table_infos), f\"{table_texts}\\n{table_infos}\"\n",
    "            # for table_text, table_info in zip(table_texts, table_infos):\n",
    "            #     variant_no, table_description = table_info\n",
    "            #     table_list.append(\n",
    "            #         {\n",
    "            #         \"text\": table_text,\n",
    "            #         \"metadata\": {\n",
    "            #             \"mode\": \"tabular\",\n",
    "            #             \"condition\": condition.lower(),\n",
    "            #             \"description\": table_description,\n",
    "            #             \"variant\": variant_no,\n",
    "            #             \"file_name\": filename,\n",
    "            #             \"page_label\": page\n",
    "            #         }\n",
    "            #         }\n",
    "            #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_vector_path = os.path.join(DATA_DIR, \"multimodal\")\n",
    "\n",
    "# if not os.path.exists(multimodal_vector_path):\n",
    "#     os.makedirs(multimodal_vector_path, exist_ok=True)\n",
    "    \n",
    "# with open(os.path.join(multimodal_vector_path, \"tables.json\"), \"w\") as f:\n",
    "#     json.dump(table_list, f)\n",
    "    \n",
    "# text_list = [dict(text=text_doc.text, metadata=text_doc.metadata)\n",
    "#               for text_doc in text_docs]\n",
    "\n",
    "# with open(os.path.join(multimodal_vector_path, \"texts.json\"), \"w\") as f:\n",
    "#     json.dump(text_list, f)\n",
    "\n",
    "with open(os.path.join(multimodal_vector_path, \"tables.json\"), \"r\") as f:\n",
    "    table_list = json.load(f)\n",
    "\n",
    "with open(os.path.join(multimodal_vector_path, \"texts.json\"), \"r\") as f:\n",
    "    text_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS: With Table Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_docs = []\n",
    "for table in table_list:\n",
    "    table[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=table[\"text\"],\n",
    "        metadata=table[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    table_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = []\n",
    "for text in text_list:\n",
    "    text[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=text[\"text\"],\n",
    "        metadata=text[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    text_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1536\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "full_persist_dir = os.path.join(multimodal_vector_path, \"full\")\n",
    "if not os.path.exists(full_persist_dir):\n",
    "    os.makedirs(full_persist_dir, exist_ok=True)\n",
    "\n",
    "# TableStore\n",
    "table_faiss_index = faiss.IndexFlatL2(d)\n",
    "table_service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, chunk_size=1024, chunk_overlap=0\n",
    ")\n",
    "\n",
    "table_vector_store = FaissVectorStore(faiss_index=table_faiss_index)\n",
    "table_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = table_vector_store\n",
    ")\n",
    "\n",
    "table_vector_index = VectorStoreIndex.from_documents(\n",
    "    table_docs, storage_context=table_storage_context, service_context=table_service_context\n",
    "    )\n",
    "\n",
    "table_vector_path = os.path.join(full_persist_dir, \"tables\")\n",
    "table_vector_index.storage_context.persist(table_vector_path)\n",
    "\n",
    "# TextStore\n",
    "text_faiss_index = faiss.IndexFlatL2(d)\n",
    "text_service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, chunk_size=512, chunk_overlap=20\n",
    ")\n",
    "\n",
    "text_vector_store = FaissVectorStore(faiss_index=text_faiss_index)\n",
    "text_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = text_vector_store\n",
    ")\n",
    "\n",
    "text_vector_index = VectorStoreIndex.from_documents(\n",
    "    text_docs, storage_context=text_storage_context, service_context=text_service_context\n",
    "    )\n",
    "\n",
    "text_vector_path = os.path.join(full_persist_dir, \"texts\")\n",
    "text_vector_index.storage_context.persist(text_vector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS: Without Table Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "table_docs = []\n",
    "for table in table_list:\n",
    "    table[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=table[\"text\"],\n",
    "        metadata=table[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    table_docs.append(doc)\n",
    "\n",
    "description_texts = [doc.get_metadata_str(mode=MetadataMode.EMBED) for doc in table_docs]\n",
    "description_embs = embed_model.get_text_embedding_batch(description_texts)\n",
    "\n",
    "for doc, emb in zip(table_docs, description_embs):\n",
    "    doc.embedding = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = []\n",
    "for text in text_list:\n",
    "    text[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=text[\"text\"],\n",
    "        metadata=text[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    text_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1536\n",
    "desc_persist_dir = os.path.join(multimodal_vector_path, \"descriptions\")\n",
    "if not os.path.exists(desc_persist_dir):\n",
    "    os.makedirs(desc_persist_dir, exist_ok=True)\n",
    "\n",
    "# TableStore\n",
    "table_faiss_index = faiss.IndexFlatL2(d)\n",
    "table_service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, chunk_size=1024, chunk_overlap=0\n",
    ")\n",
    "\n",
    "table_vector_store = FaissVectorStore(faiss_index=table_faiss_index)\n",
    "table_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = table_vector_store\n",
    ")\n",
    "\n",
    "table_vector_index = VectorStoreIndex.from_documents(\n",
    "    table_docs, storage_context=table_storage_context, service_context=table_service_context\n",
    "    )\n",
    "\n",
    "table_vector_path = os.path.join(desc_persist_dir, \"tables\")\n",
    "table_vector_index.storage_context.persist(table_vector_path)\n",
    "\n",
    "# TextStore\n",
    "text_faiss_index = faiss.IndexFlatL2(d)\n",
    "text_service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, chunk_size=512, chunk_overlap=20\n",
    ")\n",
    "\n",
    "text_vector_store = FaissVectorStore(faiss_index=text_faiss_index)\n",
    "text_storage_context = StorageContext.from_defaults(\n",
    "    vector_store = text_vector_store\n",
    ")\n",
    "\n",
    "text_vector_index = VectorStoreIndex.from_documents(\n",
    "    text_docs, storage_context=text_storage_context, service_context=text_service_context\n",
    "    )\n",
    "\n",
    "text_vector_path = os.path.join(desc_persist_dir, \"texts\")\n",
    "text_vector_index.storage_context.persist(text_vector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma: Only Table Descriptions & Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "table_docs = []\n",
    "for table in table_list:\n",
    "    table[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=table[\"text\"],\n",
    "        metadata=table[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    table_docs.append(doc)\n",
    "\n",
    "description_texts = [doc.get_metadata_str(mode=MetadataMode.EMBED) for doc in table_docs]\n",
    "description_embs = embed_model.get_text_embedding_batch(description_texts)\n",
    "\n",
    "for doc, emb in zip(table_docs, description_embs):\n",
    "    doc.embedding = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = []\n",
    "for text in text_list:\n",
    "    text[\"metadata\"][\"mode\"] = \"tabular\"\n",
    "    doc = Document(\n",
    "        text=text[\"text\"],\n",
    "        metadata=text[\"metadata\"],\n",
    "        excluded_embed_metadata_keys = ['file_name', 'page_label', 'variant', 'mode'],\n",
    "        excluded_llm_metadata_keys = ['file_name', 'page_label', 'variant']\n",
    "        )\n",
    "    text_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_vector_path = os.path.join(DATA_DIR, \"multimodal-chroma\")\n",
    "desc_persist_dir = os.path.join(multimodal_vector_path, \"descriptions\")\n",
    "if not os.path.exists(desc_persist_dir):\n",
    "    os.makedirs(desc_persist_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Index\n",
    "generate_vectorindex(\n",
    "    embeddings=embed_model,\n",
    "    emb_size=1536,\n",
    "    documents=table_docs,\n",
    "    output_directory=os.path.join(desc_persist_dir, \"tables\"),\n",
    "    emb_store_type=\"chroma\",\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=0,\n",
    "    index_name=\"tables\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-27 16:02:18,999:INFO: Processing documents from provided list.\n",
      "INFO:config:Processing documents from provided list.\n",
      "2023-10-27 16:02:19,001:INFO: 293 documents remained after page filtering.\n",
      "INFO:config:293 documents remained after page filtering.\n",
      "2023-10-27 16:02:19,003:INFO: Total number of text chunks to create vector index store: 293\n",
      "INFO:config:Total number of text chunks to create vector index store: 293\n",
      "2023-10-27 16:02:19,006:INFO: Creating chroma Vectorstore\n",
      "INFO:config:Creating chroma Vectorstore\n",
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "2023-10-27 16:04:05,261:INFO: Successfully created chroma vectorstore at ../data/multimodal-chroma/descriptions/texts\n",
      "INFO:config:Successfully created chroma vectorstore at ../data/multimodal-chroma/descriptions/texts\n"
     ]
    }
   ],
   "source": [
    "generate_vectorindex(\n",
    "    embeddings=embed_model,\n",
    "    emb_size=1536,\n",
    "    documents=text_docs,\n",
    "    output_directory=os.path.join(desc_persist_dir, \"texts\"),\n",
    "    emb_store_type=\"chroma\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    index_name=\"texts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 23:06:12,600:INFO: chroma VectorStore successfully loaded from ../data/multimodal-chroma/descriptions/tables.\n",
      "INFO:config:chroma VectorStore successfully loaded from ../data/multimodal-chroma/descriptions/tables.\n"
     ]
    }
   ],
   "source": [
    "tables_index = load_vectorindex(\n",
    "    db_directory = os.path.join(desc_persist_dir, \"tables\"),\n",
    "    emb_store_type = \"chroma\", index_name = \"tables\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "2023-10-25 23:06:21,228:INFO: chroma VectorStore successfully loaded from ../data/multimodal-chroma/descriptions/texts.\n",
      "INFO:config:chroma VectorStore successfully loaded from ../data/multimodal-chroma/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "texts_index = load_vectorindex(\n",
    "    db_directory = os.path.join(desc_persist_dir, \"texts\"),\n",
    "    emb_store_type = \"chroma\", index_name = \"texts\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
