{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import openai\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from typing import Union, Dict\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.schema import Document\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "openai.log = \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"..\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, Dict]) -> Dict:\n",
    "    if isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.metadata[\"file_name\"],\n",
    "                \"page\": doc.metadata[\"page_label\"]\n",
    "            }\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc[\"metadata\"][\"file_name\"],\n",
    "                \"page\": doc[\"metadata\"][\"page_label\"]\n",
    "            }\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=1024\n",
    "chunk_overlap=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=512)\n",
    "embs = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    embed_model = embs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/simple/openai_512_20\n"
     ]
    }
   ],
   "source": [
    "vector_dir = os.path.join(DATA_DIR, \"emb_store\", \"simple\",\n",
    "                          f\"openai_{chunk_size}_{chunk_overlap}\")\n",
    "\n",
    "print(vector_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SAVE\n",
    "# from llama_index.vector_stores import SimpleVectorStore\n",
    "\n",
    "# vector_store = SimpleVectorStore()\n",
    "# documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# vector_index = VectorStoreIndex.from_documents(\n",
    "#     documents=documents,\n",
    "#     service_context=service_context,\n",
    "#     storage_context=storage_context\n",
    "# )\n",
    "\n",
    "# vector_index.set_index_id(\"msk-mri\")\n",
    "\n",
    "# vector_index.storage_context.persist(vector_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "vector_store = SimpleVectorStore.from_persist_dir(vector_dir)\n",
    "storage_context = StorageContext.from_defaults(persist_dir = vector_dir)\n",
    "vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context, index_name=\"msk-mri\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot provide an answer to the query as it does not contain a specific question or request for information. Please provide a clear question or request and I will be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "sample_query = \"test Query\"\n",
    "response = query_engine.query(sample_query)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading indices with ids: ['vector_index']\n"
     ]
    }
   ],
   "source": [
    "## LOAD\n",
    "storage_context = StorageContext.from_defaults(persist_dir=vector_dir)\n",
    "vector_index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/faiss/openai_1024_20\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = os.path.join(\n",
    "    DATA_DIR, \"emb_store\", \"faiss\", f\"openai_{chunk_size}_{chunk_overlap}\"\n",
    "    )\n",
    "\n",
    "print(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE\n",
    "# from llama_index.vector_stores import FaissVectorStore\n",
    "# import faiss\n",
    "\n",
    "# d = 1536\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# os.makedirs(PERSIST_DIR)\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     vector_store=vector_store\n",
    "# )\n",
    "\n",
    "# documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()[:5]\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     service_context=service_context\n",
    "#     )\n",
    "\n",
    "# index.storage_context.persist(persist_dir = PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss from ../data/emb_store/faiss/openai_1024_20/vector_store.json.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# LOAD\n",
    "\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(PERSIST_DIR)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=PERSIST_DIR\n",
    ")\n",
    "\n",
    "vector_index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot answer the query as it does not provide any specific information or context to work with. Please provide more details or a specific question.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "sample_query = \"test Query\"\n",
    "response = query_engine.query(sample_query)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/chroma/openai/openai_512_20\n"
     ]
    }
   ],
   "source": [
    "chroma_dir=os.path.join(DATA_DIR, \"emb_store\", \"chroma\",\n",
    "                        f\"openai_{chunk_size}_{chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event ClientStartEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event ClientCreateCollectionEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event CollectionAddEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "# db = chromadb.PersistentClient(path=chroma_dir)\n",
    "# chroma_collection = db.get_or_create_collection(\"test\")\n",
    "\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, service_context=service_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event ClientStartEvent: module 'chromadb' has no attribute 'get_settings'\n",
      "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event CollectionQueryEvent: module 'chromadb' has no attribute 'get_settings'\n"
     ]
    }
   ],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=chroma_dir)\n",
    "chroma_collection = db2.get_or_create_collection(\"test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/weaviate/openai_512_20\n"
     ]
    }
   ],
   "source": [
    "weaviate_path = os.path.join(\n",
    "    DATA_DIR, \"emb_store\", \"weaviate\", f\"openai_{chunk_size}_{chunk_overlap}\"\n",
    ")\n",
    "print(weaviate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "embedded_options = EmbeddedOptions(\n",
    "    persistence_data_path=weaviate_path,\n",
    "    binary_path=os.path.join(weaviate_path, \"bin\"),\n",
    "    port=6666\n",
    ")\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"test_j0sIBOkr08rd\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-10-17T20:37:13+08:00\",\"took\":69685}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"Test\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    vector_store_query_mode=\"hybrid\", similarity_top_k=2\n",
    ")\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK:\n",
      "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
      "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Recommend if the image scan ordered is appropriate given the PATIENT PROFILE and CONTEXT. If the scan is not appropriate, recommend an appropriate procedure.\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT.\n",
      "==========\n",
      "OUTPUT INSTRUCTIONS:\n",
      "Your output should contain the following:\n",
      "1. Classification of appropriateness for the ordered scan.\n",
      "2. Provide explanation for the appropriateness classification.\n",
      "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
      "\n",
      "Format your output as follow:\n",
      "1. Classification: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
      "2. Explanation:\n",
      "3. Recommendation: Can be alternative procedure, NO SCAN REQUIRED or NO CHANGE REQUIRED \n",
      "==========\n",
      "CONTEXT:\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK:\n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Recommend if the image scan ordered is appropriate given the PATIENT PROFILE and CONTEXT. If the scan is not appropriate, recommend an appropriate procedure.\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT.\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "\n",
    "Format your output as follow:\n",
    "1. Classification: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "2. Explanation:\n",
    "3. Recommendation: Can be alternative procedure, NO SCAN REQUIRED or NO CHANGE REQUIRED \n",
    "==========\n",
    "CONTEXT:\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)\n",
    "\n",
    "print(CHAT_PROMPT_TEMPLATE.format(context_str=\"context_str\", query_str=\"query_str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = vector_index, similarity_top_k=5\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"simple_summarize\",\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE,\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACR scenario</th>\n",
       "      <th>Appropriateness Category</th>\n",
       "      <th>MRI scan ordered</th>\n",
       "      <th>Difficulty</th>\n",
       "      <th>Clinical File</th>\n",
       "      <th>AR Rad</th>\n",
       "      <th>AR Ortho</th>\n",
       "      <th>AR LLM</th>\n",
       "      <th>queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Traumatic shoulder pain. Nonlocalized shoulder...</td>\n",
       "      <td>UA</td>\n",
       "      <td>MRI shoulder without IV contrast</td>\n",
       "      <td>L</td>\n",
       "      <td>22 year old Chinese Male. No significant past ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patient Profile: 22 year old Chinese Male. No ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Known axial spondyloarthritis. Follow-up for t...</td>\n",
       "      <td>MBA</td>\n",
       "      <td>MRI sacroiliac joints\\nwithout and with IV con...</td>\n",
       "      <td>H</td>\n",
       "      <td>43 year old Indian Male.  Known ankylosing spo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patient Profile: 43 year old Indian Male.  Kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acute blunt or penetrating trauma to the hand ...</td>\n",
       "      <td>UNA</td>\n",
       "      <td>MRI wrist without IV\\ncontrast</td>\n",
       "      <td>L</td>\n",
       "      <td>38 year old Bangladeshi Male.  No significant ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patient Profile: 38 year old Bangladeshi Male....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ACR scenario Appropriateness Category  \\\n",
       "0  Traumatic shoulder pain. Nonlocalized shoulder...                       UA   \n",
       "1  Known axial spondyloarthritis. Follow-up for t...                      MBA   \n",
       "2  Acute blunt or penetrating trauma to the hand ...                      UNA   \n",
       "\n",
       "                                    MRI scan ordered Difficulty  \\\n",
       "0                   MRI shoulder without IV contrast          L   \n",
       "1  MRI sacroiliac joints\\nwithout and with IV con...          H   \n",
       "2                     MRI wrist without IV\\ncontrast          L   \n",
       "\n",
       "                                       Clinical File  AR Rad  AR Ortho  \\\n",
       "0  22 year old Chinese Male. No significant past ...     NaN       NaN   \n",
       "1  43 year old Indian Male.  Known ankylosing spo...     NaN       NaN   \n",
       "2  38 year old Bangladeshi Male.  No significant ...     NaN       NaN   \n",
       "\n",
       "   AR LLM                                            queries  \n",
       "0     NaN  Patient Profile: 22 year old Chinese Male. No ...  \n",
       "1     NaN  Patient Profile: 43 year old Indian Male.  Kno...  \n",
       "2     NaN  Patient Profile: 38 year old Bangladeshi Male....  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testcase_df = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\")\n",
    ")\n",
    "\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"MRI scan ordered\"]\n",
    "testcase_df[\"queries\"] = [generate_query(patient_profile, scan_order)\n",
    "        for patient_profile, scan_order in zip(patient_profiles, scan_orders)]\n",
    "\n",
    "testcase_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = testcase_df[\"queries\"][0]\n",
    "response = query_engine.query(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['38 year old Bangladeshi Male',\n",
       " '  No significant past medical history',\n",
       " '   Was using power drill when it slipped and penetrated right wrist',\n",
       " '  On examination: oozing wound on dorsum of right wrist, no obvious bone visualised',\n",
       " '  Wrist swelling',\n",
       " ' Unable to assess range of motion due to pain',\n",
       " '  Nil imaging performed thus far',\n",
       " '   MRI right wrist without IV contrast to assess soft tissue and bony injury',\n",
       " '  ']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testcase_df[\"Clinical File\"][2].split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_dict = {}\n",
    "for doc in documents:\n",
    "    if doc.metadata[\"file_name\"] not in page_dict:\n",
    "        page_dict[doc.metadata[\"file_name\"]] = 1\n",
    "    else:\n",
    "        page_dict[doc.metadata[\"file_name\"]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_pages = []\n",
    "for doc in documents:\n",
    "    if \"References\" in doc.text:\n",
    "        reference_pages.append((doc.metadata[\"file_name\"], doc.metadata[\"page_label\"], doc.text))   \n",
    "        \n",
    "reference_dict = {}\n",
    "for doc, page, _ in reference_pages:\n",
    "    reference_dict[doc] = int(page)\n",
    "    \n",
    "exclude_dict = {}\n",
    "for file_name, reference_startpage in reference_dict.items():\n",
    "    reference_endpage = page_dict[file_name]\n",
    "    exclude_dict[file_name] = list(range(reference_startpage, reference_endpage+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"exclude_pages.json\"), \"w\") as f:\n",
    "    json.dump(exclude_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
