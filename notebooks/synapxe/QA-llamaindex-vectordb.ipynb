{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import tiktoken\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "from config import MAIN_DIR, GUIDELINES\n",
    "from copy import deepcopy\n",
    "from utils import load_vectorindex\n",
    "from datetime import datetime\n",
    "from textdistance import levenshtein\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Union, Optional, List, Literal, Callable, Sequence, Tuple\n",
    "from utils import count_tokens\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate as LCChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "from llama_index import (ServiceContext, QueryBundle, SimpleDirectoryReader, get_response_synthesizer)\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.indices.base_retriever import BaseRetriever\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor, LongContextReorder\n",
    "from llama_index.indices.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.indices.query.base import BaseQueryEngine\n",
    "from llama_index.indices.query.schema import QueryBundle, QueryType\n",
    "from llama_index.llms import ChatMessage, MessageRole, OpenAI\n",
    "from llama_index.prompts import ChatPromptTemplate, BasePromptTemplate\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response.schema import Response, RESPONSE_TYPE\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.schema import Document, NodeWithScore, TextNode\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from custom import CustomCombinedRetriever, CustomRetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_guidelines = \"\\n\".join([\"- \" + guideline for guideline in GUIDELINES])\n",
    "\n",
    "refine_template = \"\"\"You are a radiologist expert. Do not make up additional information.\n",
    "=========\n",
    "TASK: You are given a PATIENT PROFILE. You need to perform the following information referencing from the PATIENT PROFILE:\n",
    "1. Extract relevant information for recommendation of imaging procedure, including age, symptomps, previous diagnosis, stage of diagnosis (INITIAL IMAGING OR NEXT STUDY) and suspected conditions, if any.\n",
    "Only return information given inside the PROFILE, do not make up other information.\n",
    "2. Return one or more guidelines from the following list of guidelines potentially relevant to the recommendations of imaging procedure given patient profile. If there are no relevant guidelines, return empty list.\n",
    "{}\n",
    "=========\n",
    "OUTPUT INSTRUCTION:\n",
    "Output your answer as follow:\n",
    "1. Relevant information:\n",
    "2. Relevant guidelines: List of guidelines. Match the exact text given in the list. If no relevant guidelines, return [].\n",
    "=========\n",
    "\"\"\".format(all_guidelines)\n",
    "\n",
    "human_template = \"PATIENT PROFILE: {query_str}\"\n",
    "\n",
    "REFINE_TEMPLATE = LCChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(refine_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template)\n",
    "    ]\n",
    ")\n",
    "\n",
    "refine_chain = LLMChain(\n",
    "    prompt=REFINE_TEMPLATE,\n",
    "    llm=ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, max_tokens=512)\n",
    "    )\n",
    "\n",
    "extract_template = \"\"\"TASK: Extract the following information from the provided text query.\n",
    "1. Appropropriateness of the scan ordered.\n",
    "2. Most Appropriate Imaging Modality\n",
    "===============\n",
    "FORMAT INSTRUCTIONS: Your output should contains the following:\n",
    "Appropriateness: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "Recommendation: The most appropriate imaging modality. If no appropriate imaging modality, return nothing.\n",
    "===============\n",
    "TEXT QUERY: {query}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate as LCPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI as LCChatOpenAI\n",
    "\n",
    "FIX_PROMPT = LCPromptTemplate.from_template(extract_template)\n",
    "\n",
    "fixing_chain = LLMChain(\n",
    "    llm=LCChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0, max_tokens=512),\n",
    "    prompt=FIX_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_pages(\n",
    "    doc_list: List[Document],\n",
    "    exclude_info: Dict[str, List]\n",
    ") -> List[Document]:\n",
    "    filtered_list = []\n",
    "    for doc in doc_list:\n",
    "        file_name = doc.metadata[\"file_name\"]\n",
    "        page = doc.metadata[\"page_label\"]\n",
    "        if file_name not in exclude_info.keys():\n",
    "            filtered_list.append(doc)\n",
    "            continue\n",
    "        if int(page) not in exclude_info[file_name]:\n",
    "            filtered_list.append(doc)\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    if isinstance(prompt, BasePromptTemplate):\n",
    "        return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "    if isinstance(prompt, Union[LCPromptTemplate, LCChatPromptTemplate]):\n",
    "        return prompt.format(**{v: v for v in prompt.input_variables})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, NodeWithScore):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": doc.score\n",
    "            } \n",
    "    elif isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": \"\"\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"score\": \"None\"\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def calculate_min_dist(\n",
    "    input_str: str,\n",
    "    text_list: List[str] = GUIDELINES,\n",
    "    return_nearest_text: bool = False\n",
    "):\n",
    "    min_dist = float(\"inf\")\n",
    "    nearest_text = None\n",
    "\n",
    "    for ref_text in text_list:\n",
    "        dist = levenshtein.distance(input_str, ref_text)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest_text = ref_text\n",
    "    return (min_dist, nearest_text) if return_nearest_text else min_dist\n",
    "\n",
    "def setup_query_engine(\n",
    "    db_directory: str,\n",
    "    emb_store_type: Literal[\"simple, faiss\"] = \"simple\",\n",
    "    index_name: Optional[str] = None,\n",
    "    similarity_top_k: int = 4,\n",
    "    text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "    synthesizer_llm: str = \"gpt-3.5-turbo-1106\",\n",
    "    emb_type: str = \"openai\",\n",
    "    synthesizer_temperature: int = 0,\n",
    "    synthesizer_max_tokens: int = 512,\n",
    "    response_mode: str = \"simple_summarize\",\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    callback_manager: Optional[CallbackManager] = None,\n",
    ") ->  BaseQueryEngine:\n",
    "    \n",
    "    vector_index = load_vectorindex(db_directory, emb_store_type=emb_store_type, index_name=index_name)\n",
    "    \n",
    "    if emb_type == \"openai\":\n",
    "        embs = OpenAIEmbedding()\n",
    "\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=vector_index, similarity_top_k=similarity_top_k,\n",
    "        callback_manager=callback_manager\n",
    "    )\n",
    "\n",
    "    # Setup Synthesizer\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=OpenAI(\n",
    "            temperature=synthesizer_temperature,\n",
    "            model=synthesizer_llm, max_tokens=synthesizer_max_tokens\n",
    "            ),\n",
    "        embed_model=embs, callback_manager=callback_manager\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        service_context=service_context, response_mode=response_mode,\n",
    "        text_qa_template=text_qa_template\n",
    "    )\n",
    "    \n",
    "    # Setup QueryEngine\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever, response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors = node_postprocessors\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def process_result_json(\n",
    "    testcase_df: pd.DataFrame, responses: List[Response], save_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    json_responses = []\n",
    "    queries = testcase_df[\"queries\"]\n",
    "    scan_orders = testcase_df[\"Scan Order\"]\n",
    "    \n",
    "    tk = tqdm(zip(queries, responses, scan_orders), total=len(responses))\n",
    "    for query, response, scan_order in tk:\n",
    "        testcase_info = {\n",
    "            \"question\": query,\n",
    "            \"result\": response.response,\n",
    "            \"source_documents\": [convert_doc_to_dict(doc) for doc in response.source_nodes]\n",
    "        }\n",
    "        answer_query = \"Scan Ordered: {}\\nAnswer: {}\".format(scan_order, testcase_info[\"result\"])\n",
    "        fixed_answer = fixing_chain(answer_query)\n",
    "        try:\n",
    "            appropriateness, recommendation = re.findall(\n",
    "            #  r\"^Appropriateness: ([0-9A-Za-z ]+)\\nRecommendation: ([0-9A-Za-z \\.]+)$\", fixed_answer[\"text\"])[0]\n",
    "                r\"^[^\\n]*Appropriateness: ([^\\n]+)\\n+[^\\n]*Recommendation: ([^\\n]+)$\", fixed_answer[\"text\"])[0]\n",
    "        except:\n",
    "            appropriateness, recommendation = \"\", \"\"\n",
    "        testcase_info[\"appropriateness\"] = appropriateness\n",
    "        testcase_info[\"recommendation\"] = recommendation\n",
    "\n",
    "        json_responses.append(testcase_info)\n",
    "        \n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(json_responses, f)\n",
    "    return json_responses\n",
    "\n",
    "def process_result_df(\n",
    "    testcase_df: pd.DataFrame, results: Union[List[Dict], List[Response]], save_path: Optional[str] = None\n",
    "):\n",
    "    if isinstance(results[0], Response):\n",
    "        results = process_result_json(testcase_df, results)\n",
    "    \n",
    "    result_df = deepcopy(testcase_df)\n",
    "    result_df[\"gpt_raw_answer\"] = [response[\"result\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = [response[\"appropriateness\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = result_df[\"gpt_classification\"].str.upper()\n",
    "    result_df[\"gpt_recommendation\"] = [response[\"recommendation\"] for response in results]\n",
    "    result_df[\"context\"] = [\n",
    "        \"\\n\\n\\n\\n\".join([\"Metadata: {}\\nScore: {}\\n\\nPage Content: {}\".format(\n",
    "            \"\\n\".join([f\"{k}: {v}\" for k, v in document[\"metadata\"].items()]),\n",
    "            document[\"score\"],  document[\"page_content\"])\n",
    "                         for document in response[\"source_documents\"]])\n",
    "        for response in results\n",
    "    ]\n",
    "\n",
    "    result_df = result_df.rename(columns = {\"Appropriateness Category\": \"human_gt\"})\n",
    "\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UA$\", \"USUALLY APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UNA$\", \"USUALLY NOT APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^MBA$\", \"MAY BE APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^ICI$\", \"INSUFFICIENT INFORMATION\", regex=True)\n",
    "    \n",
    "    result_df[\"match\"] = (result_df[\"gpt_classification\"] == result_df[\"human_gt\"])\n",
    "\n",
    "    if save_path:\n",
    "        result_df.to_csv(save_path)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def count_tokens(\n",
    "    texts: Union[str, TextNode,NodeWithScore,List],\n",
    "    tokenizer: Callable = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "):\n",
    "    token_counter = 0\n",
    "    if not isinstance(texts, List):\n",
    "        texts = [texts]\n",
    "    for text in texts:\n",
    "        if isinstance(text, NodeWithScore):\n",
    "            text_str = text.node.text\n",
    "        elif isinstance(text, TextNode):\n",
    "            text_str = text.text\n",
    "        elif isinstance(text, str):\n",
    "            text_str = text\n",
    "        token_counter += len(tokenizer.encode(text_str))\n",
    "    return token_counter\n",
    "\n",
    "def extract_guidelines(\n",
    "    profile: str,\n",
    "    extract_chain: LLMChain\n",
    ") -> Tuple[str, List[str]]:\n",
    "    extracted_response = extract_chain(profile)[\"text\"]\n",
    "    if extracted_response.endswith(\".\"):\n",
    "        extracted_response = extracted_response[:-1]\n",
    "\n",
    "    pattern = r\"1. Relevant information:([\\S\\s]+)2. Relevant guidelines:([\\S\\s]*)\"\n",
    "\n",
    "    profile, guidelines_str = re.findall(pattern, extracted_response)[0]\n",
    "    guidelines_str = guidelines_str.replace(\"- \", \"\")\n",
    "    guidelines_str = guidelines_str.strip()\n",
    "    guidelines_str = guidelines_str.replace(\"\\n\", \", \")\n",
    "\n",
    "    if not guidelines_str:\n",
    "        relevant_guidelines = []\n",
    "    else:\n",
    "        regex_guidelines = re.findall(r\"([A-Za-z ]+)\", guidelines_str)\n",
    "        relevant_guidelines = []\n",
    "        for extracted_guideline in regex_guidelines:\n",
    "            extracted_guideline = extracted_guideline.lower()\n",
    "            min_dist, nearest_text = calculate_min_dist(extracted_guideline, GUIDELINES, True)\n",
    "            if min_dist <= 1:\n",
    "                extracted_guideline = nearest_text\n",
    "                relevant_guidelines.append(extracted_guideline)\n",
    "                \n",
    "    return profile, relevant_guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_cases(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    exp_args: Dict,\n",
    "    testcases: Sequence[str] = None,\n",
    "    patient_profiles: Sequence[str] = None,\n",
    "    scan_orders: Sequence[str] = None,\n",
    "    refined_profiles: Sequence[str] = None,\n",
    "    relevant_guidelines: Sequence[List[str]] = None,\n",
    "    query_engine: Optional[BaseQueryEngine] = None,\n",
    "    query_template: str = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\",\n",
    "    text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "    refine_template: Optional[LCChatPromptTemplate] = REFINE_TEMPLATE,\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    artifact_dir: str = ARTIFACT_DIR,\n",
    "    emb_folder: str = EMB_DIR,\n",
    "):\n",
    "    save_folder = os.path.join(\n",
    "        artifact_dir, \"{}_{}_{}_{}_{}\".format(\n",
    "            exp_args[\"synthesizer_llm\"],\n",
    "            exp_args[\"chunk_size\"],\n",
    "            exp_args[\"chunk_overlap\"],\n",
    "            exp_args[\"description\"],\n",
    "            datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    logger = get_experiment_logs(exp_args[\"description\"], log_folder=save_folder)\n",
    "    \n",
    "    if not query_engine:\n",
    "        token_counter = TokenCountingHandler(\n",
    "            tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    "        )\n",
    "        callback_manager = CallbackManager([token_counter])\n",
    "        db_directory = os.path.join(\n",
    "            emb_folder, exp_args[\"vectorstore\"],\n",
    "            \"{}_{}_{}\".format(exp_args[\"emb_type\"], exp_args[\"chunk_size\"], exp_args[\"chunk_overlap\"])\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"--------------------\\nLoading VectorDB from {db_directory}\")\n",
    "        query_engine = setup_query_engine(\n",
    "            db_directory,\n",
    "            emb_store_type=exp_args[\"vectorstore\"],\n",
    "            index_name=exp_args[\"index_name\"],\n",
    "            similarity_top_k=exp_args[\"similarity_top_k\"],\n",
    "            text_qa_template=text_qa_template,\n",
    "            synthesizer_llm = exp_args[\"synthesizer_llm\"],\n",
    "            synthesizer_temperature = exp_args[\"synthesizer_temperature\"],\n",
    "            synthesizer_max_tokens = exp_args[\"synthesizer_max_tokens\"],\n",
    "            response_mode = \"simple_summarize\",\n",
    "            node_postprocessors = node_postprocessors,\n",
    "            callback_manager = callback_manager\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        token_counter = query_engine.callback_manager.handlers[0]\n",
    "\n",
    "    logger.info(\n",
    "        \"-------------\\nExperiment settings:\\n{}\".format(\n",
    "            \"\\n\".join([f\"{k}:{v}\" for k, v in exp_args.items()])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(save_folder, \"settings.yaml\"), \"w\") as f:\n",
    "        yaml.dump(exp_args, f)\n",
    "\n",
    "    responses = []\n",
    "    \n",
    "    logger.info(\n",
    "        \"-------------\\nQA PROMPT: {}\".format(convert_prompt_to_string(query_engine._response_synthesizer._text_qa_template))\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"------START RUNNING TEST CASES---------\"\n",
    "    )\n",
    "\n",
    "    if not (patient_profiles is not None and scan_orders is not None):\n",
    "        if not testcases:\n",
    "            testcases = testcase_df[\"Clinical File\"]\n",
    "        patient_profiles = [remove_final_sentence(testcase, True)[0] for testcase in testcases]\n",
    "        scan_orders = [remove_final_sentence(testcase, True)[1] for testcase in testcases]\n",
    "\n",
    "    if exp_args.get(\"refine_profile\") or exp_args.get(\"metadata_filter\"):\n",
    "        if not (refined_profiles and relevant_guidelines):\n",
    "            logger.info(\n",
    "                \"-------------\\nREFINE PROMPT: {}\".format(convert_prompt_to_string(refine_template))\n",
    "            )\n",
    "            \n",
    "            from langchain.callbacks import get_openai_callback\n",
    "            refine_chain = LLMChain(\n",
    "                llm=LCChatOpenAI(model_name=exp_args.get(\"refine_llm\", \"gpt-3.5-turbo-1106\"), temperature=0, max_tokens=512),\n",
    "                prompt=refine_template)\n",
    "            \n",
    "            with get_openai_callback() as cb:\n",
    "                refined_infos = [extract_guidelines(profile, refine_chain) for profile in tqdm(patient_profiles, total=len(patient_profiles))]\n",
    "            print(f\"Number of refined tokens: Prompt tokens = {cb.prompt_tokens}, Completion tokens = {cb.completion_tokens}\")\n",
    "    \n",
    "            refined_profiles = [refined_info[0] for refined_info in refined_infos]\n",
    "            relevant_guidelines = [refined_info[1] for refined_info in refined_infos]\n",
    "    \n",
    "    if exp_args.get(\"refine_profile\"):\n",
    "        patient_profiles = refined_profiles\n",
    "    \n",
    "    testcase_df[\"queries\"] = [query_wrapper(query_template, {\"profile\": patient_profile, \"scan_order\": scan_order})\n",
    "                 for patient_profile, scan_order in zip(patient_profiles, scan_orders)]\n",
    "        \n",
    "    metadata_filters = relevant_guidelines if exp_args.get(\"metadata_filter\") else [None] * len(testcase_df[\"queries\"])\n",
    "    \n",
    "    for query, metadata_filter in tqdm(zip(testcase_df[\"queries\"], metadata_filters), total=len(testcase_df[\"queries\"])):\n",
    "        input_query = {\"str_or_query_bundle\": query, \"table_filter\": metadata_filter, \"text_filter\": metadata_filter} if metadata_filter is not None else {\"str_or_query_bundle\": query}\n",
    "        response = query_engine.query(**input_query)\n",
    "        responses.append(response)\n",
    "    \n",
    "      \n",
    "    logger.info(\"--------------\\nTokens Consumption: Total: {}, Prompt: {}, Completion: {}, Embeddings: {}\"\n",
    "                .format(token_counter.total_llm_token_count,\n",
    "                        token_counter.prompt_llm_token_count,\n",
    "                        token_counter.completion_llm_token_count,\n",
    "                        token_counter.total_embedding_token_count))\n",
    "\n",
    "    logger.info(f\"----------\\nTest case Completed. Saving Artifacts into {save_folder}\")\n",
    "    json_responses = process_result_json(\n",
    "        testcase_df, responses=responses, save_path=os.path.join(save_folder, \"results.json\")\n",
    "        )\n",
    "\n",
    "    result_df = process_result_df(\n",
    "        testcase_df, json_responses, save_path=os.path.join(save_folder, \"result.csv\")\n",
    "        )\n",
    "\n",
    "    accuracy = result_df[\"match\"].sum() / len(result_df) * 100\n",
    "\n",
    "    logger.info(\"------EVALUATION-----\")\n",
    "    logger.info(f\"Accuracy score: {accuracy}\")\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"gpt_classification\", \"human_gt\"])[\"match\"].value_counts())\n",
    "    )\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"human_gt\", \"gpt_classification\"])[\"match\"].value_counts())\n",
    "    )\n",
    "\n",
    "    return json_responses, result_df, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Appropriateness Category', 'Scan Order',\n",
    "                   'Difficulty', 'Clinical File']\n",
    "        )\n",
    "\n",
    "patient_profiles = [remove_final_sentence(patient_profile, True)[0].strip() for patient_profile in testcase_df[\"Clinical File\"]]\n",
    "scan_orders = [remove_final_sentence(patient_profile, True)[1].strip() for patient_profile in testcase_df[\"Clinical File\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs before filtering: 546\n",
      "Total number of docs after filtering 395\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "print(\"Total no of docs before filtering:\", len(documents))\n",
    "with open(EXCLUDE_DICT, \"r\") as f:\n",
    "    exclude_pages = json.load(f)\n",
    "documents = filter_by_pages(doc_list=documents, exclude_info=exclude_pages)\n",
    "print(\"Total number of docs after filtering\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 4 - Separate Tables and Text Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from PATIENT PROFILE relevant information for classification of imaging appropriateness. DO NOT make any assumptions from the SCAN ORDER.\n",
    "Important information includes AGE, SYMPTOMS, PREVIOUS DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Identify there are superior imaging procedures or treatments with a more favorable risk-benefit ratio.\n",
    "4. Based on the SCORING CRITERIA, recommend if the SCAN ORDER is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT to give the recommendation\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Take note for scenarios involving IV CONTRAST there are 3 distinct scan protocols: (1) with IV CONTRAST, (2) without IV CONTRAST, (3) without and with IV CONTRAST. \n",
    "Each of them is different and can have different appropriateness category.\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    table_similarity_top_k = 4,\n",
    "    text_similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"DescriptionsTableAndText\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "# node_postprocessors = [LongContextReorder()]\n",
    "node_postprocessors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-28 20:19:34,356:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/descriptions/tables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-28 20:19:34,432:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-faiss\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"table_similarity_top_k\"],\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"text_similarity_top_k\"],\n",
    ")\n",
    "text_and_table_retriever = CustomCombinedRetriever(\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever, token_limit = 7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = OpenAIEmbedding()\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        temperature=exp_args[\"synthesizer_temperature\"],\n",
    "        model=exp_args[\"synthesizer_llm\"], max_tokens=exp_args[\"synthesizer_max_tokens\"]\n",
    "        ),\n",
    "    embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=exp_args[\"response_mode\"],\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=text_and_table_retriever, response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    patient_profiles=patient_profiles,\n",
    "    scan_orders=scan_orders,\n",
    "    query_engine=query_engine,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 5 - Multisteps LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save first\n",
    "# multiple_guideline_answers = []\n",
    "# for clinical_file in tqdm(testcase_df[\"Clinical File\"],\n",
    "#                           total=len(testcase_df[\"Clinical File\"])):\n",
    "#     profile = remove_final_sentence(clinical_file)\n",
    "#     response = refine_chain(profile)\n",
    "#     multiple_guideline_answers.append(response[\"text\"])\n",
    "\n",
    "# profiles, guidelines = [], []\n",
    "\n",
    "# for answer in multiple_guideline_answers:\n",
    "#     if answer.endswith(\".\"):\n",
    "#         answer = answer[:-1]\n",
    "#     pattern = r\"1. Relevant information:([\\S\\s]+)2. Relevant guidelines:([\\S\\s]*)\"\n",
    "    \n",
    "#     profile, guidelines_str = re.findall(pattern, answer)[0]\n",
    "#     guidelines_str = guidelines_str.replace(\"- \", \"\")\n",
    "#     guidelines_str = guidelines_str.strip()\n",
    "#     guidelines_str = guidelines_str.replace(\"\\n\", \", \")\n",
    "    \n",
    "#     if not guidelines_str:\n",
    "#         profiles.append(profile)\n",
    "#         guidelines.append([])\n",
    "#     else:\n",
    "#         regex_guidelines = re.findall(r\"([A-Za-z ]+)\", guidelines_str)\n",
    "#         extracted_guidelines = []\n",
    "#         for i, extracted_guideline in enumerate(regex_guidelines):\n",
    "#             extracted_guideline = extracted_guideline.lower()\n",
    "#             min_dist, nearest_text = calculate_min_dist(extracted_guideline, GUIDELINES, True)\n",
    "#             if min_dist <= 1:\n",
    "#                 extracted_guideline = nearest_text\n",
    "#                 extracted_guidelines.append(extracted_guideline)\n",
    "#             else:\n",
    "#                 print(extracted_guideline + \"_\" + nearest_text)\n",
    "                \n",
    "#     profiles.append(profile.strip())   \n",
    "#     guidelines.append(extracted_guidelines)\n",
    "\n",
    "# extracted_info_multiple_json = {\"profiles\": profiles, \"guidelines\": guidelines}\n",
    "\n",
    "# with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"w\") as f:\n",
    "#     json.dump(extracted_info_multiple_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from PATIENT PROFILE relevant information for classification of imaging appropriateness. DO NOT make any assumptions from the SCAN ORDER.\n",
    "Important information includes AGE, SYMPTOMS, PREVIOUS DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Identify there are superior imaging procedures or treatments with a more favorable risk-benefit ratio.\n",
    "4. Based on the SCORING CRITERIA, recommend if the SCAN ORDER is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure is unlikely to be recommended in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable\n",
    "- INSUFFICIENT INFORMATION: The imaging procedure or treatment is not mentioned under CONTEXT or not enough relevant information from the PATIENT PROFILE to recommend based on information in CONTEXT.\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Take note for scenarios involving IV CONTRAST there are 3 distinct scan protocols: (1) with IV CONTRAST, (2) without IV CONTRAST, (3) without and with IV CONTRAST. \n",
    "Each of them is different and can have different appropriateness category.\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"chroma\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    table_similarity_top_k = 4,\n",
    "    text_similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"DescriptionsTableAndTextWithMetadataFilter\",\n",
    "    metadata_filter = True,\n",
    "    refine_profile = False,\n",
    "    refine_llm = \"gpt-4-1106-preview\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo-1106\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "# node_postprocessors = [LongContextReorder()]\n",
    "node_postprocessors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-25 13:20:11,955:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/tables.\n",
      "2023-11-25 13:20:12,090:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-chroma\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"chroma\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"chroma\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"table_similarity_top_k\"]\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"text_similarity_top_k\"]\n",
    ")\n",
    "text_and_table_retriever = CustomCombinedRetriever(\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever, token_limit = 7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = OpenAIEmbedding()\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        temperature=exp_args[\"synthesizer_temperature\"],\n",
    "        model=exp_args[\"synthesizer_llm\"], max_tokens=exp_args[\"synthesizer_max_tokens\"]\n",
    "        ),\n",
    "    embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=exp_args[\"response_mode\"],\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "query_engine = CustomRetrieverQueryEngine(\n",
    "    retriever=text_and_table_retriever, response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"r\") as f:\n",
    "    extracted_infos = json.load(f)\n",
    "    \n",
    "refined_profiles = extracted_infos[\"profiles\"]\n",
    "relevant_guidelines = extracted_infos[\"guidelines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    # patient_profiles=patient_profiles,\n",
    "    # scan_orders=scan_orders,\n",
    "    refined_profiles=refined_profiles,\n",
    "    relevant_guidelines=relevant_guidelines,\n",
    "    query_engine=query_engine,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
