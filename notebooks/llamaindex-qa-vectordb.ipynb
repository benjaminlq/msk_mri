{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "from typing import Dict, Union, Optional, List\n",
    "from datetime import datetime\n",
    "from config import MAIN_DIR\n",
    "\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.schema import Document, NodeWithScore\n",
    "from llama_index import load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_prompt_template = \"\"\"TASK: Extract the following information from the provided text query.\n",
    "1. Appropropriateness of the scan ordered.\n",
    "2. Most Appropriate Imaging Modality\n",
    "===============\n",
    "FORMAT INSTRUCTIONS: Your output should contains the following:\n",
    "Appropriateness: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "Recommendation: The most appropriate imaging modality\n",
    "===============\n",
    "TEXT QUERY: {query}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "FIX_PROMPT = PromptTemplate.from_template(fix_prompt_template)\n",
    "\n",
    "fixing_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=512),\n",
    "    prompt=FIX_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, Union[Document, NodeWithScore]):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.metadata[\"file_name\"],\n",
    "                \"page\": doc.metadata[\"page_label\"]\n",
    "            }\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc[\"metadata\"][\"file_name\"],\n",
    "                \"page\": doc[\"metadata\"][\"page_label\"]\n",
    "            }\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str\n",
    "):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    return \".\".join(sentence_list[:-1])\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def preprocess_profile(\n",
    "    profile: str\n",
    "):\n",
    "    if profile.endswith(\".\"):\n",
    "        profile = profile[:-1]\n",
    "    sentences = profile.split(\".\")\n",
    "    return \".\".join(sentences[:-1])\n",
    "\n",
    "def process_result_json(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    responses: List[Response],\n",
    "    save_path: Optional[str] = None\n",
    "):\n",
    "    json_responses = []\n",
    "    queries = testcase_df[\"queries\"]\n",
    "    scan_orders = testcase_df[\"MRI scan ordered\"]\n",
    "    \n",
    "    for query, response, scan_order in zip(queries, responses, scan_orders):\n",
    "        testcase_info = {\n",
    "            \"question\": query,\n",
    "            \"result\": response.response,\n",
    "            \"source_documents\": [convert_doc_to_dict(doc) for doc in response.source_nodes]\n",
    "        }\n",
    "        answer_query = \"Scan Ordered: {}\\nAnswer: {}\".format(scan_order, testcase_info[\"result\"])\n",
    "        fixed_answer = fixing_chain(answer_query)\n",
    "        appropriateness, recommendation = re.findall(\n",
    "            r\"^Appropriateness: ([0-9A-Za-z ]+)\\nRecommendation: ([0-9A-Za-z \\.]+)$\", fixed_answer[\"text\"])[0]\n",
    "        testcase_info[\"appropriateness\"] = appropriateness\n",
    "        testcase_info[\"recommendation\"] = recommendation\n",
    "\n",
    "        json_responses.append(testcase_info)\n",
    "        \n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(json_responses, f)\n",
    "    return json_responses\n",
    "\n",
    "def process_result_df(\n",
    "    testcase_df: pd.DataFrame, results: Union[List[Dict], List[Response]], save_path: Optional[str] = None\n",
    "):\n",
    "    if isinstance(results[0], Response):\n",
    "        results = process_result_json(testcase_df, results)\n",
    "    \n",
    "    result_df = deepcopy(testcase_df)\n",
    "    result_df[\"gpt_raw_answer\"] = [response[\"result\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = [response[\"appropriateness\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = result_df[\"gpt_classification\"].str.upper()\n",
    "    result_df[\"gpt_recommendation\"] = [response[\"recommendation\"] for response in results]\n",
    "    result_df[\"context\"] = [\n",
    "        \"\\n\\n\".join([\"Guideline: {}, Page: {}\\nPage Content: {}\".format(\n",
    "            document[\"metadata\"][\"source\"], document[\"metadata\"][\"page\"],\n",
    "            document[\"page_content\"]) for document in response[\"source_documents\"]]\n",
    "                ) for response in results\n",
    "    ]\n",
    "\n",
    "    result_df = result_df.rename(columns = {\"Appropriateness Category\": \"human_gt\"})\n",
    "\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UA$\", \"USUALLY APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UNA$\", \"USUALLY NOT APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^MBA$\", \"MAY BE APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^ICI$\", \"INSUFFICIENT INFORMATION\", regex=True)\n",
    "    \n",
    "    result_df[\"match\"] = (result_df[\"gpt_classification\"] == result_df[\"human_gt\"])\n",
    "\n",
    "    if save_path:\n",
    "        result_df.to_csv(save_path)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Appropriateness Category', 'MRI scan ordered',\n",
    "                   'Difficulty', 'Clinical File']\n",
    "        )\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"MRI scan ordered\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiment (Rau et al 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval \n",
    "emb_type = \"openai\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 20\n",
    "db_type = \"simple\"\n",
    "similarity_top_k = 3\n",
    "index_id = \"msk-mri\"\n",
    "\n",
    "# Generation\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "max_tokens = 512\n",
    "temperature = 0\n",
    "response_mode = \"compact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/simple/openai_512_20\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(EMB_DIR, db_type, f\"{emb_type}_{chunk_size}_{chunk_overlap}\")\n",
    "print(db_directory)\n",
    "vector_store = SimpleVectorStore.from_persist_dir(db_directory)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir = db_directory\n",
    "    )\n",
    "vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    index_id=index_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(model_name).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=temperature, model=model_name, max_tokens=max_tokens\n",
    "    )\n",
    "embs = OpenAIEmbedding()\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = vector_index, similarity_top_k=similarity_top_k\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=response_mode,\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_prompt_template = (\n",
    "    \"Case: {input_text}\\n\"\n",
    "    \"Scan Ordered: {scan_order}\\n\"\n",
    "    \"Question: Is this imaging modality for this case USUALLY APPROPRIATE, \"\n",
    "    \"MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. \"\n",
    "    \"Then state precisely the most appropriate imaging modality and if contrast \"\n",
    "    \"agent is needed\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testcases (With ending scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": patient_profile, \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = dict(\n",
    "    llm_model=model_name,\n",
    "    emb_model=emb_type,\n",
    "    framework=\"llama_index\",\n",
    "    prompt=rau_prompt_template,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    description=\"BaselineExperimentRau2023\",\n",
    "    max_tokens=temperature,\n",
    "    k=similarity_top_k,\n",
    "    chain_type=\"compact\",\n",
    "    db_type=db_type,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "save_folder = os.path.join(\n",
    "    ARTIFACT_DIR,\n",
    "    \"{}_{}_{}_{}_{}\".format(\n",
    "        settings[\"llm_model\"],\n",
    "        settings[\"chunk_size\"],\n",
    "        settings[\"chunk_overlap\"],\n",
    "        settings[\"description\"],\n",
    "        datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "    )\n",
    ")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "logger = get_experiment_logs(settings[\"description\"], log_folder=save_folder)\n",
    "logger.info(\n",
    "    \"Experiment settings:\\n{}\".format(\n",
    "        \"\\n\".join([f\"{k}:{v}\" for k, v in settings.items()])\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(os.path.join(save_folder, \"settings.yaml\"), \"w\") as f:\n",
    "    yaml.dump(settings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-18 22:37:50,508:INFO: Tokens Consumption: Total: 99257, Prompt: 96913, Completion: 2344, Embeddings: []\n",
      "2023-10-18 22:37:50,508:INFO: Tokens Consumption: Total: 99257, Prompt: 96913, Completion: 2344, Embeddings: []\n"
     ]
    }
   ],
   "source": [
    "token_counter.reset_counts()\n",
    "responses = []\n",
    "\n",
    "for test_case in testcase_df[\"queries\"]:\n",
    "    response = query_engine.query(test_case)\n",
    "    responses.append(response)\n",
    "    \n",
    "logger.info(\"Tokens Consumption: Total: {}, Prompt: {}, Completion: {}, Embeddings: {}\"\n",
    "            .format(token_counter.total_llm_token_count,\n",
    "                    token_counter.prompt_llm_token_count,\n",
    "                    token_counter.completion_llm_token_count,\n",
    "                    token_counter.embedding_token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses = process_result_json(\n",
    "    testcase_df=testcase_df, responses=responses, save_path=os.path.join(save_folder, \"results.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_result_df = process_result_df(\n",
    "    testcase_df, json_responses, save_path=os.path.join(save_folder, \"result.csv\")\n",
    "    )\n",
    "rau_result_df.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 32.3943661971831\n"
     ]
    }
   ],
   "source": [
    "accuracy = rau_result_df[\"match\"].sum() / len(rau_result_df) * 100\n",
    "\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_gt                  match\n",
       "ICI                       False     1\n",
       "INSUFFICIENT INFORMATION  False     9\n",
       "MAY BE APPROPRIATE        False    14\n",
       "USUALLY APPROPRIATE       True     19\n",
       "                          False     1\n",
       "USUALLY NOT APPROPRIATE   False    23\n",
       "                          True      4\n",
       "Name: match, dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rau_result_df.groupby(\"human_gt\")[\"match\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USUALLY APPROPRIATE        65\n",
       "USUALLY NOT APPROPRIATE     6\n",
       "Name: gpt_classification, dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rau_result_df[\"gpt_classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testcases (Without ending scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": remove_final_sentence(patient_profile),\n",
    "                                        \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-19 16:43:15,032:INFO: Experiment settings:\n",
      "llm_model:gpt-3.5-turbo\n",
      "emb_model:openai\n",
      "framework:llama_index\n",
      "prompt:Case: {input_text}\n",
      "Scan Ordered: {scan_order}\n",
      "Question: Is this imaging modality for this case USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. Then state precisely the most appropriate imaging modality and if contrast agent is needed\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "description:BaselineExperimentRau2023NoEndingMRIInfo\n",
      "max_tokens:0\n",
      "k:3\n",
      "chain_type:compact\n",
      "db_type:simple\n",
      "temperature:0\n"
     ]
    }
   ],
   "source": [
    "settings = dict(\n",
    "    llm_model=model_name,\n",
    "    emb_model=emb_type,\n",
    "    framework=\"llama_index\",\n",
    "    prompt=rau_prompt_template,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    description=\"BaselineExperimentRau2023NoEndingMRIInfo\",\n",
    "    max_tokens=temperature,\n",
    "    k=similarity_top_k,\n",
    "    chain_type=\"compact\",\n",
    "    db_type=db_type,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "save_folder = os.path.join(\n",
    "    ARTIFACT_DIR,\n",
    "    \"{}_{}_{}_{}_{}\".format(\n",
    "        settings[\"llm_model\"],\n",
    "        settings[\"chunk_size\"],\n",
    "        settings[\"chunk_overlap\"],\n",
    "        settings[\"description\"],\n",
    "        datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "    )\n",
    ")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "logger = get_experiment_logs(settings[\"description\"], log_folder=save_folder)\n",
    "logger.info(\n",
    "    \"Experiment settings:\\n{}\".format(\n",
    "        \"\\n\".join([f\"{k}:{v}\" for k, v in settings.items()])\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(os.path.join(save_folder, \"settings.yaml\"), \"w\") as f:\n",
    "    yaml.dump(settings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-19 16:46:26,464:INFO: Tokens Consumption: Total: 99283, Prompt: 96997, Completion: 2286, Embeddings: []\n"
     ]
    }
   ],
   "source": [
    "token_counter.reset_counts()\n",
    "responses = []\n",
    "\n",
    "for test_case in testcase_df[\"queries\"]:\n",
    "    response = query_engine.query(test_case)\n",
    "    responses.append(response)\n",
    "    \n",
    "logger.info(\"Tokens Consumption: Total: {}, Prompt: {}, Completion: {}, Embeddings: {}\"\n",
    "            .format(token_counter.total_llm_token_count,\n",
    "                    token_counter.prompt_llm_token_count,\n",
    "                    token_counter.completion_llm_token_count,\n",
    "                    token_counter.embedding_token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    }
   ],
   "source": [
    "json_responses = process_result_json(\n",
    "    testcase_df=testcase_df, responses=responses, save_path=os.path.join(save_folder, \"results.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_result_df_2 = process_result_df(\n",
    "    testcase_df, json_responses, save_path=os.path.join(save_folder, \"result.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 30.985915492957744\n"
     ]
    }
   ],
   "source": [
    "accuracy = rau_result_df_2[\"match\"].sum() / len(rau_result_df_2) * 100\n",
    "\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_gt                  match\n",
       "ICI                       False     1\n",
       "INSUFFICIENT INFORMATION  False     9\n",
       "MAY BE APPROPRIATE        False    14\n",
       "USUALLY APPROPRIATE       True     19\n",
       "                          False     1\n",
       "USUALLY NOT APPROPRIATE   False    24\n",
       "                          True      3\n",
       "Name: match, dtype: int64"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rau_result_df_2.groupby(\"human_gt\")[\"match\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USUALLY APPROPRIATE        65\n",
       "USUALLY NOT APPROPRIATE     6\n",
       "Name: gpt_classification, dtype: int64"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rau_result_df_2[\"gpt_classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval \n",
    "emb_type = \"openai\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 20\n",
    "db_type = \"faiss\"\n",
    "similarity_top_k = 3\n",
    "index_id = \"msk-mri\"\n",
    "\n",
    "# Generation\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "max_tokens = 512\n",
    "temperature = 0\n",
    "response_mode = \"compact\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK:\n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Given the PATIENT PROFILE and CONTEXT, refer to the SCORING CRITERIA and recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "\n",
    "Format your output as follow:\n",
    "1. Classification: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "2. Explanation:\n",
    "3. Recommendation: Can be alternative procedure, NO SCAN REQUIRED or NO CHANGE REQUIRED \n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/emb_store/faiss/openai_512_20\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.vector_stores import FaissVectorStore\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(model_name).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "# Service Context\n",
    "llm = OpenAI(temperature=temperature, model=model_name, max_tokens=max_tokens)\n",
    "embs = OpenAIEmbedding()\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "db_directory = os.path.join(EMB_DIR, db_type, f\"{emb_type}_{chunk_size}_{chunk_overlap}\")\n",
    "print(db_directory)\n",
    "vector_store = FaissVectorStore.from_persist_dir(db_directory)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=db_directory)\n",
    "vector_index = load_index_from_storage(storage_context=storage_context)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = vector_index, similarity_top_k=similarity_top_k\n",
    ")\n",
    "\n",
    "# Synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=response_mode,\n",
    ")\n",
    "\n",
    "# Node Reorder\n",
    "\n",
    "# Query Engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever, response_synthesizer=response_synthesizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
