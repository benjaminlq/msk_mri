{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import logging\n",
    "from typing import Dict, Union, Optional, List, Literal\n",
    "from datetime import datetime\n",
    "from config import MAIN_DIR\n",
    "from custom_storage import load_vectorindex\n",
    "import tiktoken\n",
    "\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.schema import Document, NodeWithScore\n",
    "from llama_index import load_index_from_storage, get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import LongContextReorder\n",
    "from llama_index.indices.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_prompt_template = \"\"\"TASK: Extract the following information from the provided text query.\n",
    "1. Appropropriateness of the scan ordered.\n",
    "2. Most Appropriate Imaging Modality\n",
    "===============\n",
    "FORMAT INSTRUCTIONS: Your output should contains the following:\n",
    "Appropriateness: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "Recommendation: The most appropriate imaging modality\n",
    "===============\n",
    "TEXT QUERY: {query}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "FIX_PROMPT = PromptTemplate.from_template(fix_prompt_template)\n",
    "\n",
    "fixing_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=512),\n",
    "    prompt=FIX_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, Union[Document, NodeWithScore]):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": {\n",
    "                \"source\": doc.metadata[\"file_name\"],\n",
    "                \"page\": doc.metadata[\"page_label\"]\n",
    "            }\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": {\n",
    "                \"source\": doc[\"metadata\"][\"file_name\"],\n",
    "                \"page\": doc[\"metadata\"][\"page_label\"]\n",
    "            }\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def setup_query_engine(\n",
    "    db_directory: str,\n",
    "    emb_store_type: Literal[\"simple, faiss\"] = \"simple\",\n",
    "    index_name: Optional[str] = None,\n",
    "    similarity_top_k: int = 4,\n",
    "    text_qa_template: Optional[PromptTemplate] = None,\n",
    "    synthesizer_llm: str = \"gpt-3.5-turbo\",\n",
    "    emb_type: str = \"openai\",\n",
    "    synthesizer_temperature: int = 0,\n",
    "    synthesizer_max_tokens: int = 512,\n",
    "    response_mode: str = \"simple_summarize\",\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    callback_manager: Optional[CallbackManager] = None,\n",
    "):\n",
    "    \n",
    "    vector_index = load_vectorindex(db_directory, emb_store_type=emb_store_type, index_name=index_name)\n",
    "    \n",
    "    if emb_type == \"openai\":\n",
    "        embs = OpenAIEmbedding()\n",
    "\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index = vector_index, similarity_top_k=similarity_top_k\n",
    "    )\n",
    "\n",
    "    # Setup Synthesizer \n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=OpenAI(\n",
    "            temperature=synthesizer_temperature,\n",
    "            model=synthesizer_llm, max_tokens=synthesizer_max_tokens\n",
    "            ),\n",
    "        embed_model=embs, callback_manager=callback_manager\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        service_context=service_context, response_mode=response_mode,\n",
    "        text_qa_template=text_qa_template\n",
    "    )\n",
    "    \n",
    "    # Setup QueryEngine\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever, response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors = node_postprocessors\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def process_result_json(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    responses: List[Response],\n",
    "    save_path: Optional[str] = None\n",
    "):\n",
    "    json_responses = []\n",
    "    queries = testcase_df[\"queries\"]\n",
    "    scan_orders = testcase_df[\"MRI scan ordered\"]\n",
    "    \n",
    "    tk = tqdm(zip(queries, responses, scan_orders), total=len(responses))\n",
    "    for query, response, scan_order in tk:\n",
    "        testcase_info = {\n",
    "            \"question\": query,\n",
    "            \"result\": response.response,\n",
    "            \"source_documents\": [convert_doc_to_dict(doc) for doc in response.source_nodes]\n",
    "        }\n",
    "        answer_query = \"Scan Ordered: {}\\nAnswer: {}\".format(scan_order, testcase_info[\"result\"])\n",
    "        fixed_answer = fixing_chain(answer_query)\n",
    "        try:\n",
    "            appropriateness, recommendation = re.findall(\n",
    "            #  r\"^Appropriateness: ([0-9A-Za-z ]+)\\nRecommendation: ([0-9A-Za-z \\.]+)$\", fixed_answer[\"text\"])[0]\n",
    "                r\"^[^\\n]*Appropriateness: ([^\\n]+)\\n+[^\\n]*Recommendation: ([^\\n]+)$\", fixed_answer[\"text\"])[0]\n",
    "        except:\n",
    "            appropriateness, recommendation = \"\", \"\"\n",
    "        testcase_info[\"appropriateness\"] = appropriateness\n",
    "        testcase_info[\"recommendation\"] = recommendation\n",
    "\n",
    "        json_responses.append(testcase_info)\n",
    "        \n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(json_responses, f)\n",
    "    return json_responses\n",
    "\n",
    "def process_result_df(\n",
    "    testcase_df: pd.DataFrame, results: Union[List[Dict], List[Response]], save_path: Optional[str] = None\n",
    "):\n",
    "    if isinstance(results[0], Response):\n",
    "        results = process_result_json(testcase_df, results)\n",
    "    \n",
    "    result_df = deepcopy(testcase_df)\n",
    "    result_df[\"gpt_raw_answer\"] = [response[\"result\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = [response[\"appropriateness\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = result_df[\"gpt_classification\"].str.upper()\n",
    "    result_df[\"gpt_recommendation\"] = [response[\"recommendation\"] for response in results]\n",
    "    result_df[\"context\"] = [\n",
    "        \"\\n\\n\".join([\"Guideline: {}, Page: {}\\nPage Content: {}\".format(\n",
    "            document[\"metadata\"][\"source\"], document[\"metadata\"][\"page\"],\n",
    "            document[\"page_content\"]) for document in response[\"source_documents\"]]\n",
    "                ) for response in results\n",
    "    ]\n",
    "\n",
    "    result_df = result_df.rename(columns = {\"Appropriateness Category\": \"human_gt\"})\n",
    "\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UA$\", \"USUALLY APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UNA$\", \"USUALLY NOT APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^MBA$\", \"MAY BE APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^ICI$\", \"INSUFFICIENT INFORMATION\", regex=True)\n",
    "    \n",
    "    result_df[\"match\"] = (result_df[\"gpt_classification\"] == result_df[\"human_gt\"])\n",
    "\n",
    "    if save_path:\n",
    "        result_df.to_csv(save_path)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def run_test_cases(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    exp_args: Dict,\n",
    "    text_qa_template: Optional[PromptTemplate] = None,\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    artifact_dir: str = ARTIFACT_DIR,\n",
    "    emb_folder: str = EMB_DIR,\n",
    "):\n",
    "    save_folder = os.path.join(\n",
    "        artifact_dir, \"{}_{}_{}_{}_{}\".format(\n",
    "            exp_args[\"synthesizer_llm\"],\n",
    "            exp_args[\"chunk_size\"],\n",
    "            exp_args[\"chunk_overlap\"],\n",
    "            exp_args[\"description\"],\n",
    "            datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        print(save_folder)\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    logger = get_experiment_logs(exp_args[\"description\"], log_folder=save_folder)\n",
    "\n",
    "    db_directory = os.path.join(\n",
    "        emb_folder, exp_args[\"vectorstore\"],\n",
    "        \"{}_{}_{}\".format(exp_args[\"emb_type\"], exp_args[\"chunk_size\"], exp_args[\"chunk_overlap\"])\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"--------------------\\nLoading VectorDB from {db_directory}\")\n",
    "\n",
    "    token_counter = TokenCountingHandler(\n",
    "        tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    "    )\n",
    "    callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "    query_engine = setup_query_engine(\n",
    "        db_directory,\n",
    "        emb_store_type=exp_args[\"vectorstore\"],\n",
    "        index_name=exp_args[\"index_name\"],\n",
    "        similarity_top_k=exp_args[\"similarity_top_k\"],\n",
    "        text_qa_template=text_qa_template,\n",
    "        synthesizer_llm = exp_args[\"synthesizer_llm\"],\n",
    "        synthesizer_temperature = exp_args[\"synthesizer_temperature\"],\n",
    "        synthesizer_max_tokens = exp_args[\"synthesizer_max_tokens\"],\n",
    "        response_mode = \"simple_summarize\",\n",
    "        node_postprocessors = node_postprocessors,\n",
    "        callback_manager = callback_manager\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"-------------\\nExperiment settings:\\n{}\".format(\n",
    "            \"\\n\".join([f\"{k}:{v}\" for k, v in exp_args.items()])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(save_folder, \"settings.yaml\"), \"w\") as f:\n",
    "        yaml.dump(exp_args, f)\n",
    "\n",
    "    token_counter.reset_counts()\n",
    "    responses = []\n",
    "\n",
    "    logger.info(\n",
    "        \"-------------\\nPROMPT: {}\".format(convert_prompt_to_string(query_engine._response_synthesizer._text_qa_template))\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"------START RUNNING TEST CASES---------\"\n",
    "    )\n",
    "\n",
    "    for test_case in tqdm(testcase_df[\"queries\"], total=len(testcase_df[\"queries\"])):\n",
    "        response = query_engine.query(test_case)\n",
    "        responses.append(response)\n",
    "        \n",
    "    logger.info(\"--------------\\nTokens Consumption: Total: {}, Prompt: {}, Completion: {}, Embeddings: {}\"\n",
    "                .format(token_counter.total_llm_token_count,\n",
    "                        token_counter.prompt_llm_token_count,\n",
    "                        token_counter.completion_llm_token_count,\n",
    "                        token_counter.embedding_token_counts))\n",
    "\n",
    "    logger.info(f\"----------\\nTest case Completed. Saving Artifacts into {save_folder}\")\n",
    "    json_responses = process_result_json(\n",
    "        testcase_df, responses=responses, save_path=os.path.join(save_folder, \"results.json\")\n",
    "    )\n",
    "\n",
    "    result_df = process_result_df(\n",
    "        testcase_df, json_responses, save_path=os.path.join(save_folder, \"result.csv\")\n",
    "        )\n",
    "\n",
    "    accuracy = result_df[\"match\"].sum() / len(result_df) * 100\n",
    "\n",
    "    logger.info(\"------EVALUATION-----\")\n",
    "    logger.info(f\"Accuracy score: {accuracy}\")\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"gpt_classification\", \"human_gt\"])[\"match\"].value_counts())\n",
    "    )\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"human_gt\", \"gpt_classification\"])[\"match\"].value_counts())\n",
    "    )\n",
    "\n",
    "    return json_responses, result_df, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Appropriateness Category', 'MRI scan ordered',\n",
    "                   'Difficulty', 'Clinical File']\n",
    "        )\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"MRI scan ordered\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiment (Rau et al 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Prompt\n",
    "CHAT_PROMPT_TEMPLATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"simple\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 3,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"BaselineExperimentRau2023\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"compact\",\n",
    ")\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_prompt_template = (\n",
    "    \"Case: {input_text}\\n\"\n",
    "    \"Scan Ordered: {scan_order}\\n\"\n",
    "    \"Question: Is this imaging modality for this case USUALLY APPROPRIATE, \"\n",
    "    \"MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. \"\n",
    "    \"Then state precisely the most appropriate imaging modality and if contrast \"\n",
    "    \"agent is needed\"\n",
    "    )\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": patient_profile, \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-3.5-turbo_512_20_BaselineExperimentRau2023_22-10-2023-15-34\n"
     ]
    }
   ],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiment (Rau et al 2023) - Remove Final Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Prompt\n",
    "CHAT_PROMPT_TEMPLATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"simple\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 3,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"BaselineExperimentRau2023RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"compact\",\n",
    ")\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_prompt_template = (\n",
    "    \"Case: {input_text}\\n\"\n",
    "    \"Scan Ordered: {scan_order}\\n\"\n",
    "    \"Question: Is this imaging modality for this case USUALLY APPROPRIATE, \"\n",
    "    \"MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. \"\n",
    "    \"Then state precisely the most appropriate imaging modality and if contrast \"\n",
    "    \"agent is needed\"\n",
    "    )\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": remove_final_sentence(patient_profile), \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 - GPT-4, Chunk=512, Chunk_no=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK:\n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Given the PATIENT PROFILE and CONTEXT, refer to the SCORING CRITERIA and recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "\n",
    "Format your output as follow:\n",
    "1. Classification: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "2. Explanation:\n",
    "3. Recommendation: Can be alternative procedure, NO SCAN REQUIRED or NO CHANGE REQUIRED \n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=5_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2 - GPT-4, Chunk=1024, Overlap=128 Chunk_no=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: \n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Given the PATIENT PROFILE and CONTEXT, refer to the SCORING CRITERIA and recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args_4 = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128,\n",
    "    similarity_top_k = 7,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=7_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args_4,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3 - GPT-4, Chunk=512, Chunk_no=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=5_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]\n",
    "\n",
    "sub_testcase_df = testcase_df[testcase_df[\"Appropriateness Category\"].isin(['MBA','ICI'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:28,982:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20\n",
      "2023-10-23 00:11:29,246:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/User/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20.\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,250:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:faiss\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:5\n",
      "index_name:msk-mri\n",
      "description:Topk=5_RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:simple_summarize\n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,265:INFO: -------------\n",
      "PROMPT: system: \n",
      "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
      "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
      "==========\n",
      "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
      "Perform step-by-step the following sequence of reasoning.\n",
      "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
      "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
      "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
      "If the scan is not appropriate, recommend an appropriate procedure.\n",
      "\n",
      "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
      "==========\n",
      "SCORING CRITERIA:\n",
      "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
      "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
      "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
      "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
      "==========\n",
      "CONTEXT:\n",
      "\n",
      "context_str\n",
      "==========\n",
      "\n",
      "user: query_str\n",
      "assistant: \n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n",
      "2023-10-23 00:11:29,272:INFO: ------START RUNNING TEST CASES---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [06:38<00:00, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,276:INFO: --------------\n",
      "Tokens Consumption: Total: 62529, Prompt: 56166, Completion: 6363, Embeddings: []\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n",
      "2023-10-23 00:18:08,301:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-4_512_20_Topk=5_RemoveFinalSentence_23-10-2023-00-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 24/24 [00:29<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,579:INFO: ------EVALUATION-----\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,585:INFO: Accuracy score: 37.5\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,604:INFO: gpt_classification        human_gt                  match\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False    4\n",
      "                          MAY BE APPROPRIATE        False    7\n",
      "USUALLY NOT APPROPRIATE   MAY BE APPROPRIATE        False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n",
      "2023-10-23 00:18:37,615:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True     5\n",
      "                          MAY BE APPROPRIATE        False    1\n",
      "                          USUALLY APPROPRIATE       False    4\n",
      "MAY BE APPROPRIATE                                  False    1\n",
      "                          INSUFFICIENT INFORMATION  False    1\n",
      "                          MAY BE APPROPRIATE        True     4\n",
      "                          USUALLY APPROPRIATE       False    7\n",
      "                          USUALLY NOT APPROPRIATE   False    1\n",
      "Name: match, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=sub_testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
