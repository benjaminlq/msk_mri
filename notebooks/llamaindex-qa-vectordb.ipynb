{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import tiktoken\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "from config import MAIN_DIR, GUIDELINES\n",
    "from copy import deepcopy\n",
    "from custom_storage import load_vectorindex\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Union, Optional, List, Literal, Callable\n",
    "from textdistance import levenshtein\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate as LCChatPromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from llama_index import (\n",
    "    ServiceContext, QueryBundle, SimpleDirectoryReader, get_response_synthesizer)\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.indices.base_retriever import BaseRetriever\n",
    "from llama_index.indices.postprocessor import LongContextReorder\n",
    "from llama_index.indices.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.indices.query.base import BaseQueryEngine\n",
    "from llama_index.llms import ChatMessage, MessageRole, OpenAI\n",
    "from llama_index.prompts import ChatPromptTemplate, BasePromptTemplate\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.schema import Document, NodeWithScore, TextNode, MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_prompt_template = \"\"\"TASK: Extract the following information from the provided text query.\n",
    "1. Appropropriateness of the scan ordered.\n",
    "2. Most Appropriate Imaging Modality\n",
    "===============\n",
    "FORMAT INSTRUCTIONS: Your output should contains the following:\n",
    "Appropriateness: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "Recommendation: The most appropriate imaging modality. If no appropriate imaging modality, return nothing.\n",
    "===============\n",
    "TEXT QUERY: {query}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "FIX_PROMPT = PromptTemplate.from_template(fix_prompt_template)\n",
    "\n",
    "fixing_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=512),\n",
    "    prompt=FIX_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_pages(\n",
    "    doc_list: List[Document],\n",
    "    exclude_info: Dict[str, List]\n",
    ") -> List[Document]:\n",
    "    filtered_list = []\n",
    "    for doc in doc_list:\n",
    "        file_name = doc.metadata[\"file_name\"]\n",
    "        page = doc.metadata[\"page_label\"]\n",
    "        if file_name not in exclude_info.keys():\n",
    "            filtered_list.append(doc)\n",
    "            continue\n",
    "        if int(page) not in exclude_info[file_name]:\n",
    "            filtered_list.append(doc)\n",
    "\n",
    "    return filtered_list\n",
    "\n",
    "def convert_prompt_to_string(prompt) -> str:\n",
    "    return prompt.format(**{v: v for v in prompt.template_vars})\n",
    "\n",
    "def generate_query(profile: str, scan: str):\n",
    "    return \"Patient Profile: {}\\nScan ordered: {}\".format(profile, scan)\n",
    "\n",
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, NodeWithScore):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": doc.score\n",
    "            } \n",
    "    elif isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": \"\"\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"score\": \"None\"\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def get_experiment_logs(description: str, log_folder: str):\n",
    "    logger = logging.getLogger(description)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "    if not os.path.exists(log_folder):\n",
    "        os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "    file_handler = logging.FileHandler(filename=os.path.join(log_folder, \"logfile.log\"))\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def calculate_min_dist(\n",
    "    input_str: str,\n",
    "    text_list: List[str] = GUIDELINES,\n",
    "    return_nearest_text: bool = False\n",
    "):\n",
    "    min_dist = float(\"inf\")\n",
    "    nearest_text = None\n",
    "\n",
    "    for ref_text in text_list:\n",
    "        dist = levenshtein.distance(input_str, ref_text)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest_text = ref_text\n",
    "    return (min_dist, nearest_text) if return_nearest_text else min_dist\n",
    "\n",
    "def setup_query_engine(\n",
    "    db_directory: str,\n",
    "    emb_store_type: Literal[\"simple, faiss\"] = \"simple\",\n",
    "    index_name: Optional[str] = None,\n",
    "    similarity_top_k: int = 4,\n",
    "    text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "    synthesizer_llm: str = \"gpt-3.5-turbo\",\n",
    "    emb_type: str = \"openai\",\n",
    "    synthesizer_temperature: int = 0,\n",
    "    synthesizer_max_tokens: int = 512,\n",
    "    response_mode: str = \"simple_summarize\",\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    callback_manager: Optional[CallbackManager] = None,\n",
    ") ->  BaseQueryEngine:\n",
    "    \n",
    "    vector_index = load_vectorindex(db_directory, emb_store_type=emb_store_type, index_name=index_name)\n",
    "    \n",
    "    if emb_type == \"openai\":\n",
    "        embs = OpenAIEmbedding()\n",
    "\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index = vector_index, similarity_top_k=similarity_top_k\n",
    "    )\n",
    "\n",
    "    # Setup Synthesizer \n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=OpenAI(\n",
    "            temperature=synthesizer_temperature,\n",
    "            model=synthesizer_llm, max_tokens=synthesizer_max_tokens\n",
    "            ),\n",
    "        embed_model=embs, callback_manager=callback_manager\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        service_context=service_context, response_mode=response_mode,\n",
    "        text_qa_template=text_qa_template\n",
    "    )\n",
    "    \n",
    "    # Setup QueryEngine\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever, response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors = node_postprocessors\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def process_result_json(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    responses: List[Response],\n",
    "    save_path: Optional[str] = None\n",
    ") -> Dict:\n",
    "    json_responses = []\n",
    "    queries = testcase_df[\"queries\"]\n",
    "    scan_orders = testcase_df[\"MRI scan ordered\"]\n",
    "    \n",
    "    tk = tqdm(zip(queries, responses, scan_orders), total=len(responses))\n",
    "    for query, response, scan_order in tk:\n",
    "        testcase_info = {\n",
    "            \"question\": query,\n",
    "            \"result\": response.response,\n",
    "            \"source_documents\": [convert_doc_to_dict(doc) for doc in response.source_nodes]\n",
    "        }\n",
    "        answer_query = \"Scan Ordered: {}\\nAnswer: {}\".format(scan_order, testcase_info[\"result\"])\n",
    "        fixed_answer = fixing_chain(answer_query)\n",
    "        try:\n",
    "            appropriateness, recommendation = re.findall(\n",
    "            #  r\"^Appropriateness: ([0-9A-Za-z ]+)\\nRecommendation: ([0-9A-Za-z \\.]+)$\", fixed_answer[\"text\"])[0]\n",
    "                r\"^[^\\n]*Appropriateness: ([^\\n]+)\\n+[^\\n]*Recommendation: ([^\\n]+)$\", fixed_answer[\"text\"])[0]\n",
    "        except:\n",
    "            appropriateness, recommendation = \"\", \"\"\n",
    "        testcase_info[\"appropriateness\"] = appropriateness\n",
    "        testcase_info[\"recommendation\"] = recommendation\n",
    "\n",
    "        json_responses.append(testcase_info)\n",
    "        \n",
    "    if save_path:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(json_responses, f)\n",
    "    return json_responses\n",
    "\n",
    "def process_result_df(\n",
    "    testcase_df: pd.DataFrame, results: Union[List[Dict], List[Response]], save_path: Optional[str] = None\n",
    "):\n",
    "    if isinstance(results[0], Response):\n",
    "        results = process_result_json(testcase_df, results)\n",
    "    \n",
    "    result_df = deepcopy(testcase_df)\n",
    "    result_df[\"gpt_raw_answer\"] = [response[\"result\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = [response[\"appropriateness\"] for response in results]\n",
    "    result_df[\"gpt_classification\"] = result_df[\"gpt_classification\"].str.upper()\n",
    "    result_df[\"gpt_recommendation\"] = [response[\"recommendation\"] for response in results]\n",
    "    result_df[\"context\"] = [\n",
    "        \"\\n\\n\\n\\n\".join([\"Metadata: {}\\nScore: {}\\n\\nPage Content: {}\".format(\n",
    "            \"\\n\".join([f\"{k}: {v}\" for k, v in document[\"metadata\"].items()]),\n",
    "            document[\"score\"],  document[\"page_content\"])\n",
    "                         for document in response[\"source_documents\"]])\n",
    "        for response in results\n",
    "    ]\n",
    "\n",
    "    result_df = result_df.rename(columns = {\"Appropriateness Category\": \"human_gt\"})\n",
    "\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UA$\", \"USUALLY APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^UNA$\", \"USUALLY NOT APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^MBA$\", \"MAY BE APPROPRIATE\", regex=True)\n",
    "    result_df[\"human_gt\"] = result_df[\"human_gt\"].str.replace(r\"^ICI$\", \"INSUFFICIENT INFORMATION\", regex=True)\n",
    "    \n",
    "    result_df[\"match\"] = (result_df[\"gpt_classification\"] == result_df[\"human_gt\"])\n",
    "\n",
    "    if save_path:\n",
    "        result_df.to_csv(save_path)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def count_tokens(\n",
    "    texts: Union[str, TextNode,NodeWithScore,List],\n",
    "    tokenizer: Callable = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "):\n",
    "    token_counter = 0\n",
    "    if not isinstance(texts, List):\n",
    "        texts = [texts]\n",
    "    for text in texts:\n",
    "        if isinstance(text, NodeWithScore):\n",
    "            text_str = text.node.text\n",
    "        elif isinstance(text, TextNode):\n",
    "            text_str = text.text\n",
    "        elif isinstance(text, str):\n",
    "            text_str = text\n",
    "        token_counter += len(tokenizer.encode(text_str))\n",
    "    return token_counter\n",
    "\n",
    "def run_test_cases(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    exp_args: Dict,\n",
    "    query_engine: Optional[BaseQueryEngine] = None,\n",
    "    text_qa_template: Optional[BasePromptTemplate] = None,\n",
    "    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n",
    "    artifact_dir: str = ARTIFACT_DIR,\n",
    "    emb_folder: str = EMB_DIR,\n",
    "):\n",
    "    save_folder = os.path.join(\n",
    "        artifact_dir, \"{}_{}_{}_{}_{}\".format(\n",
    "            exp_args[\"synthesizer_llm\"],\n",
    "            exp_args[\"chunk_size\"],\n",
    "            exp_args[\"chunk_overlap\"],\n",
    "            exp_args[\"description\"],\n",
    "            datetime.now().strftime(\"%d-%m-%Y-%H-%M\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        print(save_folder)\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    logger = get_experiment_logs(exp_args[\"description\"], log_folder=save_folder)\n",
    "\n",
    "    db_directory = os.path.join(\n",
    "        emb_folder, exp_args[\"vectorstore\"],\n",
    "        \"{}_{}_{}\".format(exp_args[\"emb_type\"], exp_args[\"chunk_size\"], exp_args[\"chunk_overlap\"])\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"--------------------\\nLoading VectorDB from {db_directory}\")\n",
    "\n",
    "    token_counter = TokenCountingHandler(\n",
    "        tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    "    )\n",
    "    callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "    if not query_engine:\n",
    "        query_engine = setup_query_engine(\n",
    "            db_directory,\n",
    "            emb_store_type=exp_args[\"vectorstore\"],\n",
    "            index_name=exp_args[\"index_name\"],\n",
    "            similarity_top_k=exp_args[\"similarity_top_k\"],\n",
    "            text_qa_template=text_qa_template,\n",
    "            synthesizer_llm = exp_args[\"synthesizer_llm\"],\n",
    "            synthesizer_temperature = exp_args[\"synthesizer_temperature\"],\n",
    "            synthesizer_max_tokens = exp_args[\"synthesizer_max_tokens\"],\n",
    "            response_mode = \"simple_summarize\",\n",
    "            node_postprocessors = node_postprocessors,\n",
    "            callback_manager = callback_manager\n",
    "        )\n",
    "\n",
    "    logger.info(\n",
    "        \"-------------\\nExperiment settings:\\n{}\".format(\n",
    "            \"\\n\".join([f\"{k}:{v}\" for k, v in exp_args.items()])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(save_folder, \"settings.yaml\"), \"w\") as f:\n",
    "        yaml.dump(exp_args, f)\n",
    "\n",
    "    token_counter.reset_counts()\n",
    "    responses = []\n",
    "\n",
    "    logger.info(\n",
    "        \"-------------\\nPROMPT: {}\".format(convert_prompt_to_string(query_engine._response_synthesizer._text_qa_template))\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"------START RUNNING TEST CASES---------\"\n",
    "    )\n",
    "\n",
    "    for test_case in tqdm(testcase_df[\"queries\"], total=len(testcase_df[\"queries\"])):\n",
    "        response = query_engine.query(test_case)\n",
    "        responses.append(response)\n",
    "        \n",
    "    logger.info(\"--------------\\nTokens Consumption: Total: {}, Prompt: {}, Completion: {}, Embeddings: {}\"\n",
    "                .format(token_counter.total_llm_token_count,\n",
    "                        token_counter.prompt_llm_token_count,\n",
    "                        token_counter.completion_llm_token_count,\n",
    "                        token_counter.embedding_token_counts))\n",
    "\n",
    "    logger.info(f\"----------\\nTest case Completed. Saving Artifacts into {save_folder}\")\n",
    "    json_responses = process_result_json(\n",
    "        testcase_df, responses=responses, save_path=os.path.join(save_folder, \"results.json\")\n",
    "    )\n",
    "\n",
    "    result_df = process_result_df(\n",
    "        testcase_df, json_responses, save_path=os.path.join(save_folder, \"result.csv\")\n",
    "        )\n",
    "\n",
    "    accuracy = result_df[\"match\"].sum() / len(result_df) * 100\n",
    "\n",
    "    logger.info(\"------EVALUATION-----\")\n",
    "    logger.info(f\"Accuracy score: {accuracy}\")\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"gpt_classification\", \"human_gt\"])[\"match\"].value_counts())\n",
    "    )\n",
    "    logger.info(\n",
    "        str(result_df.groupby([\"human_gt\", \"gpt_classification\"])[\"match\"].value_counts())\n",
    "    )\n",
    "\n",
    "    return json_responses, result_df, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCombinedRetriever(BaseRetriever):\n",
    "    def __init__(\n",
    "        self,\n",
    "        table_retriever: BaseRetriever,\n",
    "        text_retriever: BaseRetriever,\n",
    "        token_limit: int = 6000,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.table_retriever = table_retriever\n",
    "        self.text_retriever = text_retriever\n",
    "        self._token_limit = token_limit\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "        token_counter = 0\n",
    "        \n",
    "        table_nodes = self.table_retriever.retrieve(query_bundle)\n",
    "        text_nodes = self.text_retriever.retrieve(query_bundle)\n",
    "\n",
    "        text_retrieved_nodes = []\n",
    "        table_retrieved_nodes = []\n",
    "        for node in table_nodes:\n",
    "            node_tokens = count_tokens(node)\n",
    "            if token_counter + node_tokens <= self._token_limit:\n",
    "                table_retrieved_nodes.append(node)\n",
    "                token_counter += node_tokens\n",
    "            else:\n",
    "                Warning(\"Maximum Tokens Exceeded from Table Nodes.\")\n",
    "                break\n",
    "            \n",
    "        for node in text_nodes:\n",
    "            node_tokens = count_tokens(node)\n",
    "            if token_counter + node_tokens <= self._token_limit:\n",
    "                text_retrieved_nodes.append(node)\n",
    "                token_counter += node_tokens\n",
    "            else:\n",
    "                Warning(\"Maximum Tokens Exceeded from Text Nodes.\")\n",
    "                break\n",
    " \n",
    "        text_retrieved_nodes = list(reversed(text_retrieved_nodes))\n",
    " \n",
    "        return table_retrieved_nodes + text_retrieved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Appropriateness Category', 'MRI scan ordered',\n",
    "                   'Difficulty', 'Clinical File']\n",
    "        )\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"MRI scan ordered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs before filtering: 546\n",
      "Total number of docs after filtering 395\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "print(\"Total no of docs before filtering:\", len(documents))\n",
    "with open(EXCLUDE_DICT, \"r\") as f:\n",
    "    exclude_pages = json.load(f)\n",
    "documents = filter_by_pages(doc_list=documents, exclude_info=exclude_pages)\n",
    "print(\"Total number of docs after filtering\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiment (Rau et al 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Prompt\n",
    "CHAT_PROMPT_TEMPLATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"simple\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 3,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"BaselineExperimentRau2023\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"compact\",\n",
    ")\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_prompt_template = (\n",
    "    \"Case: {input_text}\\n\"\n",
    "    \"Scan Ordered: {scan_order}\\n\"\n",
    "    \"Question: Is this imaging modality for this case USUALLY APPROPRIATE, \"\n",
    "    \"MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. \"\n",
    "    \"Then state precisely the most appropriate imaging modality and if contrast \"\n",
    "    \"agent is needed\"\n",
    "    )\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": patient_profile, \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/User/Desktop/lbp_mri/artifacts/gpt-3.5-turbo_512_20_BaselineExperimentRau2023_22-10-2023-15-34\n"
     ]
    }
   ],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiment (Rau et al 2023) - Remove Final Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Prompt\n",
    "CHAT_PROMPT_TEMPLATE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"simple\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 3,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"BaselineExperimentRau2023RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"compact\",\n",
    ")\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rau_prompt_template = (\n",
    "    \"Case: {input_text}\\n\"\n",
    "    \"Scan Ordered: {scan_order}\\n\"\n",
    "    \"Question: Is this imaging modality for this case USUALLY APPROPRIATE, \"\n",
    "    \"MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or INSUFFICIENT INFORMATION. \"\n",
    "    \"Then state precisely the most appropriate imaging modality and if contrast \"\n",
    "    \"agent is needed\"\n",
    "    )\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(rau_prompt_template, {\"input_text\": remove_final_sentence(patient_profile), \"scan_order\": scan_order})\n",
    "    for patient_profile, scan_order in zip(patient_profiles, scan_orders)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/lbp_mri/artifacts/gpt-4_512_20_BaselineExperimentRau2023RemoveFinalSentence_25-10-2023-13-38\n",
      "2023-10-25 13:38:49,919:INFO: --------------------\n",
      "Loading VectorDB from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/emb_store/simple/openai_512_20\n",
      "2023-10-25 13:39:24,168:INFO: simple VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/emb_store/simple/openai_512_20.\n",
      "2023-10-25 13:39:24,170:INFO: -------------\n",
      "Experiment settings:\n",
      "emb_type:openai\n",
      "vectorstore:simple\n",
      "chunk_size:512\n",
      "chunk_overlap:20\n",
      "similarity_top_k:3\n",
      "index_name:msk-mri\n",
      "description:BaselineExperimentRau2023RemoveFinalSentence\n",
      "synthesizer_llm:gpt-4\n",
      "synthesizer_max_tokens:512\n",
      "synthesizer_temperature:0\n",
      "response_mode:compact\n",
      "2023-10-25 13:39:24,177:INFO: -------------\n",
      "PROMPT: Context information is below.\n",
      "---------------------\n",
      "context_str\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: query_str\n",
      "Answer: \n",
      "2023-10-25 13:39:24,180:INFO: ------START RUNNING TEST CASES---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [12:07<00:00, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 13:51:31,236:INFO: --------------\n",
      "Tokens Consumption: Total: 99669, Prompt: 93907, Completion: 5762, Embeddings: []\n",
      "2023-10-25 13:51:31,239:INFO: ----------\n",
      "Test case Completed. Saving Artifacts into /mnt/c/Users/QUAN/Desktop/lbp_mri/artifacts/gpt-4_512_20_BaselineExperimentRau2023RemoveFinalSentence_25-10-2023-13-38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 30/71 [00:36<00:47,  1.15s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "100%|██████████| 71/71 [11:31<00:00,  9.74s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 14:03:03,215:INFO: ------EVALUATION-----\n",
      "2023-10-25 14:03:03,216:INFO: Accuracy score: 47.88732394366197\n",
      "2023-10-25 14:03:03,220:INFO: gpt_classification        human_gt                  match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True      2\n",
      "                          USUALLY NOT APPROPRIATE   False     1\n",
      "MAY BE APPROPRIATE        INSUFFICIENT INFORMATION  False     1\n",
      "                          MAY BE APPROPRIATE        True      5\n",
      "                          USUALLY APPROPRIATE       False     2\n",
      "                          USUALLY NOT APPROPRIATE   False     4\n",
      "USUALLY APPROPRIATE       INSUFFICIENT INFORMATION  False     6\n",
      "                          MAY BE APPROPRIATE        False     7\n",
      "                          USUALLY APPROPRIATE       True     17\n",
      "                          USUALLY NOT APPROPRIATE   False    12\n",
      "USUALLY NOT APPROPRIATE   INSUFFICIENT INFORMATION  False     1\n",
      "                          MAY BE APPROPRIATE        False     2\n",
      "                          USUALLY APPROPRIATE       False     1\n",
      "                          USUALLY NOT APPROPRIATE   True     10\n",
      "Name: match, dtype: int64\n",
      "2023-10-25 14:03:03,222:INFO: human_gt                  gpt_classification        match\n",
      "INSUFFICIENT INFORMATION  INSUFFICIENT INFORMATION  True      2\n",
      "                          MAY BE APPROPRIATE        False     1\n",
      "                          USUALLY APPROPRIATE       False     6\n",
      "                          USUALLY NOT APPROPRIATE   False     1\n",
      "MAY BE APPROPRIATE        MAY BE APPROPRIATE        True      5\n",
      "                          USUALLY APPROPRIATE       False     7\n",
      "                          USUALLY NOT APPROPRIATE   False     2\n",
      "USUALLY APPROPRIATE       MAY BE APPROPRIATE        False     2\n",
      "                          USUALLY APPROPRIATE       True     17\n",
      "                          USUALLY NOT APPROPRIATE   False     1\n",
      "USUALLY NOT APPROPRIATE   INSUFFICIENT INFORMATION  False     1\n",
      "                          MAY BE APPROPRIATE        False     4\n",
      "                          USUALLY APPROPRIATE       False    12\n",
      "                          USUALLY NOT APPROPRIATE   True     10\n",
      "Name: match, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 - GPT-4, Chunk=512, Chunk_no=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK:\n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Given the PATIENT PROFILE and CONTEXT, refer to the SCORING CRITERIA and recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "\n",
    "Format your output as follow:\n",
    "1. Classification: Can be one of [USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE, INSUFFICIENT INFORMATION]\n",
    "2. Explanation:\n",
    "3. Recommendation: Can be alternative procedure, NO SCAN REQUIRED or NO CHANGE REQUIRED \n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=5_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2 - GPT-4, Chunk=1024, Overlap=128 Chunk_no=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: \n",
    "1. Extract from given PATIENT PROFILE relevant information for classification of imaging appropriateness.\n",
    "Important information includes AGE, SYMPTOMS, DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Given the PATIENT PROFILE and CONTEXT, refer to the SCORING CRITERIA and recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "OUTPUT INSTRUCTIONS:\n",
    "Your output should contain the following:\n",
    "1. Classification of appropriateness for the ordered scan.\n",
    "2. Provide explanation for the appropriateness classification.\n",
    "3. If classification answer is USUALLY NOT APPROPRIATE, either recommend an alternative appropriate scan procedure or return NO SCAN REQUIRED.\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args_4 = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128,\n",
    "    similarity_top_k = 7,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=7_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args_4,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3 - GPT-4, Chunk=512, Chunk_no=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate if the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from given PATIENT PROFILE relevant information including AGE, SYMPTOMS, previous DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Based on the SCORING CRITERIA, recommend if the image scan ordered is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT information to conclude the appropriateness\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"Topk=5_RemoveFinalSentence\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-4\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "node_postprocessors = [LongContextReorder()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 4 - Separate Tables and Text Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Settings Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from PATIENT PROFILE relevant information for classification of imaging appropriateness. DO NOT make any assumptions from the SCAN ORDER.\n",
    "Important information includes AGE, SYMPTOMS, PREVIOUS DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Identify there are superior imaging procedures or treatments with a more favorable risk-benefit ratio.\n",
    "4. Based on the SCORING CRITERIA, recommend if the SCAN ORDER is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT to give the recommendation\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Take note for scenarios involving IV CONTRAST there are 3 distinct scan protocols: (1) with IV CONTRAST, (2) without IV CONTRAST, (3) without and with IV CONTRAST. \n",
    "Each of them is different and can have different appropriateness category.\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    table_similarity_top_k = 4,\n",
    "    text_similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"DescriptionsTableAndText\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "# node_postprocessors = [LongContextReorder()]\n",
    "node_postprocessors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-28 20:19:34,356:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/descriptions/tables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-28 20:19:34,432:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-faiss\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"table_similarity_top_k\"],\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"text_similarity_top_k\"],\n",
    ")\n",
    "text_and_table_retriever = CustomCombinedRetriever(\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever, token_limit = 7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = OpenAIEmbedding()\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        temperature=exp_args[\"synthesizer_temperature\"],\n",
    "        model=exp_args[\"synthesizer_llm\"], max_tokens=exp_args[\"synthesizer_max_tokens\"]\n",
    "        ),\n",
    "    embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=exp_args[\"response_mode\"],\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=text_and_table_retriever, response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    query_engine=query_engine,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 5 - Multisteps LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Relevant Info First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_guidelines = \"\\n\".join([\"- \" + guideline for guideline in GUIDELINES])\n",
    "\n",
    "extract_template = \"\"\"You are a radiologist expert. Do not make up additional information.\n",
    "=========\n",
    "TASK: You are given a PATIENT PROFILE. You need to perform the following information referencing from the PATIENT PROFILE:\n",
    "1. Extract relevant information for recommendation of imaging procedure, including age, symptomps, previous diagnosis, stage of diagnosis (INITIAL IMAGING OR NEXT STUDY) and suspected conditions, if any.\n",
    "Only return information given inside the PROFILE, do not make up other information.\n",
    "2. Return one or more guidelines from the following list of guidelines potentially relevant to the recommendations of imaging procedure given patient profile. If there are no relevant guidelines, return empty list.\n",
    "{}\n",
    "=========\n",
    "OUTPUT INSTRUCTION:\n",
    "Output your answer as follow:\n",
    "1. Relevant information:\n",
    "2. Relevant guidelines: List of guidelines. Match the exact text given in the list. If no relevant guidelines, return [].\n",
    "=========\n",
    "\"\"\".format(all_guidelines)\n",
    "\n",
    "human_template = \"PATIENT PROFILE: {query_str}\"\n",
    "\n",
    "EXTRACT_TEMPLATE = LCChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(extract_template),\n",
    "        HumanMessagePromptTemplate.from_template(human_template)\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_chain = LLMChain(\n",
    "    prompt=EXTRACT_TEMPLATE,\n",
    "    llm=ChatOpenAI(model=\"gpt-4\", temperature=0, max_tokens=512)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [15:07<00:00, 12.78s/it]\n"
     ]
    }
   ],
   "source": [
    "multiple_guideline_answers = []\n",
    "for clinical_file in tqdm(testcase_df[\"Clinical File\"],\n",
    "                          total=len(testcase_df[\"Clinical File\"])):\n",
    "    profile = remove_final_sentence(clinical_file)\n",
    "    response = extract_chain(profile)\n",
    "    multiple_guideline_answers.append(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles, guidelines = [], []\n",
    "\n",
    "for answer in multiple_guideline_answers:\n",
    "    if answer.endswith(\".\"):\n",
    "        answer = answer[:-1]\n",
    "    pattern = r\"1. Relevant information:([\\S\\s]+)2. Relevant guidelines:([\\S\\s]*)\"\n",
    "    \n",
    "    profile, guidelines_str = re.findall(pattern, answer)[0]\n",
    "    guidelines_str = guidelines_str.replace(\"- \", \"\")\n",
    "    guidelines_str = guidelines_str.strip()\n",
    "    guidelines_str = guidelines_str.replace(\"\\n\", \", \")\n",
    "    \n",
    "    if not guidelines_str:\n",
    "        profiles.append(profile)\n",
    "        guidelines.append([])\n",
    "    else:\n",
    "        regex_guidelines = re.findall(r\"([A-Za-z ]+)\", guidelines_str)\n",
    "        extracted_guidelines = []\n",
    "        for i, extracted_guideline in enumerate(regex_guidelines):\n",
    "            extracted_guideline = extracted_guideline.lower()\n",
    "            min_dist, nearest_text = calculate_min_dist(extracted_guideline, GUIDELINES, True)\n",
    "            if min_dist <= 1:\n",
    "                extracted_guideline = nearest_text\n",
    "                extracted_guidelines.append(extracted_guideline)\n",
    "            else:\n",
    "                print(extracted_guideline + \"_\" + nearest_text)\n",
    "                \n",
    "        profiles.append(profile.strip())   \n",
    "        guidelines.append(extracted_guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_info_multiple_json = {\"profiles\": profiles, \"guidelines\": guidelines}\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"w\") as f:\n",
    "    json.dump(extracted_info_multiple_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Multi-Step Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a radiologist expert at providing imaging recommendations for patients with musculoskeletal conditions.\n",
    "If you do not know an answer, just say \"I dont know\", do not make up an answer.\n",
    "==========\n",
    "TASK: You are given a PATIENT PROFILE and a SCAN ORDER. Your task is to evaluate the appropriateness of the SCAN ORDER based on the PATIENT PROFILE.\n",
    "Perform step-by-step the following sequence of reasoning.\n",
    "1. Extract from PATIENT PROFILE relevant information for classification of imaging appropriateness. DO NOT make any assumptions from the SCAN ORDER.\n",
    "Important information includes AGE, SYMPTOMS, PREVIOUS DIAGNOSIS (IF ANY), which stage of diagnosis (INITIAL IMAGING OR NEXT STUDY).\n",
    "2. Refer to the reference information given under CONTEXT to analyse the appropriate imaging recommendations given the patient profile.\n",
    "3. Identify there are superior imaging procedures or treatments with a more favorable risk-benefit ratio.\n",
    "4. Based on the SCORING CRITERIA, recommend if the SCAN ORDER is USUALLY APPROPRIATE, MAY BE APPROPRIATE, USUALLY NOT APPROPRIATE or there is INSUFFICIENT INFORMATION to recommend the appropriateness.  \n",
    "If the scan is not appropriate, recommend an appropriate procedure.\n",
    "\n",
    "STRICTLY answer based on the given PATIENT PROFILE and CONTEXT. \n",
    "==========\n",
    "SCORING CRITERIA:\n",
    "- USUALLY APPROPRIATE: The imaging procedure or treatment is indicated in the specified clinical scenarios at a favorable risk-benefit ratio for patients.\n",
    "- MAY BE APPROPRIATE: The imaging procedure or treatment may be indicated in the specified clinical scenarios as an alternative to imaging procedures or treatments with a more favorable risk-benefit ratio, or the risk-benefit ratio for patients is equivocal.\n",
    "- USUALLY NOT APPROPRIATE: The imaging procedure or treatment is unlikely to be indicated in the specified clinical scenarios, or the risk-benefit ratio for patients is likely to be unfavorable.\n",
    "- INSUFFICIENT INFORMATION: There is not enough information from PATIENT PROFILE and CONTEXT to give the recommendation\n",
    "==========\n",
    "CONTEXT:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Take note for scenarios involving IV CONTRAST there are 3 distinct scan protocols: (1) with IV CONTRAST, (2) without IV CONTRAST, (3) without and with IV CONTRAST. \n",
    "Each of them is different and can have different appropriateness category.\n",
    "==========\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{query_str}\"\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=system_template),\n",
    "    ChatMessage(role=MessageRole.USER, content=human_template)   \n",
    "]\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args = dict(\n",
    "    # Retrieval \n",
    "    emb_type = \"openai\",\n",
    "    vectorstore = \"faiss\",\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 20,\n",
    "    table_similarity_top_k = 4,\n",
    "    text_similarity_top_k = 5,\n",
    "    index_name = \"msk-mri\",\n",
    "    description=\"DescriptionsTableAndText\",\n",
    "\n",
    "    # Generation\n",
    "    synthesizer_llm = \"gpt-3.5-turbo\",\n",
    "    synthesizer_max_tokens = 512,\n",
    "    synthesizer_temperature = 0,\n",
    "    response_mode = \"simple_summarize\",\n",
    ")\n",
    "\n",
    "text_qa_template = CHAT_PROMPT_TEMPLATE\n",
    "# node_postprocessors = [LongContextReorder()]\n",
    "node_postprocessors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-24 20:26:54,654:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal/descriptions/tables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-24 20:26:54,733:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"table_similarity_top_k\"],\n",
    "    filters = None\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = exp_args[\"text_similarity_top_k\"],\n",
    "    filters = None\n",
    ")\n",
    "text_and_table_retriever = CustomCombinedRetriever(\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever, token_limit = 7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = OpenAIEmbedding()\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(exp_args[\"synthesizer_llm\"]).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        temperature=exp_args[\"synthesizer_temperature\"],\n",
    "        model=exp_args[\"synthesizer_llm\"], max_tokens=exp_args[\"synthesizer_max_tokens\"]\n",
    "        ),\n",
    "    embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=exp_args[\"response_mode\"],\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=text_and_table_retriever, response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses, result_df, responses = run_test_cases(\n",
    "    testcase_df=testcase_df,\n",
    "    exp_args=exp_args,\n",
    "    query_engine=query_engine,\n",
    "    text_qa_template=text_qa_template,\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyRetriever(BaseRetriever):\n",
    "        \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        no_info_doc = Document(\n",
    "            text=\"There is insufficient information about this condition\"\n",
    "        )\n",
    "        return [NodeWithScore(node=no_info_doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_testcase = testcase_df[\"Clinical File\"].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile, scan_order = remove_final_sentence(sample_testcase, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_response = extract_chain(profile)[\"text\"]\n",
    "if extracted_response.endswith(\".\"):\n",
    "    extracted_response = extracted_response[:-1]\n",
    "\n",
    "pattern = r\"1. Relevant information:([\\S\\s]+)2. Relevant guidelines:([\\S\\s]*)\"\n",
    "\n",
    "profile, guidelines_str = re.findall(pattern, extracted_response)[0]\n",
    "guidelines_str = guidelines_str.replace(\"- \", \"\")\n",
    "guidelines_str = guidelines_str.strip()\n",
    "guidelines_str = guidelines_str.replace(\"\\n\", \", \")\n",
    "\n",
    "if not guidelines_str:\n",
    "    profiles.append(profile)\n",
    "    guidelines.append([])\n",
    "else:\n",
    "    regex_guidelines = re.findall(r\"([A-Za-z ]+)\", guidelines_str)\n",
    "    extracted_guidelines = []\n",
    "    for i, extracted_guideline in enumerate(regex_guidelines):\n",
    "        extracted_guideline = extracted_guideline.lower()\n",
    "        min_dist, nearest_text = calculate_min_dist(extracted_guideline, GUIDELINES, True)\n",
    "        if min_dist <= 1:\n",
    "            extracted_guideline = nearest_text\n",
    "            extracted_guidelines.append(extracted_guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_retriever = EmptyRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(\n",
    "        temperature=exp_args[\"synthesizer_temperature\"],\n",
    "        model=\"gpt-4\", max_tokens=exp_args[\"synthesizer_max_tokens\"]\n",
    "        ),\n",
    "    embed_model=embs, callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context, response_mode=\"compact\",\n",
    "    text_qa_template=CHAT_PROMPT_TEMPLATE, verbose=True\n",
    ")\n",
    "\n",
    "# query_engine = RetrieverQueryEngine(\n",
    "#     retriever=text_and_table_retriever, response_synthesizer=response_synthesizer,\n",
    "#     node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    "# )\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=empty_retriever, response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors = node_postprocessors, callback_manager = CallbackManager([token_counter])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = \"Patient Profile: {}\\nScan Order: {}\".format(profile, scan_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Relevant information from the patient profile: The patient is a 58-year-old male. There are no symptoms, previous diagnosis, stage of diagnosis, or suspected conditions provided in the profile.\n",
      "\n",
      "2. The context does not provide any specific information about the condition or the appropriateness of the scan order.\n",
      "\n",
      "3. Without any information about the patient's symptoms or medical history, it's not possible to identify superior imaging procedures or treatments.\n",
      "\n",
      "4. Based on the scoring criteria, there is INSUFFICIENT INFORMATION to recommend the appropriateness of the scan order. More information about the patient's symptoms, medical history, and reason for suspecting Charcot foot is needed to make an appropriate recommendation.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appropriateness: INSUFFICIENT INFORMATION\n",
      "Recommendation: None\n"
     ]
    }
   ],
   "source": [
    "print(fixing_chain(response.response)[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
