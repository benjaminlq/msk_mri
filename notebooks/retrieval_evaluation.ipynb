{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MAIN_DIR\n",
    "from copy import deepcopy\n",
    "from custom_storage import load_vectorindex\n",
    "from pprint import pprint\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from textdistance import levenshtein\n",
    "from typing import Dict, Union, List, Literal, Sequence, Optional\n",
    "import numpy as np\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "from utils import count_tokens, filter_by_pages\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.indices.base_retriever import BaseRetriever\n",
    "from llama_index.schema import Document, NodeWithScore, MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, NodeWithScore):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": doc.score\n",
    "            } \n",
    "    elif isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": \"\"\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"score\": \"None\"\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def calculate_emb_distance(\n",
    "    emb1: List[float],\n",
    "    emb2: List[float],\n",
    "    dist_type: Literal[\"l2\", \"ip\", \"cosine\", \"neg_exp_l2\"] = \"l2\"\n",
    "):\n",
    "    assert len(emb1) == len(emb2), \"Length of embedding vectors must match\"\n",
    "    if dist_type == \"l2\":\n",
    "        return np.square(np.linalg.norm(np.array(emb1) - np.array(emb2)))\n",
    "    elif dist_type == \"ip\":\n",
    "        return 1 - np.dot(emb1, emb2)\n",
    "    elif dist_type == \"cosine\":\n",
    "        cosine_similarity = np.dot(emb1, emb2)/(np.norm(emb1)*np.norm(emb2))\n",
    "        return 1 - cosine_similarity\n",
    "    elif dist_type == \"neg_exp_l2\":\n",
    "        return np.exp(-np.square(np.linalg.norm(np.array(emb1) - np.array(emb2))))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance type\")\n",
    "    \n",
    "def calculate_string_distance(\n",
    "    str1: str,\n",
    "    str2: Union[str, Sequence[str]],\n",
    "    embeddings: BaseEmbedding,\n",
    "    dist_type: Literal[\"l2\", \"ip\", \"cosine\", \"neg_exp_l2\"] = \"l2\"\n",
    "):\n",
    "    emb1 = embeddings.get_query_embedding(str1)\n",
    "    if isinstance(str2, str):\n",
    "        emb2 = embeddings.get_text_embedding(str2)\n",
    "        return calculate_emb_distance(emb1, emb2, dist_type)\n",
    "    else:\n",
    "        emb2_list = embeddings.get_text_embedding_batch(str2)\n",
    "        return [calculate_emb_distance(emb1, emb2) for emb2 in emb2_list]\n",
    "    \n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Guideline', 'Variant', 'Appropriateness Category',\n",
    "                   'MRI scan ordered', 'Clinical File']\n",
    "        )\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"MRI scan ordered\"]\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs before filtering: 546\n",
      "Total number of docs after filtering 395\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "print(\"Total no of docs before filtering:\", len(documents))\n",
    "with open(EXCLUDE_DICT, \"r\") as f:\n",
    "    exclude_pages = json.load(f)\n",
    "documents = filter_by_pages(doc_list=documents, exclude_info=exclude_pages)\n",
    "print(\"Total number of docs after filtering\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LLM Extraction Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_best.json\"), \"r\") as f:\n",
    "    extracted_best = json.load(f)\n",
    "    extracted_profiles = extracted_best[\"profiles\"]\n",
    "    extracted_best_guidelines = extracted_best[\"guidelines\"]\n",
    "    \n",
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"r\") as f:\n",
    "    extracted_multiple = json.load(f)\n",
    "    extracted_multiple_guidelines = extracted_multiple[\"guidelines\"]\n",
    "    \n",
    "extracted_profiles = [profile.strip() for profile in extracted_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = deepcopy(testcase_df)\n",
    "emb_df[\"llm_profile\"] = extracted_profiles\n",
    "emb_df[\"best_guideline\"] = extracted_best_guidelines\n",
    "emb_df[\"multiple_guidelines\"] = extracted_multiple_guidelines\n",
    "emb_df[\"condition\"] = emb_df[\"Guideline\"].apply(lambda x: x[4:-4]).str.lower()\n",
    "emb_df[\"best_hit\"] = [(guideline in best_guideline) or (app==\"ICI\" and not best_guideline)\n",
    "                      for guideline, best_guideline, app in zip(emb_df[\"condition\"], emb_df[\"best_guideline\"], emb_df[\"Appropriateness Category\"])]\n",
    "emb_df[\"multiple_hit\"] = [(guideline in multiple_guidelines) or (app==\"ICI\" and not multiple_guidelines)\n",
    "                          for guideline, multiple_guidelines, app in zip(emb_df[\"condition\"], emb_df[\"multiple_guidelines\"], emb_df[\"Appropriateness Category\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate:  0.8873239436619719\n",
      "Multiple Hit Rate:  0.9436619718309859\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hit Rate: \", emb_df[\"best_hit\"].sum()/len(emb_df))\n",
    "print(\"Multiple Hit Rate: \", emb_df[\"multiple_hit\"].sum()/len(emb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: 19\n",
      "('Patient: 46 year old Chinese male.  Businessman, sales.  Frequent drinker '\n",
      " 'due to job, about 1-2 beers/day.  Past medical history of fatty liver, acute '\n",
      " 'cholecystitis post cholecystectomy.  Now presenting with severe pain at '\n",
      " 'right big toe for 2 weeks, pain on and off improves with paracetamol and '\n",
      " 'ibuprofen.  On examination: swelling and erythema at right big toe 1st '\n",
      " 'metatarsophalangeal joint.  No prior imaging')\n",
      "('Variant: Chronic extremity joint pain. Suspect inflammatory (seropositive or '\n",
      " 'seronegative arthritis), crystalline (gout or pseudogout), or erosive '\n",
      " 'osteoarthritis. Initial imaging.')\n",
      "['chronic foot pain', 'suspected om septic arthritis soft tissue infection']\n",
      "'ACR chroni extremity joint pain inflammatory arthritis.pdf'\n",
      "\n",
      "Case: 20\n",
      "('Patient: 55 year old Philipino female.  Domestic helper.  No significant '\n",
      " 'past medical history.  Bilateral finger joint pain, swelling and stiffness '\n",
      " 'worse in the morning for 4 months, worsening recently.  No eye redness, '\n",
      " 'shortness of breath, fever, loss of weight/appetite.  No other joint '\n",
      " \"involvement.  On examination: Heberden's nodes bilateral distal \"\n",
      " 'interphalangeal joints, mild swelling, no significant tenderness.  X-rays '\n",
      " 'show degenerative changes in bilateral hand interphalangeal joints with '\n",
      " 'equivocal gull-wing deformity')\n",
      "('Variant: Chronic extremity joint pain. Suspect erosive osteoarthritis. '\n",
      " 'Radiographs normal or inconclusive. Next imaging study.')\n",
      "['chronic hand and wrist pain']\n",
      "'ACR chroni extremity joint pain inflammatory arthritis.pdf'\n",
      "\n",
      "Case: 45\n",
      "('Patient: 13 year old Caucasian female.  No prior medical history.  Does 200 '\n",
      " 'metre sprinting for school.  Now presenting with 3 month history of right '\n",
      " 'knee pain, sometimes waking her up at night.  No fever.  No inciting '\n",
      " 'trauma.  No loss of weight or appetitie.  On examination, mild swelling in '\n",
      " 'right knee.  Otherwise unremarkable examination.  No prior imaging')\n",
      "'Variant: Suspect primary bone tumor. Initial imaging.'\n",
      "['chronic knee pain', 'stress fracture including sacrum']\n",
      "'ACR primary bone tumours.pdf'\n",
      "\n",
      "Case: 59\n",
      "('Patient: 73 year old Indian male, with history of end stage diabetic '\n",
      " 'nephropathy on haemodialysis. Complains of lower back pain for the past 2 '\n",
      " 'months with occasional night pain. Associated with radicular pain of left '\n",
      " 'lower limb. no fever. Physical exam reveals mild tenderness at L3-L4 region '\n",
      " 'but otherwise unremarkable. No prior imaging')\n",
      "('Variant: Suspected spine infection (such as epidural abscess or discitis '\n",
      " 'osteomyelitis), with new or worsening back or neck pain, with or without '\n",
      " 'fever, who may have one or more of the following red flags (diabetes '\n",
      " 'mellitus, IV drug use, cancer, HIV, or dialysis) or abnormal lab values. '\n",
      " 'Initial imaging.')\n",
      "['low back pain', 'inflammatory back pain', 'myelopathy']\n",
      "'ACR suspected spine infection.pdf'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in emb_df[~emb_df[\"multiple_hit\"]].index:\n",
    "    case_file = emb_df.iloc[idx, :]\n",
    "    print(\"Case:\", idx + 1)\n",
    "    pprint(\"Patient: \" + remove_final_sentence(case_file[\"Clinical File\"]))\n",
    "    pprint(\"Variant: \" + case_file[\"ACR scenario\"])\n",
    "    pprint(case_file[\"multiple_guidelines\"])\n",
    "    pprint(case_file[\"Guideline\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_analysis_path = os.path.join(ARTIFACT_DIR, \"emb_analysis\")\n",
    "if not os.path.exists(emb_analysis_path):\n",
    "    os.makedirs(emb_analysis_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df[\"original_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"Clinical File\"], x[\"ACR scenario\"], embed_model), axis=1)\n",
    "emb_df[\"query_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"queries\"], x[\"ACR scenario\"], embed_model), axis=1)\n",
    "emb_df[\"refine_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"llm_profile\"], x[\"ACR scenario\"], embed_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.to_csv(os.path.join(emb_analysis_path, \"emb_analysis.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 17:46:34,019:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/tables.\n",
      "2023-10-26 17:46:34,268:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-chroma\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"chroma\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"chroma\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)\n",
    "- Table Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:06:38,503:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/full/tables.\n",
      "2023-10-26 12:06:38,719:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/full/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-faiss\", \"full\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = 5, filters = None\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = 5, filters = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_table_nodes = []\n",
    "retrieved_text_nodes = []\n",
    "for test_case in testcase_df[\"queries\"]:\n",
    "    retrieved_table_nodes.append(table_retriever.retrieve(test_case))\n",
    "    retrieved_text_nodes.append(text_retriever.retrieve(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_full\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Retrieved Context from Nodes\n",
    "\n",
    "description_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_tables = [[] for _ in range(5)]\n",
    "retrieved_texts = [[] for _ in range(5)]\n",
    "for i in range(len(description_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "        retrieved_tables[idx].append(node_info)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "        retrieved_texts[idx].append(node_info)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_tables):\n",
    "    description_df[f\"Table_{idx+1}\"] = retrieved_tables[idx]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_texts):\n",
    "    description_df[f\"Text_{idx+1}\"] = retrieved_texts[idx]\n",
    "\n",
    "description_df.to_csv(os.path.join(save_folder, \"table_text.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIT Rate of Nodes at document level\n",
    "file_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_table_pages = [[] for _ in range(5)]\n",
    "retrieved_text_pages = [[] for _ in range(5)]\n",
    "for i in range(len(file_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        node_file = node.metadata[\"file_name\"]\n",
    "        retrieved_table_pages[idx].append(node_file)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        node_file = node.metadata[\"file_name\"]\n",
    "        retrieved_text_pages[idx].append(node_file)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_pages):\n",
    "    file_df[f\"Table_{idx+1}_Page\"] = retrieved_table_pages[idx]\n",
    "    file_df[f\"Table_{idx+1}_HIT\"] = file_df[f\"Table_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_text_pages):\n",
    "    file_df[f\"Text_{idx+1}_Page\"] = retrieved_text_pages[idx]\n",
    "    file_df[f\"Text_{idx+1}_HIT\"] = file_df[f\"Text_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "\n",
    "file_df[\"Total Table HITs\"] = file_df[[f\"Table_{idx+1}_HIT\"for idx in range(5)]].sum(axis=1)\n",
    "file_df[\"Total Text HITs\"] = file_df[[f\"Text_{idx+1}_HIT\"for idx in range(5)]].sum(axis=1)\n",
    "file_df[\"Total HITs\"] = file_df[[\"Total Table HITs\", \"Total Text HITs\"]].sum(axis=1)\n",
    "file_df.to_csv(os.path.join(save_folder, \"table_text_pages.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Distance of Nodes\n",
    "score_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_table_scores = [[] for _ in range(5)]\n",
    "retrieved_texts_scores = [[] for _ in range(5)]\n",
    "\n",
    "for i in range(len(score_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        retrieved_table_scores[idx].append(node.score)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        retrieved_texts_scores[idx].append(node.score)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_scores):\n",
    "    score_df[f\"Table_{idx+1}\"] = retrieved_table_scores[idx]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_texts_scores):\n",
    "    score_df[f\"Text_{idx+1}\"] = retrieved_texts_scores[idx]\n",
    "    \n",
    "score_df['avg_table'] = score_df[[f\"Table_{idx+1}\" for idx in range(5)]].mean(axis=1)\n",
    "score_df['avg_text'] = score_df[[f\"Text_{idx+1}\" for idx in range(5)]].mean(axis=1)\n",
    "score_df['avg_overall'] = score_df[[\"avg_table\", \"avg_text\"]].mean(axis=1)\n",
    "\n",
    "score_df.to_csv(os.path.join(save_folder, \"table_text_scores.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6619718309859155\n"
     ]
    }
   ],
   "source": [
    "# Generate Hit Rate of relevant table & Recall Score\n",
    "def add_punctuation(text: str):\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "    return text\n",
    "\n",
    "table_hitrate_df = deepcopy(testcase_df)\n",
    "\n",
    "table_hitrate_df['ACR scenario'] = table_hitrate_df['ACR scenario'].str.strip().apply(lambda x: add_punctuation(x))\n",
    "retrieved_table_descriptions = [[] for _ in range(5)]\n",
    "\n",
    "for i in range(len(table_hitrate_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        retrieved_table_descriptions[idx].append(node.metadata[\"description\"].strip())\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_descriptions):\n",
    "    table_hitrate_df[f\"Table_{idx+1}_descriptions\"] = retrieved_table_descriptions[idx]\n",
    "    table_hitrate_df[f\"Table_{idx+1}_LVscore\"] = table_hitrate_df.apply(lambda x: levenshtein.distance(x[\"ACR scenario\"], x[f\"Table_{idx+1}_descriptions\"]), axis=1)\n",
    "    \n",
    "table_hitrate_df[\"Min_LV_Dist\"] = table_hitrate_df[[f\"Table_{idx+1}_LVscore\" for idx in range(5)]].min(axis=1)\n",
    "table_hitrate_df[\"HIT\"] = table_hitrate_df[\"Min_LV_Dist\"] < 5\n",
    "\n",
    "print(table_hitrate_df[\"HIT\"].sum() / len(table_hitrate_df[\"HIT\"]))\n",
    "\n",
    "table_hitrate_df.to_csv(os.path.join(save_folder, \"hit_rate.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_score_df = pd.concat(\n",
    "    [score_df[['ACR scenario', 'Appropriateness Category', 'MRI scan ordered',\n",
    "       'Clinical File', 'queries', 'avg_table', 'avg_text', 'avg_overall']],\n",
    "     table_hitrate_df[['Min_LV_Dist', 'HIT']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2702985748691185, 0.20397829565210468)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_similarity = combined_score_df[combined_score_df[\"HIT\"]][\"avg_table\"]\n",
    "miss_simiarity = combined_score_df[~combined_score_df[\"HIT\"]][\"avg_table\"]\n",
    "\n",
    "ztest(hit_similarity.values, miss_simiarity.values, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-24 20:07:20,665:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal/descriptions/tables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-24 20:07:20,740:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_table_nodes = []\n",
    "retrieved_text_nodes = []\n",
    "for test_case in testcase_df[\"queries\"]:\n",
    "    retrieved_table_nodes.append(table_retriever.retrieve(test_case))\n",
    "    retrieved_text_nodes.append(text_retriever.retrieve(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_descriptions\")\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_tables = [[] for _ in range(5)]\n",
    "retrieved_texts = [[] for _ in range(5)]\n",
    "for i in range(len(description_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "        retrieved_tables[idx].append(node_info)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "        retrieved_texts[idx].append(node_info)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_tables):\n",
    "    description_df[f\"Table_{idx+1}\"] = retrieved_tables[idx]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_texts):\n",
    "    description_df[f\"Text_{idx+1}\"] = retrieved_texts[idx]\n",
    "\n",
    "description_df.to_csv(os.path.join(save_folder, \"table_text.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_table_pages = [[] for _ in range(5)]\n",
    "retrieved_text_pages = [[] for _ in range(5)]\n",
    "for i in range(len(file_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        node_file = node.metadata[\"file_name\"]\n",
    "        retrieved_table_pages[idx].append(node_file)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        node_file = node.metadata[\"file_name\"]\n",
    "        retrieved_text_pages[idx].append(node_file)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_pages):\n",
    "    file_df[f\"Table_{idx+1}_Page\"] = retrieved_table_pages[idx]\n",
    "    file_df[f\"Table_{idx+1}_HIT\"] = file_df[f\"Table_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_text_pages):\n",
    "    file_df[f\"Text_{idx+1}_Page\"] = retrieved_text_pages[idx]\n",
    "    file_df[f\"Text_{idx+1}_HIT\"] = file_df[f\"Text_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "\n",
    "file_df[\"Total Table HITs\"] = file_df[[f\"Table_{idx+1}_HIT\"for idx in range(5)]].sum(axis=1)\n",
    "file_df[\"Total Text HITs\"] = file_df[[f\"Text_{idx+1}_HIT\"for idx in range(5)]].sum(axis=1)\n",
    "file_df[\"Total HITs\"] = file_df[[\"Total Table HITs\", \"Total Text HITs\"]].sum(axis=1)\n",
    "file_df.to_csv(os.path.join(save_folder, \"table_text_pages.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = deepcopy(testcase_df)\n",
    "\n",
    "retrieved_table_scores = [[] for _ in range(5)]\n",
    "retrieved_texts_scores = [[] for _ in range(5)]\n",
    "\n",
    "for i in range(len(score_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        retrieved_table_scores[idx].append(node.score)\n",
    "    for idx, node in enumerate(retrieved_text_nodes[i]):\n",
    "        retrieved_texts_scores[idx].append(node.score)\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_scores):\n",
    "    score_df[f\"Table_{idx+1}\"] = retrieved_table_scores[idx]\n",
    "    \n",
    "for idx, texts in enumerate(retrieved_texts_scores):\n",
    "    score_df[f\"Text_{idx+1}\"] = retrieved_texts_scores[idx]\n",
    "    \n",
    "score_df['avg_table'] = score_df[[f\"Table_{idx+1}\" for idx in range(5)]].mean(axis=1)\n",
    "score_df['avg_text'] = score_df[[f\"Text_{idx+1}\" for idx in range(5)]].mean(axis=1)\n",
    "score_df['avg_overall'] = score_df[[\"avg_table\", \"avg_text\"]].mean(axis=1)\n",
    "\n",
    "score_df.to_csv(os.path.join(save_folder, \"table_text_scores.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7887323943661971\n"
     ]
    }
   ],
   "source": [
    "def add_punctuation(text: str):\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "    return text\n",
    "\n",
    "table_hitrate_df = deepcopy(testcase_df)\n",
    "\n",
    "table_hitrate_df['ACR scenario'] = table_hitrate_df['ACR scenario'].str.strip().apply(lambda x: add_punctuation(x))\n",
    "retrieved_table_descriptions = [[] for _ in range(5)]\n",
    "\n",
    "for i in range(len(table_hitrate_df[\"queries\"])):\n",
    "    for idx, node in enumerate(retrieved_table_nodes[i]):\n",
    "        retrieved_table_descriptions[idx].append(node.metadata[\"description\"].strip())\n",
    "        \n",
    "for idx, tables in enumerate(retrieved_table_descriptions):\n",
    "    table_hitrate_df[f\"Table_{idx+1}_descriptions\"] = retrieved_table_descriptions[idx]\n",
    "    table_hitrate_df[f\"Table_{idx+1}_LVscore\"] = table_hitrate_df.apply(lambda x: levenshtein.distance(x[\"ACR scenario\"], x[f\"Table_{idx+1}_descriptions\"]), axis=1)\n",
    "    \n",
    "table_hitrate_df[\"Min_LV_Dist\"] = table_hitrate_df[[f\"Table_{idx+1}_LVscore\" for idx in range(5)]].min(axis=1)\n",
    "table_hitrate_df[\"HIT\"] = table_hitrate_df[\"Min_LV_Dist\"] < 5\n",
    "\n",
    "print(table_hitrate_df[\"HIT\"].sum() / len(table_hitrate_df[\"HIT\"]))\n",
    "\n",
    "table_hitrate_df.to_csv(os.path.join(save_folder, \"hit_rate.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_score_df = pd.concat(\n",
    "    [score_df[['ACR scenario', 'Appropriateness Category', 'MRI scan ordered',\n",
    "       'Clinical File', 'queries', 'avg_table', 'avg_text', 'avg_overall']],\n",
    "     table_hitrate_df[['Min_LV_Dist', 'HIT']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.38182527768811947, 0.7025909678945342)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_similarity = combined_score_df[combined_score_df[\"HIT\"]][\"avg_table\"]\n",
    "miss_simiarity = combined_score_df[~combined_score_df[\"HIT\"]][\"avg_table\"]\n",
    "\n",
    "ztest(hit_similarity.values, miss_simiarity.values, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_analysis(\n",
    "    table_retriever: Optional[BaseRetriever] = None,\n",
    "    text_retriever: Optional[BaseRetriever] = None,\n",
    "    save_folder: Optional[str] = None\n",
    "):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
