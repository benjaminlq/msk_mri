{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MAIN_DIR\n",
    "from copy import deepcopy\n",
    "from utils import load_vectorindex, count_tokens, get_experiment_logs, filter_by_pages\n",
    "from logging import Logger\n",
    "from pprint import pprint\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from textdistance import levenshtein\n",
    "from typing import Dict, Union, List, Literal, Sequence, Optional\n",
    "import numpy as np\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.indices.base_retriever import BaseRetriever\n",
    "from llama_index.schema import Document, NodeWithScore, MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\")\n",
    "ARTIFACT_DIR = os.path.join(MAIN_DIR, \"artifacts\")\n",
    "EMB_DIR = os.path.join(DATA_DIR, \"emb_store\")\n",
    "DOCUMENT_DIR = os.path.join(MAIN_DIR, \"data\", \"document_sources\")\n",
    "EXCLUDE_DICT = os.path.join(DATA_DIR, \"exclude_pages.json\")\n",
    "\n",
    "with open(os.path.join(MAIN_DIR, \"auth\", \"api_keys.json\"), \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys[\"OPENAI_API_KEY\"]\n",
    "embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_dict(doc: Union[Document, NodeWithScore, Dict]) -> Dict:\n",
    "    if isinstance(doc, NodeWithScore):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": doc.score\n",
    "            } \n",
    "    elif isinstance(doc, Document):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc.text,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"score\": \"\"\n",
    "            }\n",
    "    elif isinstance(doc, Dict):\n",
    "        json_doc = {\n",
    "            \"page_content\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"score\": \"None\"\n",
    "        }\n",
    "    return json_doc\n",
    "\n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def calculate_emb_distance(\n",
    "    emb1: List[float],\n",
    "    emb2: List[float],\n",
    "    dist_type: Literal[\"l2\", \"ip\", \"cosine\", \"neg_exp_l2\"] = \"l2\"\n",
    "):\n",
    "    assert len(emb1) == len(emb2), \"Length of embedding vectors must match\"\n",
    "    if dist_type == \"l2\":\n",
    "        return np.square(np.linalg.norm(np.array(emb1) - np.array(emb2)))\n",
    "    elif dist_type == \"ip\":\n",
    "        return 1 - np.dot(emb1, emb2)\n",
    "    elif dist_type == \"cosine\":\n",
    "        cosine_similarity = np.dot(emb1, emb2)/(np.norm(emb1)*np.norm(emb2))\n",
    "        return 1 - cosine_similarity\n",
    "    elif dist_type == \"neg_exp_l2\":\n",
    "        return np.exp(-np.square(np.linalg.norm(np.array(emb1) - np.array(emb2))))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance type\")\n",
    "    \n",
    "def calculate_string_distance(\n",
    "    str1: str,\n",
    "    str2: Union[str, Sequence[str]],\n",
    "    embeddings: BaseEmbedding,\n",
    "    dist_type: Literal[\"l2\", \"ip\", \"cosine\", \"neg_exp_l2\"] = \"l2\"\n",
    "):\n",
    "    emb1 = embeddings.get_query_embedding(str1)\n",
    "    if isinstance(str2, str):\n",
    "        emb2 = embeddings.get_text_embedding(str2)\n",
    "        return calculate_emb_distance(emb1, emb2, dist_type)\n",
    "    else:\n",
    "        emb2_list = embeddings.get_text_embedding_batch(str2)\n",
    "        return [calculate_emb_distance(emb1, emb2) for emb2 in emb2_list]\n",
    "    \n",
    "def remove_final_sentence(\n",
    "    text: str,\n",
    "    return_final_sentence: bool = False\n",
    "):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    sentence_list = text.split(\".\")\n",
    "    previous_text = \".\".join(sentence_list[:-1])\n",
    "    final_sentence = sentence_list[-1]\n",
    "    return (previous_text, final_sentence) if return_final_sentence else previous_text\n",
    "\n",
    "def query_wrapper(\n",
    "    template: str, \n",
    "    input_text: Union[str, Dict[str, str]]\n",
    ") -> str:\n",
    "    placeholders = re.findall(pattern = r\"{([A-Za-z0-9_-]+)}\", string=template)\n",
    "    if isinstance(input_text, str):\n",
    "        assert len(placeholders) == 1, \"Must Provide a single placeholder when input_text is string.\"\n",
    "        placeholder = placeholders[0]\n",
    "        return template.format(**{placeholder:input_text})\n",
    "    \n",
    "    assert len(input_text) == len(placeholders)\n",
    "    for key in input_text.keys():\n",
    "        assert key in placeholders, f\"{key} not present in template.\"\n",
    "    \n",
    "    return template.format(**input_text)\n",
    "\n",
    "def retrieval_analysis(\n",
    "    testcase_df: pd.DataFrame,\n",
    "    testcases: Sequence[str] = None,\n",
    "    metadata_filters: Optional[Sequence[List[str]]] = None,\n",
    "    table_retriever: Optional[BaseRetriever] = None,\n",
    "    text_retriever: Optional[BaseRetriever] = None,\n",
    "    save_folder: Optional[str] = None,\n",
    "    logger: Optional[Logger] = None\n",
    "):\n",
    "    # Retrieve Nodes\n",
    "    retrieved_table_nodes = []\n",
    "    retrieved_text_nodes = []\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    if not logger:\n",
    "        logger = get_experiment_logs(\n",
    "            save_folder.split(\"/\")[-1], log_folder=save_folder\n",
    "        )\n",
    "        \n",
    "    if testcases is not None:\n",
    "        testcase_df[\"queries\"] = testcases\n",
    "    \n",
    "    if not metadata_filters:\n",
    "        if table_retriever:\n",
    "            table_retriever._kwargs = {}\n",
    "        if text_retriever:\n",
    "            text_retriever._kwargs = {}\n",
    "            \n",
    "        for test_case in testcases:\n",
    "            retrieved_table_nodes.append(table_retriever.retrieve(test_case) if table_retriever else [])\n",
    "            retrieved_text_nodes.append(text_retriever.retrieve(test_case) if text_retriever else [])\n",
    "    \n",
    "    else:\n",
    "        for test_case, filter_list in zip(testcases, metadata_filters):\n",
    "            if filter_list is not None:\n",
    "                if table_retriever:\n",
    "                    table_retriever._kwargs[\"where\"] = {\"condition\": {\"$in\": filter_list}}\n",
    "                if text_retriever:\n",
    "                    text_retriever._kwargs[\"where\"] = {\"condition\": {\"$in\": filter_list}}\n",
    "            \n",
    "            else:\n",
    "                if table_retriever:\n",
    "                    table_retriever._kwargs = {}\n",
    "                if text_retriever:\n",
    "                    text_retriever._kwargs = {}\n",
    "            \n",
    "            retrieved_table_nodes.append(table_retriever.retrieve(test_case) if (table_retriever and filter_list) else [])\n",
    "            retrieved_text_nodes.append(text_retriever.retrieve(test_case) if (text_retriever and filter_list) else [])\n",
    "\n",
    "    table_top_k = table_retriever.similarity_top_k if table_retriever else 0 \n",
    "    text_top_k = text_retriever.similarity_top_k if text_retriever else 0\n",
    "    \n",
    "    logger.info(f\"Successfully loaded table database k={table_top_k} and text database k={text_top_k}\")\n",
    "    \n",
    "    # Analyse retrieved contents\n",
    "    description_df = deepcopy(testcase_df)\n",
    "\n",
    "    retrieved_tables = [[] for _ in range(table_top_k)]\n",
    "    retrieved_texts = [[] for _ in range(text_top_k)]\n",
    "    \n",
    "    for case_idx in range(len(testcases)):\n",
    "        case_table_nodes = retrieved_table_nodes[case_idx]\n",
    "        case_text_nodes = retrieved_text_nodes[case_idx]\n",
    "        for node_idx, node_info_list in enumerate(retrieved_tables):\n",
    "            if node_idx > len(case_table_nodes) - 1:\n",
    "                node_info = np.nan\n",
    "            else:\n",
    "                node = case_table_nodes[node_idx]\n",
    "                node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "            node_info_list.append(node_info)\n",
    "        for node_idx, node_info_list in enumerate(retrieved_texts):\n",
    "            if node_idx > len(case_text_nodes) - 1:\n",
    "                node_info = np.nan\n",
    "            else:\n",
    "                node = case_text_nodes[node_idx]\n",
    "                node_info = node.get_content(MetadataMode.EMBED) + \"\\n\\nScore: {}\".format(node.score)\n",
    "            node_info_list.append(node_info)\n",
    "            \n",
    "    for idx, tables in enumerate(retrieved_tables):\n",
    "        description_df[f\"Table_{idx+1}\"] = tables\n",
    "        \n",
    "    for idx, texts in enumerate(retrieved_texts):\n",
    "        description_df[f\"Text_{idx+1}\"] = texts\n",
    "\n",
    "    description_df.to_csv(os.path.join(save_folder, \"table_text.csv\"))\n",
    "    \n",
    "    # Analyse Document level Hit Rate\n",
    "    file_df = deepcopy(testcase_df)\n",
    "\n",
    "    retrieved_table_pages = [[] for _ in range(table_top_k)]\n",
    "    retrieved_text_pages = [[] for _ in range(text_top_k)]\n",
    "    \n",
    "    for case_idx in range(len(testcases)):\n",
    "        case_table_nodes = retrieved_table_nodes[case_idx]\n",
    "        case_text_nodes = retrieved_text_nodes[case_idx]\n",
    "        \n",
    "        for node_idx, node_info_list in enumerate(retrieved_table_pages):\n",
    "            if node_idx > len(case_table_nodes) - 1:\n",
    "                node_file = np.nan\n",
    "            else:\n",
    "                node = case_table_nodes[node_idx]\n",
    "                node_file = node.metadata[\"file_name\"]\n",
    "            node_info_list.append(node_file)\n",
    "        for node_idx, node_info_list in enumerate(retrieved_text_pages):\n",
    "            if node_idx > len(case_text_nodes) - 1:\n",
    "                node_file = np.nan\n",
    "            else:\n",
    "                node = case_text_nodes[node_idx]\n",
    "                node_file = node.metadata[\"file_name\"]\n",
    "            node_info_list.append(node_file)\n",
    "         \n",
    "    if table_retriever:           \n",
    "        for idx, tables in enumerate(retrieved_table_pages):\n",
    "            file_df[f\"Table_{idx+1}_Page\"] = retrieved_table_pages[idx]\n",
    "            file_df[f\"Table_{idx+1}_HIT\"] = file_df[f\"Table_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "        file_df[\"Total Retrieved Tables\"] = (~file_df[[f\"Table_{idx+1}_Page\" for idx in range(table_top_k)]].isna()).sum(axis=1)\n",
    "        file_df[\"Total Table HITs\"] = file_df[[f\"Table_{idx+1}_HIT\" for idx in range(table_top_k)]].sum(axis=1)\n",
    "        table_retrieved_nodes_no = file_df[\"Total Retrieved Tables\"].sum()\n",
    "        table_node_retrieval_precision = file_df[\"Total Table HITs\"].sum() / table_retrieved_nodes_no\n",
    "        table_node_retrieval_recall = ((file_df[\"Total Table HITs\"] > 0) | (file_df[\"Appropriateness Category\"] == \"ICI\")).sum() / len(testcases)\n",
    "        \n",
    "        logger.info(f\"Table Node Retrieval Precision: {table_node_retrieval_precision * 100:.3f}\")\n",
    "        logger.info(f\"Table Node Retrieval Recall: {table_node_retrieval_recall * 100:.3f}\")\n",
    "    \n",
    "    if text_retriever:\n",
    "        for idx, texts in enumerate(retrieved_text_pages):\n",
    "            file_df[f\"Text_{idx+1}_Page\"] = retrieved_text_pages[idx]\n",
    "            file_df[f\"Text_{idx+1}_HIT\"] = file_df[f\"Text_{idx+1}_Page\"] == file_df[\"Guideline\"]\n",
    "        file_df[\"Total Text HITs\"] = file_df[[f\"Text_{idx+1}_HIT\"for idx in range(text_top_k)]].sum(axis=1)\n",
    "        file_df[\"Total Retrieved Texts\"] = (~file_df[[f\"Text_{idx+1}_Page\" for idx in range(text_top_k)]].isna()).sum(axis=1)\n",
    "        text_retrieved_nodes_no = file_df[\"Total Retrieved Texts\"].sum()\n",
    "        text_node_retrieval_precision = file_df[\"Total Text HITs\"].sum() / text_retrieved_nodes_no\n",
    "        text_node_retrieval_recall = ((file_df[\"Total Text HITs\"] > 0) | (file_df[\"Appropriateness Category\"] == \"ICI\")).sum() / len(testcases)\n",
    "        \n",
    "        logger.info(f\"Text Node Retrieval Precision: {text_node_retrieval_precision * 100:.3f}\")\n",
    "        logger.info(f\"Text Node Retrieval Recall: {text_node_retrieval_recall * 100:.3f}\")\n",
    "    \n",
    "    if table_retriever and text_retriever:\n",
    "        file_df[\"Total HITs\"] = file_df[[\"Total Table HITs\", \"Total Text HITs\"]].sum(axis=1)\n",
    "        file_df[\"Total Retrieved Nodes\"] = file_df[\"Total Retrieved Tables\"] + file_df[\"Total Retrieved Texts\"]\n",
    "        total_node_retrieval_precision = file_df[\"Total HITs\"].sum() / (table_retrieved_nodes_no + text_retrieved_nodes_no)\n",
    "        total_node_retrieval_recall = ((file_df[\"Total HITs\"] > 0) | (file_df[\"Appropriateness Category\"] == \"ICI\")).sum() / len(testcases)\n",
    "       \n",
    "        logger.info(f\"Total Node Retrieval Precision: {total_node_retrieval_precision * 100:.3f}\")\n",
    "        logger.info(f\"Total Node Retrieval Recall: {total_node_retrieval_recall * 100:.3f}\")\n",
    "    \n",
    "    file_df.to_csv(os.path.join(save_folder, \"table_text_pages.csv\"))\n",
    "    \n",
    "    # Analyse Node Scores\n",
    "    score_df = deepcopy(testcase_df)\n",
    "\n",
    "    retrieved_table_scores = [[] for _ in range(table_top_k)]\n",
    "    retrieved_texts_scores = [[] for _ in range(text_top_k)]\n",
    "\n",
    "    for case_idx in range(len(testcases)):\n",
    "        case_table_nodes = retrieved_table_nodes[case_idx]\n",
    "        case_text_nodes = retrieved_text_nodes[case_idx]\n",
    "        \n",
    "        for node_idx, node_info_list in enumerate(retrieved_table_scores):\n",
    "            if node_idx > len(case_table_nodes) - 1:\n",
    "                node_score = np.nan\n",
    "            else:\n",
    "                node = case_table_nodes[node_idx]\n",
    "                node_score = node.score\n",
    "            node_info_list.append(node_score)       \n",
    "            \n",
    "        for node_idx, node_info_list in enumerate(retrieved_texts_scores):\n",
    "            if node_idx > len(case_text_nodes) - 1:\n",
    "                node_score = np.nan\n",
    "            else:\n",
    "                node = case_text_nodes[node_idx]\n",
    "                node_score = node.score\n",
    "            node_info_list.append(node_score)\n",
    "            \n",
    "    if table_retriever:\n",
    "        for idx, tables in enumerate(retrieved_table_scores):\n",
    "            score_df[f\"Table_{idx+1}\"] = retrieved_table_scores[idx]\n",
    "        score_df['avg_table'] = score_df[[f\"Table_{idx+1}\" for idx in range(table_top_k)]].mean(axis=1)\n",
    "        mean_table_score = score_df['avg_table'].mean()\n",
    "        std_table_score = score_df['avg_table'].std()\n",
    "        logger.info(f\"Mean Table Distance (L2): {mean_table_score:.3f}\")\n",
    "        logger.info(f\"STD Table Distance (L2): {std_table_score:.3f}\")\n",
    "\n",
    "    if text_retriever:\n",
    "        for idx, texts in enumerate(retrieved_texts_scores):\n",
    "            score_df[f\"Text_{idx+1}\"] = retrieved_texts_scores[idx]\n",
    "        score_df['avg_text'] = score_df[[f\"Text_{idx+1}\" for idx in range(text_top_k)]].mean(axis=1)\n",
    "        mean_text_score = score_df['avg_text'].mean()\n",
    "        std_text_score = score_df['avg_text'].std()\n",
    "        logger.info(f\"Mean Text Distance (L2): {mean_text_score:.3f}\")\n",
    "        logger.info(f\"STD Text Distance (L2): {std_text_score:.3f}\")\n",
    "        \n",
    "    if table_retriever and text_retriever:\n",
    "        score_df['avg_overall'] = score_df[[\"avg_table\", \"avg_text\"]].mean(axis=1)\n",
    "        mean_overall_score = score_df['avg_overall'].mean()\n",
    "        std_overall_score = score_df['avg_overall'].std()\n",
    "        logger.info(f\"Mean Overall Distance (L2): {mean_overall_score:.3f}\")\n",
    "        logger.info(f\"STD Overall Distance (L2): {std_overall_score:.3f}\")\n",
    "\n",
    "    score_df.to_csv(os.path.join(save_folder, \"table_text_scores.csv\"))\n",
    "    \n",
    "    # Table HIT RATE\n",
    "    def add_punctuation(text: str):\n",
    "        if not text.endswith(\".\"):\n",
    "            text = text + \".\"\n",
    "        return text\n",
    "\n",
    "    if table_retriever:\n",
    "        table_hitrate_df = deepcopy(testcase_df)\n",
    "\n",
    "        table_hitrate_df['ACR scenario'] = table_hitrate_df['ACR scenario'].str.strip().apply(lambda x: add_punctuation(x))\n",
    "        retrieved_table_descriptions = [[] for _ in range(table_top_k)]\n",
    "\n",
    "        for case_idx in range(len(testcases)):\n",
    "            case_table_nodes = retrieved_table_nodes[case_idx]\n",
    "            \n",
    "            for node_idx, node_info_list in enumerate(retrieved_table_descriptions):\n",
    "                if node_idx > len(case_table_nodes) - 1:\n",
    "                    node_description = \"\"\n",
    "                else:\n",
    "                    node = case_table_nodes[node_idx]\n",
    "                    node_description = node.metadata[\"description\"].strip()\n",
    "                node_info_list.append(node_description)\n",
    "                \n",
    "        for idx, tables in enumerate(retrieved_table_descriptions):\n",
    "            table_hitrate_df[f\"Table_{idx+1}_descriptions\"] = retrieved_table_descriptions[idx]\n",
    "            table_hitrate_df[f\"Table_{idx+1}_LVscore\"] = table_hitrate_df.apply(lambda x: levenshtein.distance(x[\"ACR scenario\"], x[f\"Table_{idx+1}_descriptions\"]), axis=1)\n",
    "            \n",
    "        table_hitrate_df[\"Min_LV_Dist\"] = table_hitrate_df[[f\"Table_{idx+1}_LVscore\" for idx in range(table_top_k)]].min(axis=1)\n",
    "        table_hitrate_df[\"HIT\"] = (table_hitrate_df[\"Min_LV_Dist\"] < 5) + (table_hitrate_df[\"Appropriateness Category\"] == \"ICI\")\n",
    "\n",
    "        logger.info(\"Exact Retrieved Table Nodes Hit Rate: {:.3f}\".format(table_hitrate_df[\"HIT\"].sum() / len(table_hitrate_df[\"HIT\"])))\n",
    "\n",
    "        table_hitrate_df.to_csv(os.path.join(save_folder, \"hit_rate.csv\"))\n",
    "\n",
    "    return (description_df, file_df, score_df, table_hitrate_df) if table_retriever else (description_df, file_df, score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"queries\", \"MSK LLM Fictitious Case Files Full.csv\"),\n",
    "        usecols = ['ACR scenario', 'Guideline', 'Variant', 'Appropriateness Category',\n",
    "                   'Scan Order', 'Clinical File']\n",
    "        )\n",
    "patient_profiles = testcase_df[\"Clinical File\"]\n",
    "scan_orders = testcase_df[\"Scan Order\"]\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "\n",
    "testcase_df[\"queries\"] = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs before filtering: 546\n",
      "Total number of docs after filtering 395\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DOCUMENT_DIR).load_data()\n",
    "print(\"Total no of docs before filtering:\", len(documents))\n",
    "with open(EXCLUDE_DICT, \"r\") as f:\n",
    "    exclude_pages = json.load(f)\n",
    "documents = filter_by_pages(doc_list=documents, exclude_info=exclude_pages)\n",
    "print(\"Total number of docs after filtering\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LLM Extraction Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_best.json\"), \"r\") as f:\n",
    "    extracted_best = json.load(f)\n",
    "    extracted_profiles = extracted_best[\"profiles\"]\n",
    "    extracted_best_guidelines = extracted_best[\"guidelines\"]\n",
    "    \n",
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"r\") as f:\n",
    "    extracted_multiple = json.load(f)\n",
    "    extracted_multiple_guidelines = extracted_multiple[\"guidelines\"]\n",
    "    \n",
    "extracted_profiles = [profile.strip() for profile in extracted_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = deepcopy(testcase_df)\n",
    "emb_df[\"llm_profile\"] = extracted_profiles\n",
    "emb_df[\"best_guideline\"] = extracted_best_guidelines\n",
    "emb_df[\"multiple_guidelines\"] = extracted_multiple_guidelines\n",
    "emb_df[\"condition\"] = emb_df[\"Guideline\"].apply(lambda x: x[4:-4]).str.lower()\n",
    "emb_df[\"best_hit\"] = [(guideline in best_guideline) or (app==\"ICI\" and not best_guideline)\n",
    "                      for guideline, best_guideline, app in zip(emb_df[\"condition\"], emb_df[\"best_guideline\"], emb_df[\"Appropriateness Category\"])]\n",
    "emb_df[\"multiple_hit\"] = [(guideline in multiple_guidelines) or (app==\"ICI\" and not multiple_guidelines)\n",
    "                          for guideline, multiple_guidelines, app in zip(emb_df[\"condition\"], emb_df[\"multiple_guidelines\"], emb_df[\"Appropriateness Category\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hit Rate:  0.8873239436619719\n",
      "Multiple Hit Rate:  0.9436619718309859\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hit Rate: \", emb_df[\"best_hit\"].sum()/len(emb_df))\n",
    "print(\"Multiple Hit Rate: \", emb_df[\"multiple_hit\"].sum()/len(emb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: 19\n",
      "('Patient: 46 year old Chinese male.  Businessman, sales.  Frequent drinker '\n",
      " 'due to job, about 1-2 beers/day.  Past medical history of fatty liver, acute '\n",
      " 'cholecystitis post cholecystectomy.  Now presenting with severe pain at '\n",
      " 'right big toe for 2 weeks, pain on and off improves with paracetamol and '\n",
      " 'ibuprofen.  On examination: swelling and erythema at right big toe 1st '\n",
      " 'metatarsophalangeal joint.  No prior imaging')\n",
      "('Variant: Chronic extremity joint pain. Suspect inflammatory (seropositive or '\n",
      " 'seronegative arthritis), crystalline (gout or pseudogout), or erosive '\n",
      " 'osteoarthritis. Initial imaging.')\n",
      "['chronic foot pain', 'suspected om septic arthritis soft tissue infection']\n",
      "'ACR chroni extremity joint pain inflammatory arthritis.pdf'\n",
      "\n",
      "Case: 20\n",
      "('Patient: 55 year old Philipino female.  Domestic helper.  No significant '\n",
      " 'past medical history.  Bilateral finger joint pain, swelling and stiffness '\n",
      " 'worse in the morning for 4 months, worsening recently.  No eye redness, '\n",
      " 'shortness of breath, fever, loss of weight/appetite.  No other joint '\n",
      " \"involvement.  On examination: Heberden's nodes bilateral distal \"\n",
      " 'interphalangeal joints, mild swelling, no significant tenderness.  X-rays '\n",
      " 'show degenerative changes in bilateral hand interphalangeal joints with '\n",
      " 'equivocal gull-wing deformity')\n",
      "('Variant: Chronic extremity joint pain. Suspect erosive osteoarthritis. '\n",
      " 'Radiographs normal or inconclusive. Next imaging study.')\n",
      "['chronic hand and wrist pain']\n",
      "'ACR chroni extremity joint pain inflammatory arthritis.pdf'\n",
      "\n",
      "Case: 45\n",
      "('Patient: 13 year old Caucasian female.  No prior medical history.  Does 200 '\n",
      " 'metre sprinting for school.  Now presenting with 3 month history of right '\n",
      " 'knee pain, sometimes waking her up at night.  No fever.  No inciting '\n",
      " 'trauma.  No loss of weight or appetitie.  On examination, mild swelling in '\n",
      " 'right knee.  Otherwise unremarkable examination.  No prior imaging')\n",
      "'Variant: Suspect primary bone tumor. Initial imaging.'\n",
      "['chronic knee pain', 'stress fracture including sacrum']\n",
      "'ACR primary bone tumours.pdf'\n",
      "\n",
      "Case: 59\n",
      "('Patient: 73 year old Indian male, with history of end stage diabetic '\n",
      " 'nephropathy on haemodialysis. Complains of lower back pain for the past 2 '\n",
      " 'months with occasional night pain. Associated with radicular pain of left '\n",
      " 'lower limb. no fever. Physical exam reveals mild tenderness at L3-L4 region '\n",
      " 'but otherwise unremarkable. No prior imaging')\n",
      "('Variant: Suspected spine infection (such as epidural abscess or discitis '\n",
      " 'osteomyelitis), with new or worsening back or neck pain, with or without '\n",
      " 'fever, who may have one or more of the following red flags (diabetes '\n",
      " 'mellitus, IV drug use, cancer, HIV, or dialysis) or abnormal lab values. '\n",
      " 'Initial imaging.')\n",
      "['low back pain', 'inflammatory back pain', 'myelopathy']\n",
      "'ACR suspected spine infection.pdf'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in emb_df[~emb_df[\"multiple_hit\"]].index:\n",
    "    case_file = emb_df.iloc[idx, :]\n",
    "    print(\"Case:\", idx + 1)\n",
    "    pprint(\"Patient: \" + remove_final_sentence(case_file[\"Clinical File\"]))\n",
    "    pprint(\"Variant: \" + case_file[\"ACR scenario\"])\n",
    "    pprint(case_file[\"multiple_guidelines\"])\n",
    "    pprint(case_file[\"Guideline\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_analysis_path = os.path.join(ARTIFACT_DIR, \"emb_analysis\")\n",
    "if not os.path.exists(emb_analysis_path):\n",
    "    os.makedirs(emb_analysis_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df[\"original_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"Clinical File\"], x[\"ACR scenario\"], embed_model), axis=1)\n",
    "emb_df[\"query_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"queries\"], x[\"ACR scenario\"], embed_model), axis=1)\n",
    "emb_df[\"refine_dist\"] = emb_df.apply(lambda x: calculate_string_distance(x[\"llm_profile\"], x[\"ACR scenario\"], embed_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.to_csv(os.path.join(emb_analysis_path, \"emb_analysis.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-chroma\", \"descriptions\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"chroma\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"chroma\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    "    )\n",
    "text_retriever = text_index.as_retriever(\n",
    "    similarity_top_k = 5,\n",
    "    filters = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Single Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:03:02,555:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/emb_store/faiss/openai_512_20.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"emb_store\", \"faiss\", \"openai_512_20\")\n",
    "baseline_index = load_vectorindex(db_directory, \"faiss\")\n",
    "\n",
    "baseline_text_retriever = baseline_index.as_retriever(similarity_top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just file\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_baseline_nosplit_withscanorder_full\")\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    text_retriever=baseline_text_retriever, save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)\n",
    "- Table Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:03:23,068:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/full/tables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:03:23,192:INFO: faiss VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-faiss/full/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-faiss\", \"full\")\n",
    "table_index = load_vectorindex(os.path.join(db_directory, \"tables\"), \"faiss\")\n",
    "text_index = load_vectorindex(os.path.join(db_directory, \"texts\"), \"faiss\")\n",
    "\n",
    "table_retriever = table_index.as_retriever(similarity_top_k = 5)\n",
    "text_retriever = text_index.as_retriever(similarity_top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:04:11,339:INFO: Successfully loaded table database k=5 and text database k=5\n",
      "2023-12-08 19:04:11,339:INFO: Successfully loaded table database k=5 and text database k=5\n",
      "2023-12-08 19:04:11,410:INFO: Table Node Retrieval Precision: 60.000\n",
      "2023-12-08 19:04:11,410:INFO: Table Node Retrieval Precision: 60.000\n",
      "2023-12-08 19:04:11,412:INFO: Table Node Retrieval Recall: 87.324\n",
      "2023-12-08 19:04:11,412:INFO: Table Node Retrieval Recall: 87.324\n",
      "2023-12-08 19:04:11,418:INFO: Text Node Retrieval Precision: 67.324\n",
      "2023-12-08 19:04:11,418:INFO: Text Node Retrieval Precision: 67.324\n",
      "2023-12-08 19:04:11,420:INFO: Text Node Retrieval Recall: 87.324\n",
      "2023-12-08 19:04:11,420:INFO: Text Node Retrieval Recall: 87.324\n",
      "2023-12-08 19:04:11,425:INFO: Total Node Retrieval Precision: 63.662\n",
      "2023-12-08 19:04:11,425:INFO: Total Node Retrieval Precision: 63.662\n",
      "2023-12-08 19:04:11,427:INFO: Total Node Retrieval Recall: 90.141\n",
      "2023-12-08 19:04:11,427:INFO: Total Node Retrieval Recall: 90.141\n",
      "2023-12-08 19:04:11,440:INFO: Mean Table Distance (L2): 0.322\n",
      "2023-12-08 19:04:11,440:INFO: Mean Table Distance (L2): 0.322\n",
      "2023-12-08 19:04:11,442:INFO: STD Table Distance (L2): 0.030\n",
      "2023-12-08 19:04:11,442:INFO: STD Table Distance (L2): 0.030\n",
      "2023-12-08 19:04:11,445:INFO: Mean Text Distance (L2): 0.331\n",
      "2023-12-08 19:04:11,445:INFO: Mean Text Distance (L2): 0.331\n",
      "2023-12-08 19:04:11,447:INFO: STD Text Distance (L2): 0.028\n",
      "2023-12-08 19:04:11,447:INFO: STD Text Distance (L2): 0.028\n",
      "2023-12-08 19:04:11,450:INFO: Mean Overall Distance (L2): 0.327\n",
      "2023-12-08 19:04:11,450:INFO: Mean Overall Distance (L2): 0.327\n",
      "2023-12-08 19:04:11,452:INFO: STD Overall Distance (L2): 0.028\n",
      "2023-12-08 19:04:11,452:INFO: STD Overall Distance (L2): 0.028\n",
      "2023-12-08 19:04:14,884:INFO: Exact Retrieved Table Nodes Hit Rate: 0.690\n",
      "2023-12-08 19:04:14,884:INFO: Exact Retrieved Table Nodes Hit Rate: 0.690\n"
     ]
    }
   ],
   "source": [
    "# Just file\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_original_profile_withscanorder_full\")\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = pd.read_csv(\n",
    "    os.path.join(\"../dist_compare.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.315901198087324\n",
      "0.32204540616901406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2.5617918144365492, 0.010413371867957268)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "print(compare_df[\"table_desc_meta\"].mean())\n",
    "print(compare_df[\"table_full\"].mean())\n",
    "ztest(compare_df[\"table_desc_meta\"], compare_df[\"table_full\"], value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Content:\n",
    "- Metadata (Descriptions + Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 18:37:33,520:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/tables.\n",
      "2023-12-08 18:37:33,623:INFO: chroma VectorStore successfully loaded from /mnt/c/Users/QUAN/Desktop/lbp_mri/data/multimodal-chroma/descriptions/texts.\n"
     ]
    }
   ],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-chroma\", \"descriptions\")\n",
    "\n",
    "table_index = load_vectorindex(\n",
    "    db_directory=os.path.join(db_directory, \"tables\"),\n",
    "    emb_store_type=\"chroma\", index_name=\"tables\"\n",
    "    )\n",
    "text_index = load_vectorindex(\n",
    "    db_directory=os.path.join(db_directory, \"texts\"),\n",
    "    emb_store_type=\"chroma\", index_name=\"texts\"\n",
    "    )\n",
    "\n",
    "table_retriever = table_index.as_retriever(similarity_top_k = 5)\n",
    "text_retriever = text_index.as_retriever(similarity_top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 18:38:56,219:INFO: Successfully loaded table database k=5 and text database k=5\n",
      "2023-12-08 18:38:56,292:INFO: Table Node Retrieval Precision: 66.761\n",
      "2023-12-08 18:38:56,293:INFO: Table Node Retrieval Recall: 91.549\n",
      "2023-12-08 18:38:56,300:INFO: Text Node Retrieval Precision: 71.268\n",
      "2023-12-08 18:38:56,301:INFO: Text Node Retrieval Recall: 88.732\n",
      "2023-12-08 18:38:56,304:INFO: Total Node Retrieval Precision: 69.014\n",
      "2023-12-08 18:38:56,305:INFO: Total Node Retrieval Recall: 92.958\n",
      "2023-12-08 18:38:56,320:INFO: Mean Table Distance (L2): 0.729\n",
      "2023-12-08 18:38:56,321:INFO: STD Table Distance (L2): 0.021\n",
      "2023-12-08 18:38:56,325:INFO: Mean Text Distance (L2): 0.730\n",
      "2023-12-08 18:38:56,327:INFO: STD Text Distance (L2): 0.020\n",
      "2023-12-08 18:38:56,330:INFO: Mean Overall Distance (L2): 0.730\n",
      "2023-12-08 18:38:56,333:INFO: STD Overall Distance (L2): 0.020\n",
      "2023-12-08 18:38:59,282:INFO: Exact Retrieved Table Nodes Hit Rate: 0.803\n"
     ]
    }
   ],
   "source": [
    "# Standard Queries (Original Clinical Profile + Scan order)\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_original_profile_withscanorder_descriptions\")\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Clinical Profile\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_original_profile_noscan_descriptions\")\n",
    "testcases = [remove_final_sentence(testcase) for testcase in testcase_df[\"Clinical File\"]]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Extracted Profile\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_LLMrefine_noscanorder_descriptions\")\n",
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"r\") as f:\n",
    "    extracted_multiple = json.load(f)\n",
    "    extracted_profiles = extracted_multiple[\"profiles\"]\n",
    "    extracted_multiple_guidelines = extracted_multiple[\"guidelines\"]\n",
    "    \n",
    "testcases = [profile.strip() for profile in extracted_profiles]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted Profile + Scan Order\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_refine_withscanorder_descriptions\")\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": extracted_profile.strip(),\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for extracted_profile, patient_profile in zip(extracted_profiles, patient_profiles)\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_directory = os.path.join(DATA_DIR, \"multimodal-chroma\", \"descriptions\")\n",
    "\n",
    "table_index = load_vectorindex(\n",
    "    db_directory=os.path.join(db_directory, \"tables\"),\n",
    "    emb_store_type=\"chroma\", index_name=\"tables\"\n",
    "    )\n",
    "text_index = load_vectorindex(\n",
    "    db_directory=os.path.join(db_directory, \"texts\"),\n",
    "    emb_store_type=\"chroma\", index_name=\"texts\"\n",
    "    )\n",
    "\n",
    "table_retriever = table_index.as_retriever(similarity_top_k = 5)\n",
    "text_retriever = text_index.as_retriever(similarity_top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted Profile + Scan Order + Original Profiles\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_original_withscan_metadata_filter_descriptions\")\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": remove_final_sentence(patient_profile, True)[0],\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for patient_profile in patient_profiles\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    metadata_filters = extracted_multiple_guidelines,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTIFACT_DIR, \"extracted_multiple.json\"), \"r\") as f:\n",
    "    extracted_multiple = json.load(f)\n",
    "    extracted_profiles = extracted_multiple[\"profiles\"]\n",
    "    extracted_multiple_guidelines = extracted_multiple[\"guidelines\"]\n",
    "    \n",
    "extracted_profiles = [profile.strip() for profile in extracted_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted Profile + Scan Order + With LLM Refine\n",
    "save_folder = os.path.join(ARTIFACT_DIR, \"retrieval_analysis_refine_withscan_metadata_filter_descriptions\")\n",
    "\n",
    "question_template = \"Patient Profile: {profile}\\nScan ordered: {scan_order}\"\n",
    "testcases = [\n",
    "    query_wrapper(question_template, {\"profile\": extracted_profile.strip(),\n",
    "                                      \"scan_order\": remove_final_sentence(patient_profile, True)[1]})\n",
    "    for extracted_profile, patient_profile in zip(extracted_profiles, patient_profiles)\n",
    "    ]\n",
    "\n",
    "description_df, file_df, score_df, table_hitrate_df = retrieval_analysis(\n",
    "    testcase_df=testcase_df, testcases=testcases,\n",
    "    table_retriever=table_retriever, text_retriever=text_retriever,\n",
    "    metadata_filters = extracted_multiple_guidelines,\n",
    "    save_folder=save_folder\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
